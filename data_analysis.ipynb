{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "854f15c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, filtfilt, iirnotch, periodogram\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import kurtosis\n",
    "from torch.utils.data import random_split\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gc\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3b74ce94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise Filters\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = fs / 2.0\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def bandpass_filter(data, lowcut=1.0, highcut=200.0, fs=1000.0, order=4):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    return filtfilt(b, a, data, axis=0)\n",
    "\n",
    "# Apply after bandpass\n",
    "def notch_filter(data, freq=60.0, fs=1000.0, quality=30.0):\n",
    "    b, a = iirnotch(freq, quality, fs)\n",
    "    return filtfilt(b, a, data, axis=0)\n",
    "\n",
    "# Noise Metrics for evaluation\n",
    "def compute_rmse(true, estimate):\n",
    "    return np.sqrt(np.mean((true - estimate) ** 2))\n",
    "\n",
    "def compute_spectral_flatness_band(signal, fs, band):\n",
    "    f, Pxx = periodogram(signal, fs=fs)\n",
    "    idx = (f >=band[0]) & (f <=band[1])\n",
    "    Pxx = Pxx[idx] + 1e-12\n",
    "    gm = np.exp(np.mean(np.log(Pxx)))\n",
    "    am = np.mean(Pxx)\n",
    "    return gm / am\n",
    "\n",
    "mu_band = (8, 13)\n",
    "beta_band = (13, 30)\n",
    "hg_band = (70, 200)\n",
    "\n",
    "# Kurtosis signal reduction > 0 shows a denoised signal\n",
    "def proportion_of_positive_kurtosis_signals(kurtosis_raw, kurtosis_denoised):\n",
    "    return (np.array([(kurtosis_raw - kurtosis_denoised) > 0]).sum() / len(kurtosis_raw)) * 100\n",
    "\n",
    "# Use a Standard scaler to reduce the mean to 0 and std to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "377dae80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definitions\n",
    "class EcogMotionDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "# CNN\n",
    "class EcogToMotionNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size = 3, padding = 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(128, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.squeeze(-1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Linear Model\n",
    "class LinearEcogToMotionNet(nn.Module):\n",
    "    def __init__(self, input_channels = 64, sequence_length = 20, output_dim = 3):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_channels * sequence_length, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "# LSTM\n",
    "class EcogLSTM(nn.Module):\n",
    "    def __init__(self, input_size = 64, hidden_size = 128, num_layers = 1, output_size = 3):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x) # lstm_out shape: (batch_size, seq_len, hidden_size)\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        output = self.fc(last_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "00512565",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessData:\n",
    "    def __init__(self, ecog_file_path, motion_file_path):\n",
    "        self.ecog_file_path = ecog_file_path\n",
    "        self.motion_file_path = motion_file_path\n",
    "        self.ecog_data = None\n",
    "        self.motion_data = None\n",
    "        self.filtered_ecog = None\n",
    "        self.scaled_ecog = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "\n",
    "    def process(self, eval=False, window_size=20, duration_limit=900):\n",
    "        self.read_data()\n",
    "        self.filter_signal(eval=eval)\n",
    "        self.format_data(window_size=window_size, duration_limit=duration_limit)\n",
    "        return self.X, self.y\n",
    "    \n",
    "    def read_data(self):\n",
    "        self.ecog_data = pd.read_csv(self.ecog_file_path)\n",
    "        self.motion_data = pd.read_csv(self.motion_file_path)\n",
    "        return self\n",
    "\n",
    "    def filter_signal(self, eval=False):\n",
    "        ecog_raw = self.ecog_data[self.ecog_data.columns[1:-1]].values\n",
    "\n",
    "        # Apply filters\n",
    "        filtered = bandpass_filter(ecog_raw, lowcut=1.0, highcut=200.0, fs=1000.0, order=4)\n",
    "        denoised = notch_filter(filtered, freq=60, fs=1000.0)\n",
    "\n",
    "        # Evaluate filters\n",
    "        if eval:\n",
    "            kurt_raw = kurtosis(ecog_raw, axis=0, fisher=True)\n",
    "            kurt_denoised = kurtosis(denoised, axis=0, fisher=True)\n",
    "            proportion_of_positive_kurtosis_signals(kurt_raw, kurt_denoised)\n",
    "            compute_rmse(ecog_raw, denoised)\n",
    "\n",
    "        # Scale\n",
    "        scaler = StandardScaler()\n",
    "        self.scaled_ecog = scaler.fit_transform(denoised)\n",
    "\n",
    "        # Replace in DataFrame\n",
    "        self.ecog_data = self.ecog_data.copy()\n",
    "        self.ecog_data[self.ecog_data.columns[1:-1]] = self.scaled_ecog\n",
    "\n",
    "        # Clean memory\n",
    "        del ecog_raw, filtered, denoised\n",
    "        gc.collect()\n",
    "        return self\n",
    "\n",
    "    def format_data(self, window_size=20, duration_limit=900):\n",
    "        ecog_df = self.ecog_data[self.ecog_data[\"Time\"] <= duration_limit]\n",
    "        motion_df = self.motion_data[self.motion_data[\"Motion_time\"] <= duration_limit]\n",
    "\n",
    "        ecog_values = ecog_df[ecog_df.columns[1:-1]].values\n",
    "        motion_values = motion_df[motion_df.columns[2:]].values\n",
    "\n",
    "        num_windows = motion_values.shape[0]\n",
    "\n",
    "        # Create sliding windows\n",
    "        self.X = np.stack([\n",
    "            ecog_values[i*window_size:(i+1)*window_size]\n",
    "            for i in range(num_windows)\n",
    "        ])\n",
    "        self.y = motion_values[:num_windows]\n",
    "        \n",
    "        # Clean up\n",
    "        del ecog_values, motion_values\n",
    "        gc.collect()\n",
    "\n",
    "    def save(self):\n",
    "        output_file_path_base = self.ecog_file_path.strip(\"ecog_data.csv\")\n",
    "        np.save(output_file_path_base + \"X.npy\", self.X)\n",
    "        np.save(output_file_path_base + \"y.npy\", self.y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "283cc29d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/linux-pc/gh/CRCNS/src/motor_cortex/data/'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(os.getcwd(), \"src/\", \"motor_cortex/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6729e9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_data_file_l = glob(os.path.join(os.getcwd(), \"src/\", \"motor_cortex/data/data/\", \"**\", \"motion*.csv\"), recursive=True)\n",
    "ecog_data_file_l = glob(os.path.join(os.getcwd(), \"src/\", \"motor_cortex/data/data/\", \"**\", \"ecog*.csv\"), recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c26f274",
   "metadata": {},
   "source": [
    "# Reading only the Left Wrist data and training to detect depending on the Left Wrist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ded4913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Bilateral/2018-07-12_(S1)/motion_data.csv',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Bilateral/2018-10-04_(S6)/motion_data.csv',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Bilateral/2018-09-27_(S5)/motion_data.csv',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Bilateral/2018-07-19_(S2)/motion_data.csv',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Bilateral/2018-08-14_(S4)/motion_data.csv',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Bilateral/2018-07-26_(S3)/motion_data.csv',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S7)/motion_data.csv',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-24_(S10)/motion_data.csv',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S1)/motion_data.csv',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-06_(S6)/motion_data.csv',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S5)/motion_data.csv',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S3)/motion_data.csv',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S4)/motion_data.csv',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S8)/motion_data.csv',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-17_(S9)/motion_data.csv',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S2)/motion_data.csv',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-03-15_(S1)/motion_data.csv',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S3)/motion_data.csv',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-08_(S2)/motion_data.csv',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S7)/motion_data.csv',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S4)/motion_data.csv',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S9)/motion_data.csv',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S5)/motion_data.csv',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S8)/motion_data.csv',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-05-31_(S10)/motion_data.csv',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-15_(S6)/motion_data.csv']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motion_data_file_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3490a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Bilateral/2018-07-12_(S1)/motion_data.csv'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motion_data_file_l[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fe4bdef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S7)/motion_data.csv'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motion_data_file_l[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ed4c084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-03-15_(S1)/motion_data.csv'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motion_data_file_l[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ef8451a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Bilateral/2018-07-12_(S1)/ecog_data.csv'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecog_data_file_l[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "398affbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S7)/ecog_data.csv'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecog_data_file_l[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4ef6433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-03-15_(S1)/ecog_data.csv'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecog_data_file_l[16]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc74d3e4",
   "metadata": {},
   "source": [
    "## Bilateral Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff3bfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_data_file_l[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907f36ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_data_file = motion_data_file_l[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a5a011",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecog_data_file = ecog_data_file_l[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07bc434",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfec741",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecog_data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b3a041",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_data_bilateral_2018_07_12_S1 = pd.read_csv(motion_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d41fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_data_bilateral_2018_07_12_S1[\"Motion_time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafb318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecog_data_bilateral_2018_07_12_S1 = pd.read_csv(ecog_data_file)\n",
    "channel_data = ecog_data_bilateral_2018_07_12_S1.columns[1:-1].values\n",
    "ecog_data_bilateral_2018_07_12_S1[channel_data].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4ad7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecog_data_bilateral_2018_07_12_S1[\"Time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d811d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ecog_data_bilateral_2018_07_12_S1[\"Time\"], ecog_data_bilateral_2018_07_12_S1[channel_data].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ef02ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecog_channels = ecog_data_bilateral_2018_07_12_S1.columns[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7fda74",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecog_data_bilateral_2018_07_12_S1[ecog_channels].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806049ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_data_bilateral_2018_07_12_S1_left_wrist = motion_data_bilateral_2018_07_12_S1[motion_data_bilateral_2018_07_12_S1.columns[1:4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e925d624",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_data_bilateral_2018_07_12_S1_left_wrist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a05342",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_data_bilateral_2018_07_12_S1_left_wrist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8b2f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecog_data_bilateral_2018_07_12_S1[channel_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da53d06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del ecog_data_bilateral_2018_07_12_S1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e491bb",
   "metadata": {},
   "source": [
    "## Ipsilateral Data (Right Wrist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf2de551",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_ecog_data_file = ecog_data_file_l[6]\n",
    "current_motion_data_file = motion_data_file_l[6]\n",
    "preprocessor = PreprocessData(current_ecog_data_file, current_motion_data_file)\n",
    "X, y = preprocessor.process()\n",
    "preprocessor.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7ac0622",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m16\u001b[39m):\n\u001b[1;32m      3\u001b[0m     preprocessor \u001b[38;5;241m=\u001b[39m PreprocessData(ecog_data_file_l[index], motion_data_file_l[index])\n\u001b[0;32m----> 4\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     preprocessor\u001b[38;5;241m.\u001b[39msave()\n",
      "Cell \u001b[0;32mIn[22], line 20\u001b[0m, in \u001b[0;36mPreprocessData.process\u001b[0;34m(self, eval, window_size, duration_limit)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28meval\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, window_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, duration_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m900\u001b[39m):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_data()\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter_signal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43meval\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43meval\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_data(window_size\u001b[38;5;241m=\u001b[39mwindow_size, duration_limit\u001b[38;5;241m=\u001b[39mduration_limit)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my\n",
      "Cell \u001b[0;32mIn[22], line 33\u001b[0m, in \u001b[0;36mPreprocessData.filter_signal\u001b[0;34m(self, eval)\u001b[0m\n\u001b[1;32m     30\u001b[0m ecog_raw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mecog_data[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mecog_data\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Apply filters\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m filtered \u001b[38;5;241m=\u001b[39m \u001b[43mbandpass_filter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mecog_raw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlowcut\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhighcut\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m denoised \u001b[38;5;241m=\u001b[39m notch_filter(filtered, freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m, fs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000.0\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Evaluate filters\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 11\u001b[0m, in \u001b[0;36mbandpass_filter\u001b[0;34m(data, lowcut, highcut, fs, order)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbandpass_filter\u001b[39m(data, lowcut\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, highcut\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200.0\u001b[39m, fs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000.0\u001b[39m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m):\n\u001b[1;32m     10\u001b[0m     b, a \u001b[38;5;241m=\u001b[39m butter_bandpass(lowcut, highcut, fs, order\u001b[38;5;241m=\u001b[39morder)\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfiltfilt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/scipy/signal/_signaltools.py:4543\u001b[0m, in \u001b[0;36mfiltfilt\u001b[0;34m(b, a, x, axis, padtype, padlen, method, irlen)\u001b[0m\n\u001b[1;32m   4540\u001b[0m x0 \u001b[38;5;241m=\u001b[39m axis_slice(ext, stop\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m   4542\u001b[0m \u001b[38;5;66;03m# Forward filter.\u001b[39;00m\n\u001b[0;32m-> 4543\u001b[0m (y, zf) \u001b[38;5;241m=\u001b[39m \u001b[43mlfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4545\u001b[0m \u001b[38;5;66;03m# Backward filter.\u001b[39;00m\n\u001b[1;32m   4546\u001b[0m \u001b[38;5;66;03m# Create y0 so zi*y0 broadcasts appropriately.\u001b[39;00m\n\u001b[1;32m   4547\u001b[0m y0 \u001b[38;5;241m=\u001b[39m axis_slice(y, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/scipy/signal/_signaltools.py:2177\u001b[0m, in \u001b[0;36mlfilter\u001b[0;34m(b, a, x, axis, zi)\u001b[0m\n\u001b[1;32m   2175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _sigtools\u001b[38;5;241m.\u001b[39m_linear_filter(b, a, x, axis)\n\u001b[1;32m   2176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sigtools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_linear_filter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzi\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Preprocess Ipsilateral Data\n",
    "for index in range(6, 16):\n",
    "    preprocessor = PreprocessData(ecog_data_file_l[index], motion_data_file_l[index])\n",
    "    X, y = preprocessor.process()\n",
    "    preprocessor.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddf91ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-03-15_(S1)/ecog_data.csv',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S3)/ecog_data.csv',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-08_(S2)/ecog_data.csv',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S7)/ecog_data.csv',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S4)/ecog_data.csv',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S9)/ecog_data.csv',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S5)/ecog_data.csv',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S8)/ecog_data.csv',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-05-31_(S10)/ecog_data.csv',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-15_(S6)/ecog_data.csv']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecog_data_file_l[16:ecog_data_file_l.__len__()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae28952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocess Contralateral Data\n",
    "# for index in range(16, ecog_data_file_l.__len__()):\n",
    "#     preprocessor = PreprocessData(ecog_data_file_l[index], motion_data_file_l[index])\n",
    "#     X, y = preprocessor.process()\n",
    "#     preprocessor.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "72db258b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "processed_data_l = glob(os.path.join('/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/', '**', \"**\", \"*.npy\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "88d61567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S7)/X.npy'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data_l[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c7f3c56a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S7)/y.npy'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data_l[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3ec728c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_data_l) // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ee63e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S7)/X.npy\n",
      "/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S7)/y.npy\n",
      "/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-24_(S10)/X.npy\n",
      "/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-24_(S10)/y.npy\n",
      "/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S1)/X.npy\n",
      "/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S1)/y.npy\n",
      "/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-06_(S6)/X.npy\n",
      "/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-06_(S6)/y.npy\n",
      "/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S5)/X.npy\n",
      "/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S5)/y.npy\n",
      "/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S3)/X.npy\n",
      "/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S3)/y.npy\n",
      "/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S4)/X.npy\n",
      "/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S4)/y.npy\n",
      "/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S8)/X.npy\n",
      "/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S8)/y.npy\n",
      "/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-17_(S9)/X.npy\n",
      "/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-17_(S9)/y.npy\n",
      "/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S2)/X.npy\n",
      "/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S2)/y.npy\n"
     ]
    }
   ],
   "source": [
    "# # Perform K-Fold Cross Validation\n",
    "# iterator = iter(processed_data_l)\n",
    "\n",
    "# for X, y in zip(iterator, iterator):\n",
    "#     print(X)\n",
    "#     print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480cc51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Train and Validation Sets\n",
    "dataset = EcogMotionDataset(X, y)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066e8641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model\n",
    "\n",
    "# Train Linear Model\n",
    "input_channels = X.shape[2]\n",
    "sequence_length = X.shape[1]\n",
    "model = LinearEcogToMotionNet(input_channels, sequence_length)\n",
    "\n",
    "# Train 1D CNN\n",
    "# model = EcogToMotionNet()\n",
    "\n",
    "# Train LSTM\n",
    "# model = EcogLSTM(input_size=64, hidden_size=128, num_layers=1, output_size=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc6a8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Model \n",
    "\n",
    "# Train Linear Model\n",
    "input_channels = X.shape[2]\n",
    "sequence_length = X.shape[1]\n",
    "model = LinearEcogToMotionNet(input_channels, sequence_length)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(X_batch)\n",
    "        loss = criterion(preds, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            preds = model(X_batch)\n",
    "            loss = criterion(preds, y_batch)\n",
    "            val_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Val.Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c143e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 1D CNN\n",
    "model = EcogToMotionNet()\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(X_batch)\n",
    "        loss = criterion(preds, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            preds = model(X_batch)\n",
    "            loss = criterion(preds, y_batch)\n",
    "            val_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Val.Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488148af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EcogLSTM(input_size=64, hidden_size=128, num_layers=1, output_size=3)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(X_batch)\n",
    "        loss = criterion(preds, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            preds = model(X_batch)\n",
    "            loss = criterion(preds, y_batch)\n",
    "            val_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Val.Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3795d93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs=20, early_stopping_patience=5, model_name=\"model\"):\n",
    "    model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(X_batch)\n",
    "            loss = criterion(preds, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item() * X_batch.size(0)\n",
    "        avg_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                preds = model(X_batch)\n",
    "                loss = criterion(preds, y_batch)\n",
    "                running_val_loss += loss.item() * X_batch.size(0)\n",
    "        avg_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        print(f\"{model_name} Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f}\")\n",
    "        \n",
    "        # Save best model checkpoint\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), f\"{model_name}_best.pth\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if epoch - best_epoch >= early_stopping_patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "def plot_losses(losses_dict):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    for model_name, (train_losses, val_losses) in losses_dict.items():\n",
    "        plt.plot(train_losses, label=f\"{model_name} Train\")\n",
    "        plt.plot(val_losses, label=f\"{model_name} Val\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss (MSE)\")\n",
    "    plt.title(\"Training and Validation Loss Curves\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming train_loader, val_loader, criterion are defined\n",
    "\n",
    "# 1. LSTM\n",
    "lstm_model = EcogLSTM(input_size=64, hidden_size=128, num_layers=1, output_size=3)\n",
    "lstm_optimizer = torch.optim.Adam(lstm_model.parameters(), lr=1e-3)\n",
    "lstm_train_losses, lstm_val_losses = train_model(lstm_model, train_loader, val_loader, criterion, lstm_optimizer, device, epochs=20, model_name=\"LSTM\")\n",
    "\n",
    "# 2. CNN\n",
    "cnn_model = EcogToMotionNet()\n",
    "cnn_optimizer = torch.optim.Adam(cnn_model.parameters(), lr=1e-3)\n",
    "cnn_train_losses, cnn_val_losses = train_model(cnn_model, train_loader, val_loader, criterion, cnn_optimizer, device, epochs=20, model_name=\"CNN\")\n",
    "\n",
    "# 3. Linear\n",
    "input_channels = X.shape[2]\n",
    "sequence_length = X.shape[1]\n",
    "linear_model = LinearEcogToMotionNet(input_channels, sequence_length)\n",
    "linear_optimizer = torch.optim.Adam(linear_model.parameters(), lr=1e-3)\n",
    "linear_train_losses, linear_val_losses = train_model(linear_model, train_loader, val_loader, criterion, linear_optimizer, device, epochs=20, model_name=\"Linear\")\n",
    "\n",
    "# Plot all losses together\n",
    "plot_losses({\n",
    "    \"LSTM\": (lstm_train_losses, lstm_val_losses),\n",
    "    \"CNN\": (cnn_train_losses, cnn_val_losses),\n",
    "    \"Linear\": (linear_train_losses, linear_val_losses)\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518e580c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
