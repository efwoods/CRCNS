{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "854f15c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 22:27:50.445845: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-23 22:27:50.551219: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748053670.593486  105431 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748053670.605425  105431 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748053670.703964  105431 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748053670.703983  105431 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748053670.703985  105431 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748053670.703986  105431 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-23 22:27:50.714968: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, filtfilt, iirnotch, periodogram\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import kurtosis\n",
    "from torch.utils.data import random_split\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gc\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.io import savemat\n",
    "import joblib\n",
    "from scipy.signal import hilbert\n",
    "from sklearn.decomposition import PCA\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from typing import Literal\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b74ce94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise Filters\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = fs / 2.0\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def bandpass_filter(data, lowcut=1.0, highcut=200.0, fs=1000.0, order=4):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    return filtfilt(b, a, data, axis=0)\n",
    "\n",
    "# Apply after bandpass\n",
    "def notch_filter(data, freq=60.0, fs=1000.0, quality=30.0):\n",
    "    b, a = iirnotch(freq, quality, fs)\n",
    "    return filtfilt(b, a, data, axis=0)\n",
    "\n",
    "# Noise Metrics for evaluation\n",
    "def compute_rmse(true, estimate):\n",
    "    return np.sqrt(np.mean((true - estimate) ** 2))\n",
    "\n",
    "# Kurtosis signal reduction > 0 shows a denoised signal\n",
    "def proportion_of_positive_kurtosis_signals(kurtosis_raw, kurtosis_denoised):\n",
    "    return (np.array([(kurtosis_raw - kurtosis_denoised) > 0]).sum() / len(kurtosis_raw)) * 100\n",
    "\n",
    "# Use a Standard scaler to reduce the mean to 0 and std to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8e5a040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the power envelope of each channel\n",
    "\n",
    "def band_power_envelope(ecog_signal: np.ndarray, lowcut: float, highcut: float, fs: float = 1000.0, order: int = 4) -> np.ndarray:\n",
    "    \"\"\"Computes band-limited envelope via Hilbert transform.\n",
    "    Parameters\n",
    "    ----------\n",
    "    self.ecog_signal : np.ndarray (T, channels)\n",
    "        This is the ecog signal that has been filtered.\n",
    "    lowcut : float\n",
    "        This is the lower band limit in Hz.\n",
    "    highcut : float\n",
    "        This is the upper band limit in Hz.\n",
    "    fs : float, optional\n",
    "        This is the frequency of the sample., by default 1000.0\n",
    "    order : int, optional\n",
    "        This is the Butterworth order, by default 4\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        envelope\n",
    "    \"\"\"\n",
    "    # 1. Narrowband bandpass\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    narrow = filtfilt(b, a, ecog_signal, axis=0)\n",
    "    # 2. Hilbert transform to get analytic signal\n",
    "    analytic = hilbert(narrow, axis=0)\n",
    "    # 3. Envelope = absolute value\n",
    "    envelope = np.abs(analytic)\n",
    "    return envelope\n",
    "\n",
    "def multiband_features(ecog_raw: np.ndarray, fs: float = 1000.0) -> np.ndarray:\n",
    "    \"\"\"Builds concatenated band-power features for μ, β, and high-gamma.\n",
    "    Parameters\n",
    "    ----------\n",
    "    ecog_raw : np.ndarray\n",
    "        (T, 64)\n",
    "    fs : float, optional\n",
    "        Frequency of the sample, by default 1000.0\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        features: (T, 64, 3) (μ, β, high-gamma per electrode)\n",
    "    \"\"\"\n",
    "    mu_env = band_power_envelope(ecog_raw, lowcut=8.0, highcut=13.0, fs=fs)\n",
    "    beta_env = band_power_envelope(ecog_raw, lowcut=13.0, highcut=30.0, fs=fs)\n",
    "    hg_env = band_power_envelope(ecog_raw, lowcut=70.0, highcut=200.0, fs=fs)\n",
    "    # Concatenate along channel dimension\n",
    "    return np.concatenate([mu_env, beta_env, hg_env], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e758286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_overlapping_windows(ecog_values: np.ndarray, motion_values: np.ndarray, window_size: int = 20, hop_size: int = 10):\n",
    "    \"\"\"Builds overlapping windows to increase sample count and capture smoother transitions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ecog_values : np.ndarray\n",
    "        (T, features)\n",
    "    motion_values : np.ndarray\n",
    "        (T_motion, 3)_\n",
    "    window_size : int, optional\n",
    "        number of timepoints per window, by default 20\n",
    "    hop_size : int, optional\n",
    "        step bewteen windows, by default 10\n",
    "    \"\"\"\n",
    "    num_samples, num_features = ecog_values.shape\n",
    "    max_windows = (num_samples - window_size) // hop_size + 1\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    for w in range(max_windows):\n",
    "        start = w * hop_size\n",
    "        end = start + window_size\n",
    "        if end > num_samples:\n",
    "            break\n",
    "        # Assign label as motion at center of window (or last timepoint)\n",
    "        X_list.append(ecog_values[start:end, :])\n",
    "        y_list.append(motion_values[min(end -1, motion_values.shape[0] -1), :])\n",
    "    X = np.stack(X_list, axis=0)\n",
    "    y = np.stack(y_list, axis=0)\n",
    "    return X, y        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0b78df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definitions\n",
    "class EcogMotionDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "# CNN/LSTM hybrid\n",
    "class EcogToMotionNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # CNN component: outputs 256 channels\n",
    "        self.convolv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1),  # Fixed to 256 channels\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.3)\n",
    "        )\n",
    "\n",
    "        # Bi-LSTM component (2 Layers)\n",
    "        self.lstm = nn.LSTM(input_size=256, hidden_size=128, num_layers=2, batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.attn_weight = nn.Linear(2 * 128, 1, bias=False)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(2*128, 3)  # Matches hidden_size=128\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch, 20, 64)\n",
    "        x = x.permute(0, 2, 1)  # Shape: (batch, 64, 20)\n",
    "        x = self.convolv(x)      # Shape: (batch, 256, 20)\n",
    "        x = x.permute(0, 2, 1)   # Shape: (batch, 20, 256)\n",
    "\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)  # lstm_out shape: (batch, 20, 128)\n",
    "\n",
    "        # Compute attention scores\n",
    "        # Flatten across features: attn_score[i, t] = wT * h_{i, t}\n",
    "        # Then softmax over t to get α_{i, t}\n",
    "        attn_scores = self.attn_weight(lstm_out).squeeze(-1)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=1)\n",
    "        # Weighted sum of LSTM outputs:\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(1), lstm_out).squeeze(1)\n",
    "\n",
    "        # Regression to 3D motion\n",
    "        output = self.fc(attn_applied)\n",
    "        return output\n",
    "\n",
    "# Linear Model\n",
    "class LinearEcogToMotionNet(nn.Module):\n",
    "    def __init__(self, input_channels = 64, sequence_length = 20, output_dim = 3):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_channels * sequence_length, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "# LSTM\n",
    "class EcogLSTM(nn.Module):\n",
    "    def __init__(self, input_size = 64, hidden_size = 128, num_layers = 1, output_size = 3):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x) # lstm_out shape: (batch_size, seq_len, hidden_size)\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        output = self.fc(last_output)\n",
    "        return output\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72b211e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_export(model, data_loader, device, output_file_path):\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            all_preds.append(outputs.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "    predictions = np.concatenate(all_preds, axis=0)\n",
    "    targets = np.concatenate(all_targets, axis=0)\n",
    "    \n",
    "    # Save as .mat file for visualization\n",
    "    savemat(output_file_path, {\n",
    "        \"predictions\":predictions,\n",
    "        \"targets\": targets\n",
    "    })\n",
    "    print(\"Saved predictions to ecog_predictions.mat\")\n",
    "\n",
    "    return predictions, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00512565",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessData:\n",
    "    def __init__(self, ecog_file_path, motion_file_path):\n",
    "        self.ecog_file_path = ecog_file_path\n",
    "        self.motion_file_path = motion_file_path\n",
    "        self.ecog_data = None\n",
    "        self.motion_data = None\n",
    "        self.filtered_ecog = None\n",
    "        self.scaled_ecog = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.scaler = None\n",
    "\n",
    "    def process(self, eval=False, window_size=20, duration_limit=900):\n",
    "        self.read_data()\n",
    "        self.common_average_reference()\n",
    "        self.filter_signal(eval=eval)\n",
    "        self.format_data(window_size=window_size, duration_limit=duration_limit)\n",
    "        return self.X, self.y\n",
    "    \n",
    "    def read_data(self):\n",
    "        self.ecog_data = pd.read_csv(self.ecog_file_path)\n",
    "        self.motion_data = pd.read_csv(self.motion_file_path)\n",
    "        return self\n",
    "\n",
    "    def common_average_reference(self):\n",
    "        # Subtract the common mean from the signals \n",
    "        common_average_reference = np.mean(self.ecog_data.drop([\"Time\", \"Fs\"], axis=1).values, axis=1, keepdims=1)\n",
    "        ecog_data_values = self.ecog_data[self.ecog_data.columns[1:-1]].values\n",
    "        ecog_data_common_mean_subtracted = ecog_data_values - common_average_reference\n",
    "        self.ecog_data[self.ecog_data.columns[1:-1]] = ecog_data_common_mean_subtracted\n",
    "        del ecog_data_values, ecog_data_common_mean_subtracted, common_average_reference\n",
    "        gc.collect()\n",
    "        return self\n",
    "\n",
    "    def filter_signal(self, eval=False):\n",
    "        ecog_raw = self.ecog_data[self.ecog_data.columns[1:-1]].values\n",
    "\n",
    "        # Apply filters\n",
    "        filtered = bandpass_filter(ecog_raw, lowcut=1.0, highcut=200.0, fs=1000.0, order=4)\n",
    "        denoised = notch_filter(filtered, freq=60, fs=1000.0)\n",
    "\n",
    "        # Evaluate filters\n",
    "        if eval:\n",
    "            kurt_raw = kurtosis(ecog_raw, axis=0, fisher=True)\n",
    "            kurt_denoised = kurtosis(denoised, axis=0, fisher=True)\n",
    "            proportion_of_positive_kurtosis_signals(kurt_raw, kurt_denoised)\n",
    "            compute_rmse(ecog_raw, denoised)\n",
    "\n",
    "        # Compute Power Envelopes\n",
    "        features = multiband_features(denoised, fs=1000.0) # shape (T, 192)\n",
    "\n",
    "        # Identify the principal components of the network\n",
    "        pca = PCA(n_components = 64, random_state=42)\n",
    "        reduced = pca.fit_transform(features)\n",
    "\n",
    "        # Scale\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaled_ecog = self.scaler.fit_transform(reduced)\n",
    "\n",
    "        # Replace in DataFrame\n",
    "        self.ecog_data = self.ecog_data.copy()\n",
    "        self.ecog_data[self.ecog_data.columns[1:-1]] = self.scaled_ecog\n",
    "\n",
    "        # Clean memory\n",
    "        del ecog_raw, filtered, denoised\n",
    "        gc.collect()\n",
    "        return self\n",
    "\n",
    "    def format_data(self, window_size=20, duration_limit=900):\n",
    "        ecog_df = self.ecog_data[self.ecog_data[\"Time\"] <= duration_limit]\n",
    "        motion_df = self.motion_data[self.motion_data[\"Motion_time\"] <= duration_limit]\n",
    "\n",
    "        ecog_values = ecog_df.drop(columns=[\"Fs\", \"Time\"]).values\n",
    "        motion_values = motion_df.drop(columns=[\"Fsm\", \"Motion_time\"]).values\n",
    "\n",
    "        print(f\"motion_values.shape: {motion_values.shape}\")\n",
    "\n",
    "        # Smooth the signal\n",
    "        X, y = create_overlapping_windows(ecog_values, motion_values, window_size=20, hop_size=10)\n",
    "        print(f\"y.shape: {y.shape}\")\n",
    "        self.X, self.y = X, y\n",
    "        \n",
    "        print(self.X.shape)\n",
    "        print(self.y.shape)\n",
    "        \n",
    "        # Clean up\n",
    "        del ecog_values, motion_values\n",
    "        gc.collect()\n",
    "\n",
    "    def save(self):\n",
    "        output_file_path_base = self.ecog_file_path.strip(\"ecog_data.csv\")\n",
    "        joblib.dump(self.scaler, output_file_path_base + \"scaler_ecog.pkl\")\n",
    "        np.save(output_file_path_base + \"X.npy\", self.X)\n",
    "        np.save(output_file_path_base + \"y.npy\", self.y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a59faa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, device, train_loader, val_loader=None, epochs=20, model_name=\"model\", example_input=torch.rand(1,20,64), checkpoint_dir=\"models/\"):\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=5)\n",
    "    writer = SummaryWriter(log_dir='runs/' + model_name)\n",
    "    best_val_loss = float('inf')\n",
    "    early_stop_counter = 0\n",
    "    patience = 10 # epochs\n",
    "    \n",
    "    # Add the model graph to TensorBoard using example_input\n",
    "    if example_input is not None:\n",
    "        writer.add_graph(model, example_input.to(device))\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    r2_scores = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(X_batch)\n",
    "            loss = criterion(preds, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item() * X_batch.size(0)\n",
    "        avg_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        writer.add_scalar(\"Loss/Train\", avg_train_loss, epoch)\n",
    "        writer.add_scalar(\"Learning Rate\", optimizer.param_groups[0]['lr'], epoch)\n",
    "            \n",
    "        if val_loader is not None:\n",
    "            # Validate\n",
    "            model.eval()\n",
    "            running_val_loss = 0.0\n",
    "            all_preds = []\n",
    "            all_targets = []\n",
    "            with torch.no_grad():\n",
    "                for X_batch, y_batch in val_loader:\n",
    "                    X_batch = X_batch.to(device)\n",
    "                    y_batch = y_batch.to(device)\n",
    "                    preds = model(X_batch)\n",
    "                    loss = criterion(preds, y_batch)\n",
    "                    running_val_loss += loss.item() * X_batch.size(0)\n",
    "                    all_preds.append(preds.cpu())\n",
    "                    all_targets.append(y_batch.cpu())\n",
    "            all_preds = torch.cat(all_preds).numpy()\n",
    "            all_targets = torch.cat(all_targets).numpy()\n",
    "            r2 = r2_score(all_targets, all_preds)\n",
    "            r2_scores.append(r2)\n",
    "            avg_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "            val_losses.append(avg_val_loss)\n",
    "            \n",
    "            # Log to TensorBoard\n",
    "            \n",
    "            writer.add_scalar(\"Loss/Validation\", avg_val_loss, epoch)\n",
    "            writer.add_scalar(\"R2/Validation\", r2, epoch)\n",
    "            \n",
    "\n",
    "            print(f\"{model_name} Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f} | R2: {r2:.6f}\")\n",
    "\n",
    "            scheduler.step(avg_val_loss)\n",
    "\n",
    "            # Save best model checkpoint\n",
    "            if avg_val_loss < best_val_loss - 1e-5:\n",
    "                best_val_loss = avg_val_loss\n",
    "                early_stop_counter = 0\n",
    "                print(f\"Model Checkpoint | epoch: {epoch} | best_val_loss: {best_val_loss}\")\n",
    "                torch.save(model.state_dict(), checkpoint_dir + model_name + \".pth\")\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "                if early_stop_counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "    \n",
    "    writer.close()\n",
    "    return train_losses, val_losses, r2_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e37a251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(losses_dict):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    for model_name, (train_losses, val_losses) in losses_dict.items():\n",
    "        plt.plot(train_losses, label=f\"{model_name} Train\")\n",
    "        plt.plot(val_losses, label=f\"{model_name} Val\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss (MSE)\")\n",
    "    plt.title(\"Training and Validation Loss Curves\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "283cc29d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/linux-pc/gh/CRCNS/src/motor_cortex/data/'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(os.getcwd(), \"src/\", \"motor_cortex/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6729e9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_data_file_l = glob(os.path.join(os.getcwd(), \"src/\", \"motor_cortex/data/data/\", \"**\", \"motion*.csv\"), recursive=True)\n",
    "ecog_data_file_l = glob(os.path.join(os.getcwd(), \"src/\", \"motor_cortex/data/data/\", \"**\", \"ecog*.csv\"), recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fbf47eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(motion_data_file_l[9])\n",
    "# df.drop(columns=[\"Left_Wrist_X.1\", \"Left_Wrist_Y.1\", \"Left_Wrist_Z.1\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fafc523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create zero column data for ipsilateral data (right-wrist, left is zero)\n",
    "# for index in range(6, 16):\n",
    "#     current_motion_data_file = motion_data_file_l[index]\n",
    "#     current_motion_data_file_df = pd.read_csv(current_motion_data_file)\n",
    "#     current_motion_data_file_df_update = create_six_motion_outputs_for_df(current_motion_data_file_df, wrist=\"RIGHT\")\n",
    "#     current_motion_data_file_df_update.to_csv(current_motion_data_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a265972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create zero column data for contralateral data (left-wrist, right is zero)\n",
    "# for index in range(16, 26):\n",
    "#     current_motion_data_file = motion_data_file_l[index]\n",
    "#     current_motion_data_file_df = pd.read_csv(current_motion_data_file)\n",
    "#     current_motion_data_file_df_update = create_six_motion_outputs_for_df(current_motion_data_file_df, wrist=\"LEFT\")\n",
    "#     current_motion_data_file_df_update.to_csv(current_motion_data_file, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c26f274",
   "metadata": {},
   "source": [
    "# Reading only the Right Wrist data and training to detect depending on the Right Wrist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc74d3e4",
   "metadata": {},
   "source": [
    "## Bilateral Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff3bfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_data_file_l[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907f36ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_data_file = motion_data_file_l[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a5a011",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecog_data_file = ecog_data_file_l[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07bc434",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfec741",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecog_data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b3a041",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_data_bilateral_2018_07_12_S1 = pd.read_csv(motion_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d41fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_data_bilateral_2018_07_12_S1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafb318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecog_data_bilateral_2018_07_12_S1 = pd.read_csv(ecog_data_file)\n",
    "channel_data = ecog_data_bilateral_2018_07_12_S1.columns[1:-1].values\n",
    "ecog_data_bilateral_2018_07_12_S1[channel_data].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167a762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(motion_data_bilateral_2018_07_12_S1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4ad7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecog_data_bilateral_2018_07_12_S1[\"Time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d811d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ecog_data_bilateral_2018_07_12_S1[\"Time\"], ecog_data_bilateral_2018_07_12_S1[channel_data].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ef02ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecog_channels = ecog_data_bilateral_2018_07_12_S1.columns[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7fda74",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecog_data_bilateral_2018_07_12_S1[ecog_channels].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806049ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_data_bilateral_2018_07_12_S1_left_wrist = motion_data_bilateral_2018_07_12_S1[motion_data_bilateral_2018_07_12_S1.columns[1:4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e925d624",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_data_bilateral_2018_07_12_S1_left_wrist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a05342",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_data_bilateral_2018_07_12_S1_left_wrist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8b2f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecog_data_bilateral_2018_07_12_S1[channel_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da53d06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del ecog_data_bilateral_2018_07_12_S1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e491bb",
   "metadata": {},
   "source": [
    "## Ipsilateral Data (Right Wrist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cba513d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf2de551",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_ecog_data_file = ecog_data_file_l[INDEX]\n",
    "current_motion_data_file = motion_data_file_l[INDEX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d958a0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(motion_data_file_l[INDEX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3cc6242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fsm</th>\n",
       "      <th>Left_Wrist_X</th>\n",
       "      <th>Left_Wrist_Y</th>\n",
       "      <th>Left_Wrist_Z</th>\n",
       "      <th>Motion_time</th>\n",
       "      <th>Right_Wrist_X</th>\n",
       "      <th>Right_Wrist_Y</th>\n",
       "      <th>Right_Wrist_Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.401104</td>\n",
       "      <td>0.230454</td>\n",
       "      <td>-0.160554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.406948</td>\n",
       "      <td>0.233907</td>\n",
       "      <td>-0.160399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.402722</td>\n",
       "      <td>0.231508</td>\n",
       "      <td>-0.160204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.059</td>\n",
       "      <td>-0.412511</td>\n",
       "      <td>0.239396</td>\n",
       "      <td>-0.159490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.418708</td>\n",
       "      <td>0.244420</td>\n",
       "      <td>-0.159540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44981</th>\n",
       "      <td>50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>899.903</td>\n",
       "      <td>-0.600712</td>\n",
       "      <td>0.416428</td>\n",
       "      <td>-0.260001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44982</th>\n",
       "      <td>50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>899.923</td>\n",
       "      <td>-0.599108</td>\n",
       "      <td>0.415988</td>\n",
       "      <td>-0.260355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44983</th>\n",
       "      <td>50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>899.943</td>\n",
       "      <td>-0.604163</td>\n",
       "      <td>0.419191</td>\n",
       "      <td>-0.259465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44984</th>\n",
       "      <td>50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>899.963</td>\n",
       "      <td>-0.602424</td>\n",
       "      <td>0.418739</td>\n",
       "      <td>-0.259521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44985</th>\n",
       "      <td>50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>899.983</td>\n",
       "      <td>-0.606870</td>\n",
       "      <td>0.420352</td>\n",
       "      <td>-0.259157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44986 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Fsm  Left_Wrist_X  Left_Wrist_Y  Left_Wrist_Z  Motion_time  \\\n",
       "0       50           0.0           0.0           0.0        0.000   \n",
       "1       50           0.0           0.0           0.0        0.019   \n",
       "2       50           0.0           0.0           0.0        0.040   \n",
       "3       50           0.0           0.0           0.0        0.059   \n",
       "4       50           0.0           0.0           0.0        0.080   \n",
       "...    ...           ...           ...           ...          ...   \n",
       "44981   50           0.0           0.0           0.0      899.903   \n",
       "44982   50           0.0           0.0           0.0      899.923   \n",
       "44983   50           0.0           0.0           0.0      899.943   \n",
       "44984   50           0.0           0.0           0.0      899.963   \n",
       "44985   50           0.0           0.0           0.0      899.983   \n",
       "\n",
       "       Right_Wrist_X  Right_Wrist_Y  Right_Wrist_Z  \n",
       "0          -0.401104       0.230454      -0.160554  \n",
       "1          -0.406948       0.233907      -0.160399  \n",
       "2          -0.402722       0.231508      -0.160204  \n",
       "3          -0.412511       0.239396      -0.159490  \n",
       "4          -0.418708       0.244420      -0.159540  \n",
       "...              ...            ...            ...  \n",
       "44981      -0.600712       0.416428      -0.260001  \n",
       "44982      -0.599108       0.415988      -0.260355  \n",
       "44983      -0.604163       0.419191      -0.259465  \n",
       "44984      -0.602424       0.418739      -0.259521  \n",
       "44985      -0.606870       0.420352      -0.259157  \n",
       "\n",
       "[44986 rows x 8 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"Motion_time\"] <= 900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067b8609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ecog_data = pd.read_csv(current_ecog_data_file)\n",
    "# common_average_reference = np.mean(ecog_data.drop([\"Time\", \"Fs\"], axis=1).values, axis=1, keepdims=1)\n",
    "# # ecog_data[ecog_data[1:-1]\n",
    "# ecog_data_common_mean_subtracted = ecog_data_values - common_average_reference\n",
    "# ecog_data[ecog_data.columns[1:-1]] -= np.mean(ecog_data.drop([\"Time\", \"Fs\"], axis=1).values, axis=1, keepdims=1)\n",
    "# np.mean(ecog_data.drop([\"Time\", \"Fs\"], axis=1).values, axis=1, keepdims=1)\n",
    "# ecog_data -= np.mean(ecog_data.drop(columns=[\"Time\", \"Fs\"]), axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9624cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = PreprocessData(current_ecog_data_file, current_motion_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f953563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "motion_values.shape: (44986, 6)\n",
      "y.shape: (89999, 6)\n",
      "(89999, 20, 64)\n",
      "(89999, 6)\n",
      "         502540 function calls (501267 primitive calls) in 52.986 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "   List reduced from 803 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        6   34.133    5.689   34.133    5.689 {built-in method scipy.fft._pocketfft.pypocketfft.c2c}\n",
      "       10   10.938    1.094   10.938    1.094 {built-in method scipy.signal._sigtools._linear_filter}\n",
      "        2    2.673    1.336    2.813    1.406 /home/linux-pc/anaconda3/envs/torch/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:222(read)\n",
      "        3    0.547    0.182   34.684   11.561 /home/linux-pc/anaconda3/envs/torch/lib/python3.10/site-packages/scipy/signal/_signaltools.py:2318(hilbert)\n",
      "      209    0.524    0.003    0.526    0.003 /home/linux-pc/anaconda3/envs/torch/lib/python3.10/site-packages/pandas/core/array_algos/take.py:120(_take_nd_ndarray)\n",
      "      350    0.429    0.001    0.429    0.001 {method 'copy' of 'numpy.ndarray' objects}\n",
      "        5    0.398    0.080    0.398    0.080 /home/linux-pc/anaconda3/envs/torch/lib/python3.10/site-packages/scipy/signal/_arraytools.py:57(odd_ext)\n",
      "        3    0.389    0.130   42.482   14.161 /tmp/ipykernel_41080/3697014144.py:3(band_power_envelope)\n",
      "      329    0.356    0.001    0.356    0.001 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "        1    0.337    0.337    0.337    0.337 /home/linux-pc/anaconda3/envs/torch/lib/python3.10/site-packages/sklearn/decomposition/_base.py:147(_transform)\n",
      "        3    0.333    0.111    0.333    0.111 {built-in method gc.collect}\n",
      "        1    0.322    0.322   42.804   42.804 /tmp/ipykernel_41080/3697014144.py:31(multiband_features)\n",
      "        1    0.229    0.229    0.379    0.379 /home/linux-pc/anaconda3/envs/torch/lib/python3.10/site-packages/sklearn/decomposition/_pca.py:546(_fit_full)\n",
      "        2    0.176    0.088    0.213    0.107 /home/linux-pc/anaconda3/envs/torch/lib/python3.10/site-packages/numpy/_core/shape_base.py:380(stack)\n",
      "        2    0.157    0.078    0.157    0.079 /home/linux-pc/anaconda3/envs/torch/lib/python3.10/site-packages/pandas/core/internals/managers.py:1707(_interleave)\n",
      "        1    0.154    0.154    0.272    0.272 /home/linux-pc/anaconda3/envs/torch/lib/python3.10/site-packages/sklearn/utils/extmath.py:1021(_incremental_mean_and_var)\n",
      "       74    0.106    0.001    0.107    0.001 /home/linux-pc/anaconda3/envs/torch/lib/python3.10/site-packages/pandas/core/dtypes/concat.py:52(concat_compat)\n",
      "        1    0.094    0.094    0.468    0.468 /tmp/ipykernel_41080/3393990963.py:25(common_average_reference)\n",
      "        1    0.086    0.086    0.176    0.176 /home/linux-pc/anaconda3/envs/torch/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:1044(transform)\n",
      "        4    0.082    0.021    0.082    0.021 /home/linux-pc/anaconda3/envs/torch/lib/python3.10/site-packages/pandas/core/internals/managers.py:2246(_stack_arrays)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x7faee4693a90>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "with cProfile.Profile() as pr:\n",
    "    preprocessor = PreprocessData(current_ecog_data_file, current_motion_data_file)\n",
    "    X, y = preprocessor.process()\n",
    "stats = pstats.Stats(pr)\n",
    "stats.sort_stats(pstats.SortKey.TIME).print_stats(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dacdec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2f6abb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "motion_values.shape: (44986, 6)\n",
      "y.shape: (89999, 6)\n",
      "(89999, 20, 64)\n",
      "(89999, 6)\n"
     ]
    }
   ],
   "source": [
    "preprocessor = PreprocessData(current_ecog_data_file, current_motion_data_file)\n",
    "X, y = preprocessor.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3f71d5d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(920498, 66)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(current_ecog_data_file).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b35a6b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89999, 20, 64)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f56237f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89999, 6)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8fc6207d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46227, 8)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(current_motion_data_file).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ac0622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "motion_values.shape: (44983, 3)\n",
      "y.shape: (89999, 3)\n",
      "(89999, 20, 64)\n",
      "(89999, 3)\n",
      "motion_values.shape: (44989, 3)\n",
      "y.shape: (89999, 3)\n",
      "(89999, 20, 64)\n",
      "(89999, 3)\n",
      "motion_values.shape: (44982, 3)\n",
      "y.shape: (89999, 3)\n",
      "(89999, 20, 64)\n",
      "(89999, 3)\n",
      "motion_values.shape: (44990, 3)\n",
      "y.shape: (89999, 3)\n",
      "(89999, 20, 64)\n",
      "(89999, 3)\n",
      "motion_values.shape: (44991, 3)\n",
      "y.shape: (89999, 3)\n",
      "(89999, 20, 64)\n",
      "(89999, 3)\n",
      "motion_values.shape: (44980, 3)\n",
      "y.shape: (89999, 3)\n",
      "(89999, 20, 64)\n",
      "(89999, 3)\n",
      "motion_values.shape: (44986, 3)\n",
      "y.shape: (89999, 3)\n",
      "(89999, 20, 64)\n",
      "(89999, 3)\n",
      "motion_values.shape: (44991, 3)\n",
      "y.shape: (89999, 3)\n",
      "(89999, 20, 64)\n",
      "(89999, 3)\n",
      "motion_values.shape: (44989, 3)\n",
      "y.shape: (89999, 3)\n",
      "(89999, 20, 64)\n",
      "(89999, 3)\n",
      "motion_values.shape: (44984, 3)\n",
      "y.shape: (89999, 3)\n",
      "(89999, 20, 64)\n",
      "(89999, 3)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess Ipsilateral Data\n",
    "# for index in range(6, 16):\n",
    "#     preprocessor = PreprocessData(ecog_data_file_l[index], motion_data_file_l[index])\n",
    "#     X, y = preprocessor.process()\n",
    "#     preprocessor.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddf91ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecog_data_file_l[16:ecog_data_file_l.__len__()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae28952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "motion_values.shape: (44992, 3)\n",
      "y.shape: (89999, 3)\n",
      "(89999, 20, 64)\n",
      "(89999, 3)\n",
      "motion_values.shape: (44978, 3)\n",
      "y.shape: (89999, 3)\n",
      "(89999, 20, 64)\n",
      "(89999, 3)\n",
      "motion_values.shape: (44990, 3)\n",
      "y.shape: (89999, 3)\n",
      "(89999, 20, 64)\n",
      "(89999, 3)\n",
      "motion_values.shape: (44987, 3)\n",
      "y.shape: (89999, 3)\n",
      "(89999, 20, 64)\n",
      "(89999, 3)\n",
      "motion_values.shape: (44985, 3)\n",
      "y.shape: (89999, 3)\n",
      "(89999, 20, 64)\n",
      "(89999, 3)\n",
      "motion_values.shape: (44989, 3)\n",
      "y.shape: (89999, 3)\n",
      "(89999, 20, 64)\n",
      "(89999, 3)\n",
      "motion_values.shape: (44990, 3)\n",
      "y.shape: (89999, 3)\n",
      "(89999, 20, 64)\n",
      "(89999, 3)\n",
      "motion_values.shape: (44991, 3)\n",
      "y.shape: (89999, 3)\n",
      "(89999, 20, 64)\n",
      "(89999, 3)\n",
      "motion_values.shape: (44990, 3)\n",
      "y.shape: (89999, 3)\n",
      "(89999, 20, 64)\n",
      "(89999, 3)\n",
      "motion_values.shape: (44988, 3)\n",
      "y.shape: (89999, 3)\n",
      "(89999, 20, 64)\n",
      "(89999, 3)\n"
     ]
    }
   ],
   "source": [
    "# # Preprocess Contralateral Data\n",
    "# for index in range(16, ecog_data_file_l.__len__()):\n",
    "#     preprocessor = PreprocessData(ecog_data_file_l[index], motion_data_file_l[index])\n",
    "#     X, y = preprocessor.process()\n",
    "#     preprocessor.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72db258b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "processed_data_l_X = glob(os.path.join('/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/', '**', \"**\", \"X.npy\"))\n",
    "processed_data_l_y = glob(os.path.join('/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/', '**', \"**\", \"y.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88d61567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S7)/X.npy',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-24_(S10)/X.npy',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S1)/X.npy',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-06_(S6)/X.npy',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S5)/X.npy',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S3)/X.npy',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S4)/X.npy',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S8)/X.npy',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-17_(S9)/X.npy',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S2)/X.npy',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-03-15_(S1)/X.npy',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S3)/X.npy',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-08_(S2)/X.npy',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S7)/X.npy',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S4)/X.npy',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S9)/X.npy',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S5)/X.npy',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S8)/X.npy',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-05-31_(S10)/X.npy',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-15_(S6)/X.npy']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data_l_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "748c0cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S7)/y.npy',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-24_(S10)/y.npy',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S1)/y.npy',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-06_(S6)/y.npy',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S5)/y.npy',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S3)/y.npy',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S4)/y.npy',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S8)/y.npy',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-17_(S9)/y.npy',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S2)/y.npy',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-03-15_(S1)/y.npy',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S3)/y.npy',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-08_(S2)/y.npy',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S7)/y.npy',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S4)/y.npy',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S9)/y.npy',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S5)/y.npy',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S8)/y.npy',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-05-31_(S10)/y.npy',\n",
       " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-15_(S6)/y.npy']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data_l_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ee63e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform K-Fold Cross Validation\n",
    "# iterator = iter(processed_data_l)\n",
    "\n",
    "# for X, y in zip(iterator, iterator):\n",
    "#     print(X)\n",
    "#     print(y)\n",
    "\n",
    "# Create k-fold cross validation\n",
    "# select the best model\n",
    "# make a prediction\n",
    "# visualize the predictions in matlab\n",
    "# create a live demo\n",
    "# deploy demo onto the web\n",
    "# share results for testing with real people\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90f54ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define K-fold sets\n",
    "test_list_X = []\n",
    "train_list_X = []\n",
    "test_list_y = []\n",
    "train_list_y = []\n",
    "\n",
    "for i in range(len(processed_data_l_X)):\n",
    "    test_list_X.append(processed_data_l_X[i])\n",
    "    test_list_y.append(processed_data_l_y[i])\n",
    "    train_X = [x for idx, x in enumerate(processed_data_l_X) if idx != i]\n",
    "    train_y = [y for idx, y in enumerate(processed_data_l_y) if idx != i]\n",
    "    train_list_X.append(train_X)\n",
    "    train_list_y.append(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c45412d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4dc06c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(train_list_X[INDEX][0])\n",
    "y = np.load(train_list_y[INDEX][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3214fb99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-24_(S10)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S1)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-06_(S6)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S5)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S3)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S4)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S8)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-17_(S9)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S2)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-03-15_(S1)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S3)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-08_(S2)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S7)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S4)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S9)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S5)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S8)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-05-31_(S10)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-15_(S6)/X.npy'],\n",
       " ['/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S7)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S1)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-06_(S6)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S5)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S3)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S4)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S8)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-17_(S9)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S2)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-03-15_(S1)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S3)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-08_(S2)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S7)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S4)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S9)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S5)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S8)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-05-31_(S10)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-15_(S6)/X.npy'],\n",
       " ['/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S7)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-24_(S10)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-06_(S6)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S5)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S3)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S4)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S8)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-17_(S9)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S2)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-03-15_(S1)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S3)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-08_(S2)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S7)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S4)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S9)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S5)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S8)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-05-31_(S10)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-15_(S6)/X.npy'],\n",
       " ['/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S7)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-24_(S10)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S1)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S5)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S3)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S4)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S8)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-17_(S9)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S2)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-03-15_(S1)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S3)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-08_(S2)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S7)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S4)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S9)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S5)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S8)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-05-31_(S10)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-15_(S6)/X.npy'],\n",
       " ['/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S7)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-24_(S10)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S1)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-06_(S6)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S3)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S4)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S8)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-17_(S9)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S2)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-03-15_(S1)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S3)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-08_(S2)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S7)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S4)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S9)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S5)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S8)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-05-31_(S10)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-15_(S6)/X.npy'],\n",
       " ['/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S7)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-24_(S10)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S1)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-06_(S6)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S5)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S4)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S8)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-17_(S9)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S2)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-03-15_(S1)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S3)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-08_(S2)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S7)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S4)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S9)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S5)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S8)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-05-31_(S10)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-15_(S6)/X.npy'],\n",
       " ['/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S7)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-24_(S10)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S1)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-06_(S6)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S5)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S3)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S8)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-17_(S9)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S2)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-03-15_(S1)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S3)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-08_(S2)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S7)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S4)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S9)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S5)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S8)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-05-31_(S10)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-15_(S6)/X.npy'],\n",
       " ['/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S7)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-24_(S10)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S1)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-06_(S6)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S5)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S3)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S4)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-17_(S9)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S2)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-03-15_(S1)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S3)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-08_(S2)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S7)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S4)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S9)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S5)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S8)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-05-31_(S10)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-15_(S6)/X.npy'],\n",
       " ['/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S7)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-24_(S10)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S1)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-06_(S6)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S5)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S3)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S4)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S8)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S2)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-03-15_(S1)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S3)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-08_(S2)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S7)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S4)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S9)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S5)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S8)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-05-31_(S10)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-15_(S6)/X.npy'],\n",
       " ['/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S7)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-24_(S10)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S1)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-06_(S6)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S5)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S3)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S4)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S8)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-17_(S9)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-03-15_(S1)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S3)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-08_(S2)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S7)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S4)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S9)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S5)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S8)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-05-31_(S10)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-15_(S6)/X.npy'],\n",
       " ['/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S7)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-24_(S10)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S1)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-06_(S6)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S5)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S3)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S4)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S8)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-17_(S9)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S2)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S3)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-08_(S2)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S7)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S4)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S9)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S5)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S8)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-05-31_(S10)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-15_(S6)/X.npy'],\n",
       " ['/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S7)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-24_(S10)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S1)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-06_(S6)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S5)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S3)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S4)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S8)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-17_(S9)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S2)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-03-15_(S1)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-08_(S2)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S7)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S4)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S9)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S5)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S8)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-05-31_(S10)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-15_(S6)/X.npy'],\n",
       " ['/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S7)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-24_(S10)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S1)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-06_(S6)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S5)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S3)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S4)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S8)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-17_(S9)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S2)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-03-15_(S1)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S3)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S7)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S4)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S9)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S5)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S8)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-05-31_(S10)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-15_(S6)/X.npy'],\n",
       " ['/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S7)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-24_(S10)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S1)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-06_(S6)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S5)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S3)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S4)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S8)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-17_(S9)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S2)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-03-15_(S1)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S3)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-08_(S2)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S4)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S9)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S5)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S8)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-05-31_(S10)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-15_(S6)/X.npy'],\n",
       " ['/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S7)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-24_(S10)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S1)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-06_(S6)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S5)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S3)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S4)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S8)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-17_(S9)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S2)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-03-15_(S1)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S3)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-08_(S2)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S7)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S9)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S5)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S8)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-05-31_(S10)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-15_(S6)/X.npy'],\n",
       " ['/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S7)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-24_(S10)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S1)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-06_(S6)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S5)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S3)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S4)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S8)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-17_(S9)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S2)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-03-15_(S1)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S3)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-08_(S2)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S7)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S4)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S5)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S8)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-05-31_(S10)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-15_(S6)/X.npy'],\n",
       " ['/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S7)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-24_(S10)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S1)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-06_(S6)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S5)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S3)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S4)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S8)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-17_(S9)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S2)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-03-15_(S1)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S3)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-08_(S2)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S7)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S4)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S9)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S8)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-05-31_(S10)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-15_(S6)/X.npy'],\n",
       " ['/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S7)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-24_(S10)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S1)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-06_(S6)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S5)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S3)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S4)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S8)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-17_(S9)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S2)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-03-15_(S1)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S3)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-08_(S2)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S7)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S4)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S9)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S5)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-05-31_(S10)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-15_(S6)/X.npy'],\n",
       " ['/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S7)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-24_(S10)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S1)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-06_(S6)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S5)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S3)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S4)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S8)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-17_(S9)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S2)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-03-15_(S1)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S3)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-08_(S2)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S7)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S4)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S9)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S5)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S8)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-15_(S6)/X.npy'],\n",
       " ['/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S7)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-24_(S10)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S1)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-06_(S6)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S5)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S3)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S4)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S8)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-17_(S9)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S2)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-03-15_(S1)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S3)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-08_(S2)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S7)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S4)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S9)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-12_(S5)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-04-22_(S8)/X.npy',\n",
       "  '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-05-31_(S10)/X.npy']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_list_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa744ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a41f623",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed2b381",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480cc51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Train and Validation Sets\n",
    "dataset = EcogMotionDataset(X, y)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066e8641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model\n",
    "\n",
    "# Train Linear Model\n",
    "input_channels = X.shape[2]\n",
    "sequence_length = X.shape[1]\n",
    "model = LinearEcogToMotionNet(input_channels, sequence_length)\n",
    "\n",
    "# Train 1D CNN\n",
    "# model = EcogToMotionNet()\n",
    "\n",
    "# Train LSTM\n",
    "# model = EcogLSTM(input_size=64, hidden_size=128, num_layers=1, output_size=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b6a2ff",
   "metadata": {},
   "source": [
    "## Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc6a8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Model \n",
    "\n",
    "# Train Linear Model\n",
    "input_channels = X.shape[2]\n",
    "sequence_length = X.shape[1]\n",
    "linear_model = LinearEcogToMotionNet(input_channels, sequence_length)\n",
    "\n",
    "linear_model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(linear_model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(20):\n",
    "    linear_model.train()\n",
    "    train_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = linear_model(X_batch)\n",
    "        loss = criterion(preds, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * X_batch.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    linear_model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            preds = linear_model(X_batch)\n",
    "            loss = criterion(preds, y_batch)\n",
    "            val_loss += loss.item() * X_batch.size(0)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Val.Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7440c29",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c143e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 1D CNN\n",
    "cnn_model = EcogToMotionNet()\n",
    "\n",
    "cnn_model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(cnn_model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(20):\n",
    "    cnn_model.train()\n",
    "    train_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = cnn_model(X_batch)\n",
    "        loss = criterion(preds, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * X_batch.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    cnn_model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            preds = cnn_model(X_batch)\n",
    "            loss = criterion(preds, y_batch)\n",
    "            val_loss += loss.item() * X_batch.size(0)\n",
    "        val_loss /= len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Val.Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38fc935",
   "metadata": {},
   "source": [
    "## Long Short-Term Memory Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488148af",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = EcogLSTM(input_size=64, hidden_size=128, num_layers=1, output_size=3)\n",
    "lstm_model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(20):\n",
    "    lstm_model.train()\n",
    "    train_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = lstm_model(X_batch)\n",
    "        loss = criterion(preds, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * X_batch.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    lstm_model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            preds = lstm_model(X_batch)\n",
    "            loss = criterion(preds, y_batch)\n",
    "            val_loss += loss.item() * X_batch.size(0)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Val.Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d28eef",
   "metadata": {},
   "source": [
    "# Refined Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f3dec8",
   "metadata": {},
   "source": [
    "### Single Session Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a16d254c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 1/100 | Train Loss: 0.053915 | Val Loss: 0.046009 | R2: 0.019645\n",
      "Model Checkpoint | epoch: 0 | best_val_loss: 0.046008632621520926\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 2/100 | Train Loss: 0.051276 | Val Loss: 0.044893 | R2: 0.043653\n",
      "Model Checkpoint | epoch: 1 | best_val_loss: 0.04489347511278983\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 3/100 | Train Loss: 0.047520 | Val Loss: 0.045381 | R2: 0.033413\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 4/100 | Train Loss: 0.043741 | Val Loss: 0.048289 | R2: -0.031997\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 5/100 | Train Loss: 0.038546 | Val Loss: 0.034776 | R2: 0.262292\n",
      "Model Checkpoint | epoch: 4 | best_val_loss: 0.03477576765102438\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 6/100 | Train Loss: 0.035500 | Val Loss: 0.033362 | R2: 0.292818\n",
      "Model Checkpoint | epoch: 5 | best_val_loss: 0.03336158778186008\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 7/100 | Train Loss: 0.033531 | Val Loss: 0.032116 | R2: 0.318890\n",
      "Model Checkpoint | epoch: 6 | best_val_loss: 0.032116111922957415\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 8/100 | Train Loss: 0.031009 | Val Loss: 0.028821 | R2: 0.390921\n",
      "Model Checkpoint | epoch: 7 | best_val_loss: 0.02882106686530622\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 9/100 | Train Loss: 0.029740 | Val Loss: 0.027330 | R2: 0.422009\n",
      "Model Checkpoint | epoch: 8 | best_val_loss: 0.027330053940362833\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 10/100 | Train Loss: 0.028627 | Val Loss: 0.031180 | R2: 0.339099\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 11/100 | Train Loss: 0.025652 | Val Loss: 0.032028 | R2: 0.318579\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 12/100 | Train Loss: 0.020358 | Val Loss: 0.026196 | R2: 0.444352\n",
      "Model Checkpoint | epoch: 11 | best_val_loss: 0.026196407163696776\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 13/100 | Train Loss: 0.020990 | Val Loss: 0.021717 | R2: 0.539585\n",
      "Model Checkpoint | epoch: 12 | best_val_loss: 0.021717279565248947\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 14/100 | Train Loss: 0.019852 | Val Loss: 0.025258 | R2: 0.463489\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 15/100 | Train Loss: 0.017230 | Val Loss: 0.022800 | R2: 0.516531\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 16/100 | Train Loss: 0.017400 | Val Loss: 0.023535 | R2: 0.499583\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 17/100 | Train Loss: 0.016727 | Val Loss: 0.021574 | R2: 0.541773\n",
      "Model Checkpoint | epoch: 16 | best_val_loss: 0.021574416833179486\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 18/100 | Train Loss: 0.014602 | Val Loss: 0.022487 | R2: 0.523060\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 19/100 | Train Loss: 0.014644 | Val Loss: 0.021899 | R2: 0.535376\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 20/100 | Train Loss: 0.015881 | Val Loss: 0.023078 | R2: 0.510147\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 21/100 | Train Loss: 0.015772 | Val Loss: 0.021301 | R2: 0.547994\n",
      "Model Checkpoint | epoch: 20 | best_val_loss: 0.02130138291845813\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 22/100 | Train Loss: 0.013963 | Val Loss: 0.020735 | R2: 0.560356\n",
      "Model Checkpoint | epoch: 21 | best_val_loss: 0.020734999553779037\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 23/100 | Train Loss: 0.015073 | Val Loss: 0.019158 | R2: 0.593793\n",
      "Model Checkpoint | epoch: 22 | best_val_loss: 0.01915824354073589\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 24/100 | Train Loss: 0.013855 | Val Loss: 0.020447 | R2: 0.565968\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 25/100 | Train Loss: 0.013337 | Val Loss: 0.019620 | R2: 0.584041\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 26/100 | Train Loss: 0.012948 | Val Loss: 0.020001 | R2: 0.575759\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 27/100 | Train Loss: 0.013039 | Val Loss: 0.019687 | R2: 0.582831\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 28/100 | Train Loss: 0.013537 | Val Loss: 0.018454 | R2: 0.609033\n",
      "Model Checkpoint | epoch: 27 | best_val_loss: 0.018454348842434735\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 29/100 | Train Loss: 0.012509 | Val Loss: 0.017264 | R2: 0.634653\n",
      "Model Checkpoint | epoch: 28 | best_val_loss: 0.01726421338868709\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 30/100 | Train Loss: 0.011545 | Val Loss: 0.018137 | R2: 0.615939\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 31/100 | Train Loss: 0.012508 | Val Loss: 0.020131 | R2: 0.573709\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 32/100 | Train Loss: 0.013408 | Val Loss: 0.019296 | R2: 0.591335\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 33/100 | Train Loss: 0.011545 | Val Loss: 0.016976 | R2: 0.640548\n",
      "Model Checkpoint | epoch: 32 | best_val_loss: 0.016976319655543193\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 34/100 | Train Loss: 0.011356 | Val Loss: 0.018661 | R2: 0.604460\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 35/100 | Train Loss: 0.011659 | Val Loss: 0.019983 | R2: 0.576509\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 36/100 | Train Loss: 0.013388 | Val Loss: 0.019434 | R2: 0.588328\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 37/100 | Train Loss: 0.012573 | Val Loss: 0.016941 | R2: 0.641993\n",
      "Model Checkpoint | epoch: 36 | best_val_loss: 0.01694054743210371\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 38/100 | Train Loss: 0.011581 | Val Loss: 0.019926 | R2: 0.577873\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 39/100 | Train Loss: 0.011546 | Val Loss: 0.015431 | R2: 0.673460\n",
      "Model Checkpoint | epoch: 38 | best_val_loss: 0.015431407093920926\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 40/100 | Train Loss: 0.011482 | Val Loss: 0.017346 | R2: 0.632556\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 41/100 | Train Loss: 0.011880 | Val Loss: 0.019133 | R2: 0.594269\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 42/100 | Train Loss: 0.010796 | Val Loss: 0.019231 | R2: 0.591646\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 43/100 | Train Loss: 0.010704 | Val Loss: 0.017560 | R2: 0.628183\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 44/100 | Train Loss: 0.010169 | Val Loss: 0.015426 | R2: 0.673435\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 45/100 | Train Loss: 0.009645 | Val Loss: 0.016722 | R2: 0.645817\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 46/100 | Train Loss: 0.010185 | Val Loss: 0.016338 | R2: 0.654139\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 47/100 | Train Loss: 0.011203 | Val Loss: 0.015538 | R2: 0.671291\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 48/100 | Train Loss: 0.010587 | Val Loss: 0.016482 | R2: 0.650221\n",
      "Hybrid_CNN_LSTM_ipsilateral_3_output Epoch 49/100 | Train Loss: 0.010064 | Val Loss: 0.015479 | R2: 0.672182\n",
      "Early stopping at epoch 49\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define Training and Test data\n",
    "# Read in the data\n",
    "processed_data_l_X = glob(os.path.join('/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/', '**', \"**\", \"X.npy\"))\n",
    "processed_data_l_y = glob(os.path.join('/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/', '**', \"**\", \"y.npy\"))\n",
    "# Define K-fold sets\n",
    "test_list_X = []\n",
    "train_list_X = []\n",
    "test_list_y = []\n",
    "train_list_y = []\n",
    "\n",
    "for i in range(len(processed_data_l_X)):\n",
    "    test_list_X.append(processed_data_l_X[i])\n",
    "    test_list_y.append(processed_data_l_y[i])\n",
    "    train_X = [x for idx, x in enumerate(processed_data_l_X) if idx != i]\n",
    "    train_y = [y for idx, y in enumerate(processed_data_l_y) if idx != i]\n",
    "    train_list_X.append(train_X)\n",
    "    train_list_y.append(train_y)\n",
    "\n",
    "# Load a single specific dataset\n",
    "# K-Fold 0 uses session 7 as the test set:\n",
    "KFOLD = 0\n",
    "SESSION_SET = 6\n",
    "\n",
    "\"\"\"\n",
    "['/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-24_(S10)/X.npy', 0\n",
    " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S1)/X.npy', 1\n",
    " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-06_(S6)/X.npy', 2\n",
    " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S5)/X.npy', 3\n",
    " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S3)/X.npy', 4\n",
    " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S4)/X.npy', 5\n",
    " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S8)/X.npy', 6 \n",
    " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-17_(S9)/X.npy', 7\n",
    " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S2)/X.npy'] 8\n",
    "\"\"\"\n",
    "\n",
    "X = np.load(train_list_X[KFOLD][SESSION_SET])\n",
    "y = np.load(train_list_y[KFOLD][SESSION_SET])\n",
    "\n",
    "# X = np.load(test_list_X[0]) # Identify the test set\n",
    "# y = np.load(test_list_y[0]) # Identify the test set\n",
    "\n",
    "# Creating Train and Validation Sets\n",
    "dataset = EcogMotionDataset(X, y)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=64)\n",
    "\n",
    "# Assuming train_loader, val_loader, criterion are defined\n",
    "\n",
    "# 2. CNN_LSTM Hybrid Model\n",
    "hybrid_model = EcogToMotionNet()\n",
    "criterion = nn.MSELoss()\n",
    "hybrid_train_losses, hybrid_val_losses, hybrid_r2 = train_model(hybrid_model, device, train_loader, val_loader, epochs=100, model_name=\"Hybrid_CNN_LSTM_ipsilateral_3_output\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad900469",
   "metadata": {},
   "source": [
    "### Contralateral Single Session Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10a47732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define Training and Test data\n",
    "# Read in the data\n",
    "movement_direction = \"Contralateral\"\n",
    "\n",
    "processed_data_l_X = sorted(glob(os.path.join('/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/', movement_direction, \"**\", \"X.npy\")))\n",
    "processed_data_l_y = sorted(glob(os.path.join('/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/', movement_direction, \"**\", \"y.npy\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0a65d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Contralateral/2018-03-15_(S1)/X.npy'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data_l_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "138c91ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 1/100 | Train Loss: 0.076476 | Val Loss: 0.065103 | R2: 0.018215\n",
      "Model Checkpoint | epoch: 0 | best_val_loss: 0.06510252501567204\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 2/100 | Train Loss: 0.071910 | Val Loss: 0.061534 | R2: 0.082820\n",
      "Model Checkpoint | epoch: 1 | best_val_loss: 0.06153363577918046\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 3/100 | Train Loss: 0.065237 | Val Loss: 0.053672 | R2: 0.201713\n",
      "Model Checkpoint | epoch: 2 | best_val_loss: 0.053671987336542874\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 4/100 | Train Loss: 0.054486 | Val Loss: 0.050726 | R2: 0.250228\n",
      "Model Checkpoint | epoch: 3 | best_val_loss: 0.05072632818379336\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 5/100 | Train Loss: 0.048533 | Val Loss: 0.046457 | R2: 0.319657\n",
      "Model Checkpoint | epoch: 4 | best_val_loss: 0.0464571131248441\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 6/100 | Train Loss: 0.042897 | Val Loss: 0.044687 | R2: 0.337792\n",
      "Model Checkpoint | epoch: 5 | best_val_loss: 0.044686566806398334\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 7/100 | Train Loss: 0.040162 | Val Loss: 0.043078 | R2: 0.367121\n",
      "Model Checkpoint | epoch: 6 | best_val_loss: 0.04307810249055425\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 8/100 | Train Loss: 0.037495 | Val Loss: 0.034729 | R2: 0.506881\n",
      "Model Checkpoint | epoch: 7 | best_val_loss: 0.034728613941826754\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 9/100 | Train Loss: 0.034584 | Val Loss: 0.037187 | R2: 0.458590\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 10/100 | Train Loss: 0.032436 | Val Loss: 0.032713 | R2: 0.533214\n",
      "Model Checkpoint | epoch: 9 | best_val_loss: 0.03271303516481486\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 11/100 | Train Loss: 0.030189 | Val Loss: 0.031084 | R2: 0.551057\n",
      "Model Checkpoint | epoch: 10 | best_val_loss: 0.03108443226147857\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 12/100 | Train Loss: 0.027796 | Val Loss: 0.030174 | R2: 0.574864\n",
      "Model Checkpoint | epoch: 11 | best_val_loss: 0.03017435435609271\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 13/100 | Train Loss: 0.027557 | Val Loss: 0.028142 | R2: 0.599391\n",
      "Model Checkpoint | epoch: 12 | best_val_loss: 0.028142109863356583\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 14/100 | Train Loss: 0.026186 | Val Loss: 0.027136 | R2: 0.614892\n",
      "Model Checkpoint | epoch: 13 | best_val_loss: 0.027136364137546883\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 15/100 | Train Loss: 0.024091 | Val Loss: 0.026625 | R2: 0.619339\n",
      "Model Checkpoint | epoch: 14 | best_val_loss: 0.026624704112195308\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 16/100 | Train Loss: 0.024234 | Val Loss: 0.026838 | R2: 0.610726\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 17/100 | Train Loss: 0.023310 | Val Loss: 0.024806 | R2: 0.645291\n",
      "Model Checkpoint | epoch: 16 | best_val_loss: 0.024805767490632003\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 18/100 | Train Loss: 0.020162 | Val Loss: 0.024122 | R2: 0.652255\n",
      "Model Checkpoint | epoch: 17 | best_val_loss: 0.024122419535699817\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 19/100 | Train Loss: 0.020094 | Val Loss: 0.023405 | R2: 0.668886\n",
      "Model Checkpoint | epoch: 18 | best_val_loss: 0.023405404111422185\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 20/100 | Train Loss: 0.019029 | Val Loss: 0.020355 | R2: 0.709556\n",
      "Model Checkpoint | epoch: 19 | best_val_loss: 0.02035462504133789\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 21/100 | Train Loss: 0.017606 | Val Loss: 0.022504 | R2: 0.672825\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 22/100 | Train Loss: 0.018023 | Val Loss: 0.024200 | R2: 0.645419\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 23/100 | Train Loss: 0.017458 | Val Loss: 0.019288 | R2: 0.722850\n",
      "Model Checkpoint | epoch: 22 | best_val_loss: 0.019288056918316417\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 24/100 | Train Loss: 0.017028 | Val Loss: 0.019366 | R2: 0.723027\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 25/100 | Train Loss: 0.016468 | Val Loss: 0.018709 | R2: 0.729727\n",
      "Model Checkpoint | epoch: 24 | best_val_loss: 0.018708949648568198\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 26/100 | Train Loss: 0.014396 | Val Loss: 0.017140 | R2: 0.752840\n",
      "Model Checkpoint | epoch: 25 | best_val_loss: 0.017139637449756266\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 27/100 | Train Loss: 0.014654 | Val Loss: 0.016490 | R2: 0.764946\n",
      "Model Checkpoint | epoch: 26 | best_val_loss: 0.0164904439294866\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 28/100 | Train Loss: 0.014312 | Val Loss: 0.018298 | R2: 0.733007\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 29/100 | Train Loss: 0.014405 | Val Loss: 0.016478 | R2: 0.763910\n",
      "Model Checkpoint | epoch: 28 | best_val_loss: 0.01647827382199466\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 30/100 | Train Loss: 0.013206 | Val Loss: 0.015791 | R2: 0.769784\n",
      "Model Checkpoint | epoch: 29 | best_val_loss: 0.015791328750188564\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 31/100 | Train Loss: 0.014399 | Val Loss: 0.018720 | R2: 0.725664\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 32/100 | Train Loss: 0.013432 | Val Loss: 0.014616 | R2: 0.792296\n",
      "Model Checkpoint | epoch: 31 | best_val_loss: 0.014615504181478172\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 33/100 | Train Loss: 0.014258 | Val Loss: 0.014591 | R2: 0.789787\n",
      "Model Checkpoint | epoch: 32 | best_val_loss: 0.014590595927943165\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 34/100 | Train Loss: 0.011811 | Val Loss: 0.015899 | R2: 0.767181\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 35/100 | Train Loss: 0.011802 | Val Loss: 0.015358 | R2: 0.780711\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 36/100 | Train Loss: 0.011483 | Val Loss: 0.013855 | R2: 0.799466\n",
      "Model Checkpoint | epoch: 35 | best_val_loss: 0.01385470592463389\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 37/100 | Train Loss: 0.012013 | Val Loss: 0.013465 | R2: 0.806514\n",
      "Model Checkpoint | epoch: 36 | best_val_loss: 0.01346454712924444\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 38/100 | Train Loss: 0.011195 | Val Loss: 0.013268 | R2: 0.808960\n",
      "Model Checkpoint | epoch: 37 | best_val_loss: 0.013267896506159256\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 39/100 | Train Loss: 0.009856 | Val Loss: 0.011025 | R2: 0.842788\n",
      "Model Checkpoint | epoch: 38 | best_val_loss: 0.011025489917878682\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 40/100 | Train Loss: 0.012230 | Val Loss: 0.012183 | R2: 0.822806\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 41/100 | Train Loss: 0.010238 | Val Loss: 0.011674 | R2: 0.831582\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 42/100 | Train Loss: 0.008556 | Val Loss: 0.011259 | R2: 0.836929\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 43/100 | Train Loss: 0.009995 | Val Loss: 0.012390 | R2: 0.819229\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 44/100 | Train Loss: 0.008811 | Val Loss: 0.012175 | R2: 0.819674\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 45/100 | Train Loss: 0.009206 | Val Loss: 0.011395 | R2: 0.834254\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 46/100 | Train Loss: 0.007612 | Val Loss: 0.009466 | R2: 0.858986\n",
      "Model Checkpoint | epoch: 45 | best_val_loss: 0.009466453400104203\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 47/100 | Train Loss: 0.006916 | Val Loss: 0.009096 | R2: 0.865681\n",
      "Model Checkpoint | epoch: 46 | best_val_loss: 0.009095823809077653\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 48/100 | Train Loss: 0.006541 | Val Loss: 0.008363 | R2: 0.878500\n",
      "Model Checkpoint | epoch: 47 | best_val_loss: 0.008362732661868601\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 49/100 | Train Loss: 0.006319 | Val Loss: 0.008445 | R2: 0.877864\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 50/100 | Train Loss: 0.006188 | Val Loss: 0.008048 | R2: 0.881448\n",
      "Model Checkpoint | epoch: 49 | best_val_loss: 0.008048340910592944\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 51/100 | Train Loss: 0.006121 | Val Loss: 0.008200 | R2: 0.880139\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 52/100 | Train Loss: 0.006054 | Val Loss: 0.008068 | R2: 0.880693\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 53/100 | Train Loss: 0.005585 | Val Loss: 0.007332 | R2: 0.891848\n",
      "Model Checkpoint | epoch: 52 | best_val_loss: 0.007331533420281226\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 54/100 | Train Loss: 0.005750 | Val Loss: 0.007680 | R2: 0.886755\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 55/100 | Train Loss: 0.005681 | Val Loss: 0.008138 | R2: 0.880582\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 56/100 | Train Loss: 0.005416 | Val Loss: 0.007967 | R2: 0.881307\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 57/100 | Train Loss: 0.005480 | Val Loss: 0.007678 | R2: 0.888197\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 58/100 | Train Loss: 0.005076 | Val Loss: 0.008177 | R2: 0.879147\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 59/100 | Train Loss: 0.005026 | Val Loss: 0.008143 | R2: 0.879835\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 60/100 | Train Loss: 0.004935 | Val Loss: 0.007773 | R2: 0.883810\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 61/100 | Train Loss: 0.004502 | Val Loss: 0.007306 | R2: 0.891824\n",
      "Model Checkpoint | epoch: 60 | best_val_loss: 0.007305527076810702\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 62/100 | Train Loss: 0.004369 | Val Loss: 0.006605 | R2: 0.902046\n",
      "Model Checkpoint | epoch: 61 | best_val_loss: 0.006605255900261303\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 63/100 | Train Loss: 0.004374 | Val Loss: 0.007285 | R2: 0.891490\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 64/100 | Train Loss: 0.004289 | Val Loss: 0.007314 | R2: 0.892027\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 65/100 | Train Loss: 0.004142 | Val Loss: 0.006807 | R2: 0.900119\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 66/100 | Train Loss: 0.004066 | Val Loss: 0.006748 | R2: 0.900569\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 67/100 | Train Loss: 0.004025 | Val Loss: 0.006524 | R2: 0.902517\n",
      "Model Checkpoint | epoch: 66 | best_val_loss: 0.006523814227069832\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 68/100 | Train Loss: 0.003931 | Val Loss: 0.006392 | R2: 0.904853\n",
      "Model Checkpoint | epoch: 67 | best_val_loss: 0.006391798340029911\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 69/100 | Train Loss: 0.003817 | Val Loss: 0.006875 | R2: 0.898146\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 70/100 | Train Loss: 0.003809 | Val Loss: 0.006298 | R2: 0.906468\n",
      "Model Checkpoint | epoch: 69 | best_val_loss: 0.006298358975133548\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 71/100 | Train Loss: 0.003763 | Val Loss: 0.006522 | R2: 0.902090\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 72/100 | Train Loss: 0.003670 | Val Loss: 0.006174 | R2: 0.908487\n",
      "Model Checkpoint | epoch: 71 | best_val_loss: 0.006173815322981681\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 73/100 | Train Loss: 0.003692 | Val Loss: 0.006052 | R2: 0.911231\n",
      "Model Checkpoint | epoch: 72 | best_val_loss: 0.006052208255116259\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 74/100 | Train Loss: 0.003583 | Val Loss: 0.006292 | R2: 0.906774\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 75/100 | Train Loss: 0.003672 | Val Loss: 0.006099 | R2: 0.910999\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 76/100 | Train Loss: 0.003510 | Val Loss: 0.006008 | R2: 0.911634\n",
      "Model Checkpoint | epoch: 75 | best_val_loss: 0.0060084720678925\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 77/100 | Train Loss: 0.003379 | Val Loss: 0.006312 | R2: 0.905278\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 78/100 | Train Loss: 0.003420 | Val Loss: 0.006405 | R2: 0.903682\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 79/100 | Train Loss: 0.003406 | Val Loss: 0.005519 | R2: 0.918227\n",
      "Model Checkpoint | epoch: 78 | best_val_loss: 0.005519266653075142\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 80/100 | Train Loss: 0.003389 | Val Loss: 0.005786 | R2: 0.914129\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 81/100 | Train Loss: 0.003249 | Val Loss: 0.005695 | R2: 0.914589\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 82/100 | Train Loss: 0.003109 | Val Loss: 0.006079 | R2: 0.908868\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 83/100 | Train Loss: 0.003181 | Val Loss: 0.005577 | R2: 0.917157\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 84/100 | Train Loss: 0.003053 | Val Loss: 0.005947 | R2: 0.912001\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 85/100 | Train Loss: 0.003076 | Val Loss: 0.004871 | R2: 0.927072\n",
      "Model Checkpoint | epoch: 84 | best_val_loss: 0.004871160694747232\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 86/100 | Train Loss: 0.002966 | Val Loss: 0.005404 | R2: 0.921110\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 87/100 | Train Loss: 0.002935 | Val Loss: 0.005421 | R2: 0.919042\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 88/100 | Train Loss: 0.002938 | Val Loss: 0.005391 | R2: 0.920076\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 89/100 | Train Loss: 0.002896 | Val Loss: 0.005279 | R2: 0.922397\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 90/100 | Train Loss: 0.002860 | Val Loss: 0.005265 | R2: 0.920794\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 91/100 | Train Loss: 0.002701 | Val Loss: 0.005034 | R2: 0.925378\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 92/100 | Train Loss: 0.002674 | Val Loss: 0.005042 | R2: 0.924700\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 93/100 | Train Loss: 0.002568 | Val Loss: 0.005084 | R2: 0.923854\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 94/100 | Train Loss: 0.002532 | Val Loss: 0.004999 | R2: 0.924684\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_0 Epoch 95/100 | Train Loss: 0.002427 | Val Loss: 0.004928 | R2: 0.926363\n",
      "Early stopping at epoch 95\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 1/100 | Train Loss: 0.083461 | Val Loss: 0.075677 | R2: 0.053182\n",
      "Model Checkpoint | epoch: 0 | best_val_loss: 0.07567700900344385\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 2/100 | Train Loss: 0.074972 | Val Loss: 0.072701 | R2: 0.088996\n",
      "Model Checkpoint | epoch: 1 | best_val_loss: 0.07270135995869835\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 3/100 | Train Loss: 0.068052 | Val Loss: 0.068848 | R2: 0.137135\n",
      "Model Checkpoint | epoch: 2 | best_val_loss: 0.06884818451106549\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 4/100 | Train Loss: 0.060443 | Val Loss: 0.064968 | R2: 0.182332\n",
      "Model Checkpoint | epoch: 3 | best_val_loss: 0.06496802492129305\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 5/100 | Train Loss: 0.055619 | Val Loss: 0.059671 | R2: 0.247563\n",
      "Model Checkpoint | epoch: 4 | best_val_loss: 0.059671171851352685\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 6/100 | Train Loss: 0.051031 | Val Loss: 0.056081 | R2: 0.291433\n",
      "Model Checkpoint | epoch: 5 | best_val_loss: 0.05608067521991001\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 7/100 | Train Loss: 0.047222 | Val Loss: 0.052791 | R2: 0.329098\n",
      "Model Checkpoint | epoch: 6 | best_val_loss: 0.052790598235610456\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 8/100 | Train Loss: 0.044045 | Val Loss: 0.050782 | R2: 0.356236\n",
      "Model Checkpoint | epoch: 7 | best_val_loss: 0.05078200164085461\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 9/100 | Train Loss: 0.041933 | Val Loss: 0.050072 | R2: 0.364801\n",
      "Model Checkpoint | epoch: 8 | best_val_loss: 0.05007176731682072\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 10/100 | Train Loss: 0.039158 | Val Loss: 0.047303 | R2: 0.398306\n",
      "Model Checkpoint | epoch: 9 | best_val_loss: 0.04730330998745436\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 11/100 | Train Loss: 0.037644 | Val Loss: 0.042286 | R2: 0.459776\n",
      "Model Checkpoint | epoch: 10 | best_val_loss: 0.0422859250428155\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 12/100 | Train Loss: 0.036120 | Val Loss: 0.040429 | R2: 0.483632\n",
      "Model Checkpoint | epoch: 11 | best_val_loss: 0.04042878780652407\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 13/100 | Train Loss: 0.033267 | Val Loss: 0.040098 | R2: 0.487724\n",
      "Model Checkpoint | epoch: 12 | best_val_loss: 0.04009817270662946\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 14/100 | Train Loss: 0.032176 | Val Loss: 0.038060 | R2: 0.511051\n",
      "Model Checkpoint | epoch: 13 | best_val_loss: 0.03805957532229109\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 15/100 | Train Loss: 0.030457 | Val Loss: 0.038983 | R2: 0.501490\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 16/100 | Train Loss: 0.029327 | Val Loss: 0.036029 | R2: 0.537613\n",
      "Model Checkpoint | epoch: 15 | best_val_loss: 0.03602891217002697\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 17/100 | Train Loss: 0.027481 | Val Loss: 0.033830 | R2: 0.564811\n",
      "Model Checkpoint | epoch: 16 | best_val_loss: 0.033830050679104814\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 18/100 | Train Loss: 0.025881 | Val Loss: 0.034216 | R2: 0.561193\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 19/100 | Train Loss: 0.025388 | Val Loss: 0.034936 | R2: 0.551288\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 20/100 | Train Loss: 0.024199 | Val Loss: 0.035832 | R2: 0.540588\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 21/100 | Train Loss: 0.023121 | Val Loss: 0.031408 | R2: 0.595072\n",
      "Model Checkpoint | epoch: 20 | best_val_loss: 0.031407611705802585\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 22/100 | Train Loss: 0.021593 | Val Loss: 0.036057 | R2: 0.530077\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 23/100 | Train Loss: 0.021317 | Val Loss: 0.029341 | R2: 0.622872\n",
      "Model Checkpoint | epoch: 22 | best_val_loss: 0.029341417479079812\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 24/100 | Train Loss: 0.020724 | Val Loss: 0.028522 | R2: 0.632500\n",
      "Model Checkpoint | epoch: 23 | best_val_loss: 0.028521854951240433\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 25/100 | Train Loss: 0.018777 | Val Loss: 0.026849 | R2: 0.652804\n",
      "Model Checkpoint | epoch: 24 | best_val_loss: 0.026849006557112767\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 26/100 | Train Loss: 0.018107 | Val Loss: 0.025994 | R2: 0.664757\n",
      "Model Checkpoint | epoch: 25 | best_val_loss: 0.02599376433118919\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 27/100 | Train Loss: 0.018266 | Val Loss: 0.026989 | R2: 0.651714\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 28/100 | Train Loss: 0.017272 | Val Loss: 0.028119 | R2: 0.637279\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 29/100 | Train Loss: 0.017175 | Val Loss: 0.026123 | R2: 0.663117\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 30/100 | Train Loss: 0.015684 | Val Loss: 0.023860 | R2: 0.690951\n",
      "Model Checkpoint | epoch: 29 | best_val_loss: 0.02386044503943736\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 31/100 | Train Loss: 0.016056 | Val Loss: 0.024125 | R2: 0.689188\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 32/100 | Train Loss: 0.014509 | Val Loss: 0.025447 | R2: 0.671841\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 33/100 | Train Loss: 0.014654 | Val Loss: 0.024777 | R2: 0.680220\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 34/100 | Train Loss: 0.013756 | Val Loss: 0.023395 | R2: 0.697511\n",
      "Model Checkpoint | epoch: 33 | best_val_loss: 0.023395405516390584\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 35/100 | Train Loss: 0.013547 | Val Loss: 0.023546 | R2: 0.695592\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 36/100 | Train Loss: 0.013066 | Val Loss: 0.025248 | R2: 0.674970\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 37/100 | Train Loss: 0.013231 | Val Loss: 0.021889 | R2: 0.716142\n",
      "Model Checkpoint | epoch: 36 | best_val_loss: 0.02188879242329858\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 38/100 | Train Loss: 0.012153 | Val Loss: 0.021882 | R2: 0.718035\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 39/100 | Train Loss: 0.012225 | Val Loss: 0.022176 | R2: 0.711584\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 40/100 | Train Loss: 0.011420 | Val Loss: 0.020470 | R2: 0.735920\n",
      "Model Checkpoint | epoch: 39 | best_val_loss: 0.020469504334792468\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 41/100 | Train Loss: 0.011625 | Val Loss: 0.020722 | R2: 0.733102\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 42/100 | Train Loss: 0.010624 | Val Loss: 0.020589 | R2: 0.734102\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 43/100 | Train Loss: 0.010987 | Val Loss: 0.019605 | R2: 0.746526\n",
      "Model Checkpoint | epoch: 42 | best_val_loss: 0.019604842478972085\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 44/100 | Train Loss: 0.011088 | Val Loss: 0.021367 | R2: 0.724669\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 45/100 | Train Loss: 0.010354 | Val Loss: 0.020992 | R2: 0.730262\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 46/100 | Train Loss: 0.009687 | Val Loss: 0.021304 | R2: 0.726385\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 47/100 | Train Loss: 0.010344 | Val Loss: 0.021278 | R2: 0.725248\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 48/100 | Train Loss: 0.009475 | Val Loss: 0.019287 | R2: 0.752582\n",
      "Model Checkpoint | epoch: 47 | best_val_loss: 0.019286671710744687\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 49/100 | Train Loss: 0.009619 | Val Loss: 0.019068 | R2: 0.754598\n",
      "Model Checkpoint | epoch: 48 | best_val_loss: 0.019068317901130893\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 50/100 | Train Loss: 0.009363 | Val Loss: 0.019909 | R2: 0.744311\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 51/100 | Train Loss: 0.009326 | Val Loss: 0.018228 | R2: 0.766592\n",
      "Model Checkpoint | epoch: 50 | best_val_loss: 0.01822840358244947\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 52/100 | Train Loss: 0.009074 | Val Loss: 0.017766 | R2: 0.770386\n",
      "Model Checkpoint | epoch: 51 | best_val_loss: 0.01776644816203366\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 53/100 | Train Loss: 0.008182 | Val Loss: 0.018915 | R2: 0.756717\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 54/100 | Train Loss: 0.008517 | Val Loss: 0.018366 | R2: 0.762996\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 55/100 | Train Loss: 0.008538 | Val Loss: 0.019081 | R2: 0.755451\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 56/100 | Train Loss: 0.008312 | Val Loss: 0.018631 | R2: 0.759668\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 57/100 | Train Loss: 0.008497 | Val Loss: 0.017677 | R2: 0.772540\n",
      "Model Checkpoint | epoch: 56 | best_val_loss: 0.017677005307993873\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 58/100 | Train Loss: 0.007801 | Val Loss: 0.016153 | R2: 0.791374\n",
      "Model Checkpoint | epoch: 57 | best_val_loss: 0.016153003124305138\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 59/100 | Train Loss: 0.007531 | Val Loss: 0.016230 | R2: 0.789347\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 60/100 | Train Loss: 0.007409 | Val Loss: 0.018278 | R2: 0.766398\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 61/100 | Train Loss: 0.008369 | Val Loss: 0.016807 | R2: 0.783552\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 62/100 | Train Loss: 0.007366 | Val Loss: 0.017263 | R2: 0.778548\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 63/100 | Train Loss: 0.007037 | Val Loss: 0.015894 | R2: 0.795018\n",
      "Model Checkpoint | epoch: 62 | best_val_loss: 0.015893865075788605\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 64/100 | Train Loss: 0.006757 | Val Loss: 0.017296 | R2: 0.778047\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 65/100 | Train Loss: 0.007547 | Val Loss: 0.015338 | R2: 0.801933\n",
      "Model Checkpoint | epoch: 64 | best_val_loss: 0.01533778352192409\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 66/100 | Train Loss: 0.006916 | Val Loss: 0.015312 | R2: 0.802041\n",
      "Model Checkpoint | epoch: 65 | best_val_loss: 0.015312398226203287\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 67/100 | Train Loss: 0.006177 | Val Loss: 0.015760 | R2: 0.796300\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 68/100 | Train Loss: 0.006464 | Val Loss: 0.014859 | R2: 0.807451\n",
      "Model Checkpoint | epoch: 67 | best_val_loss: 0.0148590999622134\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 69/100 | Train Loss: 0.006666 | Val Loss: 0.016108 | R2: 0.791554\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 70/100 | Train Loss: 0.006468 | Val Loss: 0.015593 | R2: 0.799515\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 71/100 | Train Loss: 0.006505 | Val Loss: 0.016038 | R2: 0.791759\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 72/100 | Train Loss: 0.006109 | Val Loss: 0.015662 | R2: 0.797117\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 73/100 | Train Loss: 0.006717 | Val Loss: 0.015550 | R2: 0.799562\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 74/100 | Train Loss: 0.006006 | Val Loss: 0.015849 | R2: 0.794924\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 75/100 | Train Loss: 0.005234 | Val Loss: 0.014347 | R2: 0.814462\n",
      "Model Checkpoint | epoch: 74 | best_val_loss: 0.014347152805896055\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 76/100 | Train Loss: 0.004522 | Val Loss: 0.014233 | R2: 0.815389\n",
      "Model Checkpoint | epoch: 75 | best_val_loss: 0.014232732741302849\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 77/100 | Train Loss: 0.004546 | Val Loss: 0.013606 | R2: 0.823150\n",
      "Model Checkpoint | epoch: 76 | best_val_loss: 0.01360593884810401\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 78/100 | Train Loss: 0.004550 | Val Loss: 0.013940 | R2: 0.819421\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 79/100 | Train Loss: 0.004235 | Val Loss: 0.012583 | R2: 0.835901\n",
      "Model Checkpoint | epoch: 78 | best_val_loss: 0.01258268248438738\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 80/100 | Train Loss: 0.004246 | Val Loss: 0.013069 | R2: 0.829361\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 81/100 | Train Loss: 0.004428 | Val Loss: 0.012638 | R2: 0.835698\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 82/100 | Train Loss: 0.004204 | Val Loss: 0.013255 | R2: 0.826658\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 83/100 | Train Loss: 0.004012 | Val Loss: 0.012332 | R2: 0.838014\n",
      "Model Checkpoint | epoch: 82 | best_val_loss: 0.012332100630605662\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 84/100 | Train Loss: 0.004023 | Val Loss: 0.013023 | R2: 0.830803\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 85/100 | Train Loss: 0.004026 | Val Loss: 0.012848 | R2: 0.831483\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 86/100 | Train Loss: 0.003807 | Val Loss: 0.013366 | R2: 0.825042\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 87/100 | Train Loss: 0.004066 | Val Loss: 0.013081 | R2: 0.829680\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 88/100 | Train Loss: 0.003857 | Val Loss: 0.013027 | R2: 0.830217\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 89/100 | Train Loss: 0.004003 | Val Loss: 0.013171 | R2: 0.829197\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 90/100 | Train Loss: 0.003630 | Val Loss: 0.012142 | R2: 0.841743\n",
      "Model Checkpoint | epoch: 89 | best_val_loss: 0.012141876254419913\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 91/100 | Train Loss: 0.003462 | Val Loss: 0.011813 | R2: 0.846141\n",
      "Model Checkpoint | epoch: 90 | best_val_loss: 0.011813157897332631\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 92/100 | Train Loss: 0.003313 | Val Loss: 0.011921 | R2: 0.844250\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 93/100 | Train Loss: 0.003151 | Val Loss: 0.011900 | R2: 0.844745\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 94/100 | Train Loss: 0.003191 | Val Loss: 0.011819 | R2: 0.845614\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 95/100 | Train Loss: 0.003126 | Val Loss: 0.011768 | R2: 0.846165\n",
      "Model Checkpoint | epoch: 94 | best_val_loss: 0.01176774593522229\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 96/100 | Train Loss: 0.003055 | Val Loss: 0.011471 | R2: 0.849566\n",
      "Model Checkpoint | epoch: 95 | best_val_loss: 0.01147106129183117\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 97/100 | Train Loss: 0.003102 | Val Loss: 0.011365 | R2: 0.850727\n",
      "Model Checkpoint | epoch: 96 | best_val_loss: 0.01136460368690718\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 98/100 | Train Loss: 0.002993 | Val Loss: 0.011024 | R2: 0.854896\n",
      "Model Checkpoint | epoch: 97 | best_val_loss: 0.011024352429628683\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 99/100 | Train Loss: 0.002958 | Val Loss: 0.011157 | R2: 0.853103\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_1 Epoch 100/100 | Train Loss: 0.002876 | Val Loss: 0.011053 | R2: 0.854756\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 1/100 | Train Loss: 0.074093 | Val Loss: 0.059729 | R2: 0.046256\n",
      "Model Checkpoint | epoch: 0 | best_val_loss: 0.059728716753009295\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 2/100 | Train Loss: 0.069104 | Val Loss: 0.057416 | R2: 0.084767\n",
      "Model Checkpoint | epoch: 1 | best_val_loss: 0.05741567953593201\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 3/100 | Train Loss: 0.063305 | Val Loss: 0.053077 | R2: 0.154658\n",
      "Model Checkpoint | epoch: 2 | best_val_loss: 0.05307703813289603\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 4/100 | Train Loss: 0.052693 | Val Loss: 0.049482 | R2: 0.212332\n",
      "Model Checkpoint | epoch: 3 | best_val_loss: 0.04948219099019965\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 5/100 | Train Loss: 0.044994 | Val Loss: 0.051383 | R2: 0.182271\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 6/100 | Train Loss: 0.040917 | Val Loss: 0.040193 | R2: 0.359686\n",
      "Model Checkpoint | epoch: 5 | best_val_loss: 0.04019347164241804\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 7/100 | Train Loss: 0.036737 | Val Loss: 0.040659 | R2: 0.353723\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 8/100 | Train Loss: 0.035578 | Val Loss: 0.039118 | R2: 0.379599\n",
      "Model Checkpoint | epoch: 7 | best_val_loss: 0.03911789229181078\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 9/100 | Train Loss: 0.034206 | Val Loss: 0.036456 | R2: 0.422215\n",
      "Model Checkpoint | epoch: 8 | best_val_loss: 0.03645553131618848\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 10/100 | Train Loss: 0.033506 | Val Loss: 0.035681 | R2: 0.433868\n",
      "Model Checkpoint | epoch: 9 | best_val_loss: 0.035680621064216315\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 11/100 | Train Loss: 0.031078 | Val Loss: 0.034974 | R2: 0.447292\n",
      "Model Checkpoint | epoch: 10 | best_val_loss: 0.03497409097136309\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 12/100 | Train Loss: 0.029282 | Val Loss: 0.036278 | R2: 0.422496\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 13/100 | Train Loss: 0.030209 | Val Loss: 0.034678 | R2: 0.451452\n",
      "Model Checkpoint | epoch: 12 | best_val_loss: 0.034677951040367284\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 14/100 | Train Loss: 0.029258 | Val Loss: 0.033738 | R2: 0.463778\n",
      "Model Checkpoint | epoch: 13 | best_val_loss: 0.03373750662969218\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 15/100 | Train Loss: 0.027847 | Val Loss: 0.031829 | R2: 0.496132\n",
      "Model Checkpoint | epoch: 14 | best_val_loss: 0.03182862439099699\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 16/100 | Train Loss: 0.026838 | Val Loss: 0.031993 | R2: 0.491001\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 17/100 | Train Loss: 0.027051 | Val Loss: 0.029365 | R2: 0.533556\n",
      "Model Checkpoint | epoch: 16 | best_val_loss: 0.029364622872322797\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 18/100 | Train Loss: 0.025374 | Val Loss: 0.031305 | R2: 0.500240\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 19/100 | Train Loss: 0.025853 | Val Loss: 0.029076 | R2: 0.536445\n",
      "Model Checkpoint | epoch: 18 | best_val_loss: 0.029075504059075483\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 20/100 | Train Loss: 0.024433 | Val Loss: 0.029413 | R2: 0.528292\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 21/100 | Train Loss: 0.023723 | Val Loss: 0.028633 | R2: 0.540782\n",
      "Model Checkpoint | epoch: 20 | best_val_loss: 0.02863297038571909\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 22/100 | Train Loss: 0.022720 | Val Loss: 0.028390 | R2: 0.545435\n",
      "Model Checkpoint | epoch: 21 | best_val_loss: 0.028389961380511523\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 23/100 | Train Loss: 0.022516 | Val Loss: 0.026538 | R2: 0.573289\n",
      "Model Checkpoint | epoch: 22 | best_val_loss: 0.026538280312696266\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 24/100 | Train Loss: 0.020907 | Val Loss: 0.025394 | R2: 0.593734\n",
      "Model Checkpoint | epoch: 23 | best_val_loss: 0.02539411450881097\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 25/100 | Train Loss: 0.019955 | Val Loss: 0.024074 | R2: 0.612563\n",
      "Model Checkpoint | epoch: 24 | best_val_loss: 0.0240737808149101\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 26/100 | Train Loss: 0.019659 | Val Loss: 0.024997 | R2: 0.599177\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 27/100 | Train Loss: 0.019022 | Val Loss: 0.021989 | R2: 0.648180\n",
      "Model Checkpoint | epoch: 26 | best_val_loss: 0.021989462984518874\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 28/100 | Train Loss: 0.017747 | Val Loss: 0.022592 | R2: 0.637290\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 29/100 | Train Loss: 0.018141 | Val Loss: 0.020963 | R2: 0.662443\n",
      "Model Checkpoint | epoch: 28 | best_val_loss: 0.02096323205354727\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 30/100 | Train Loss: 0.017790 | Val Loss: 0.023383 | R2: 0.624741\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 31/100 | Train Loss: 0.016689 | Val Loss: 0.020972 | R2: 0.661834\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 32/100 | Train Loss: 0.016604 | Val Loss: 0.021367 | R2: 0.655613\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 33/100 | Train Loss: 0.016891 | Val Loss: 0.025029 | R2: 0.595801\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 34/100 | Train Loss: 0.014997 | Val Loss: 0.021354 | R2: 0.655039\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 35/100 | Train Loss: 0.015703 | Val Loss: 0.020552 | R2: 0.669083\n",
      "Model Checkpoint | epoch: 34 | best_val_loss: 0.02055173961719912\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 36/100 | Train Loss: 0.016174 | Val Loss: 0.022493 | R2: 0.637297\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 37/100 | Train Loss: 0.015068 | Val Loss: 0.020204 | R2: 0.672619\n",
      "Model Checkpoint | epoch: 36 | best_val_loss: 0.020204468467065858\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 38/100 | Train Loss: 0.014170 | Val Loss: 0.019651 | R2: 0.680658\n",
      "Model Checkpoint | epoch: 37 | best_val_loss: 0.01965121628760567\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 39/100 | Train Loss: 0.013190 | Val Loss: 0.017995 | R2: 0.707305\n",
      "Model Checkpoint | epoch: 38 | best_val_loss: 0.017994951723195197\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 40/100 | Train Loss: 0.012659 | Val Loss: 0.018519 | R2: 0.698591\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 41/100 | Train Loss: 0.014070 | Val Loss: 0.018098 | R2: 0.706189\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 42/100 | Train Loss: 0.012767 | Val Loss: 0.017278 | R2: 0.719557\n",
      "Model Checkpoint | epoch: 41 | best_val_loss: 0.017278152431322573\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 43/100 | Train Loss: 0.011894 | Val Loss: 0.017391 | R2: 0.716559\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 44/100 | Train Loss: 0.012183 | Val Loss: 0.017880 | R2: 0.709789\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 45/100 | Train Loss: 0.011780 | Val Loss: 0.016431 | R2: 0.730754\n",
      "Model Checkpoint | epoch: 44 | best_val_loss: 0.016431314575020225\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 46/100 | Train Loss: 0.011000 | Val Loss: 0.016966 | R2: 0.722845\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 47/100 | Train Loss: 0.010813 | Val Loss: 0.015578 | R2: 0.744873\n",
      "Model Checkpoint | epoch: 46 | best_val_loss: 0.015577643541140586\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 48/100 | Train Loss: 0.011499 | Val Loss: 0.016910 | R2: 0.724232\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 49/100 | Train Loss: 0.010583 | Val Loss: 0.015872 | R2: 0.737141\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 50/100 | Train Loss: 0.010825 | Val Loss: 0.017814 | R2: 0.708157\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 51/100 | Train Loss: 0.011271 | Val Loss: 0.015789 | R2: 0.740045\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 52/100 | Train Loss: 0.010441 | Val Loss: 0.015335 | R2: 0.746702\n",
      "Model Checkpoint | epoch: 51 | best_val_loss: 0.015334975827588804\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 53/100 | Train Loss: 0.010557 | Val Loss: 0.015092 | R2: 0.753820\n",
      "Model Checkpoint | epoch: 52 | best_val_loss: 0.015092018926878356\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 54/100 | Train Loss: 0.009403 | Val Loss: 0.013915 | R2: 0.771205\n",
      "Model Checkpoint | epoch: 53 | best_val_loss: 0.013914971482112177\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 55/100 | Train Loss: 0.008722 | Val Loss: 0.014043 | R2: 0.769600\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 56/100 | Train Loss: 0.009458 | Val Loss: 0.014583 | R2: 0.761557\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 57/100 | Train Loss: 0.010137 | Val Loss: 0.014814 | R2: 0.756299\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 58/100 | Train Loss: 0.009533 | Val Loss: 0.014405 | R2: 0.764962\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 59/100 | Train Loss: 0.009000 | Val Loss: 0.014164 | R2: 0.767198\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 60/100 | Train Loss: 0.010456 | Val Loss: 0.013831 | R2: 0.771779\n",
      "Model Checkpoint | epoch: 59 | best_val_loss: 0.013831059996478467\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 61/100 | Train Loss: 0.008251 | Val Loss: 0.014215 | R2: 0.764296\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 62/100 | Train Loss: 0.009646 | Val Loss: 0.013519 | R2: 0.776764\n",
      "Model Checkpoint | epoch: 61 | best_val_loss: 0.013518500077382973\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 63/100 | Train Loss: 0.008520 | Val Loss: 0.012500 | R2: 0.793300\n",
      "Model Checkpoint | epoch: 62 | best_val_loss: 0.012500061458623452\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 64/100 | Train Loss: 0.008417 | Val Loss: 0.013247 | R2: 0.780111\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 65/100 | Train Loss: 0.007715 | Val Loss: 0.013565 | R2: 0.774570\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 66/100 | Train Loss: 0.008413 | Val Loss: 0.012608 | R2: 0.792050\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 67/100 | Train Loss: 0.008091 | Val Loss: 0.012853 | R2: 0.787823\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 68/100 | Train Loss: 0.008302 | Val Loss: 0.013287 | R2: 0.782718\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 69/100 | Train Loss: 0.008238 | Val Loss: 0.012371 | R2: 0.794577\n",
      "Model Checkpoint | epoch: 68 | best_val_loss: 0.012370556618656135\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 70/100 | Train Loss: 0.007079 | Val Loss: 0.012458 | R2: 0.794674\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 71/100 | Train Loss: 0.007147 | Val Loss: 0.012651 | R2: 0.790975\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 72/100 | Train Loss: 0.008155 | Val Loss: 0.012554 | R2: 0.792654\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 73/100 | Train Loss: 0.007421 | Val Loss: 0.011933 | R2: 0.801573\n",
      "Model Checkpoint | epoch: 72 | best_val_loss: 0.011932821687103974\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 74/100 | Train Loss: 0.007013 | Val Loss: 0.013801 | R2: 0.772166\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 75/100 | Train Loss: 0.007346 | Val Loss: 0.012069 | R2: 0.799167\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 76/100 | Train Loss: 0.007141 | Val Loss: 0.012066 | R2: 0.799776\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 77/100 | Train Loss: 0.007429 | Val Loss: 0.012373 | R2: 0.793327\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 78/100 | Train Loss: 0.006724 | Val Loss: 0.011846 | R2: 0.802141\n",
      "Model Checkpoint | epoch: 77 | best_val_loss: 0.011845551615852552\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 79/100 | Train Loss: 0.007047 | Val Loss: 0.011237 | R2: 0.813044\n",
      "Model Checkpoint | epoch: 78 | best_val_loss: 0.011236593591857753\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 80/100 | Train Loss: 0.006915 | Val Loss: 0.011991 | R2: 0.801362\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 81/100 | Train Loss: 0.007074 | Val Loss: 0.012819 | R2: 0.788208\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 82/100 | Train Loss: 0.006242 | Val Loss: 0.010952 | R2: 0.816820\n",
      "Model Checkpoint | epoch: 81 | best_val_loss: 0.010951527137762039\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 83/100 | Train Loss: 0.006048 | Val Loss: 0.011028 | R2: 0.814405\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 84/100 | Train Loss: 0.005910 | Val Loss: 0.011292 | R2: 0.809401\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 85/100 | Train Loss: 0.006480 | Val Loss: 0.011918 | R2: 0.801630\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 86/100 | Train Loss: 0.007704 | Val Loss: 0.011065 | R2: 0.815591\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 87/100 | Train Loss: 0.006384 | Val Loss: 0.010213 | R2: 0.829377\n",
      "Model Checkpoint | epoch: 86 | best_val_loss: 0.010212637029911598\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 88/100 | Train Loss: 0.006347 | Val Loss: 0.011457 | R2: 0.807172\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 89/100 | Train Loss: 0.005882 | Val Loss: 0.010610 | R2: 0.823510\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 90/100 | Train Loss: 0.005248 | Val Loss: 0.010420 | R2: 0.825322\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 91/100 | Train Loss: 0.005600 | Val Loss: 0.010058 | R2: 0.833234\n",
      "Model Checkpoint | epoch: 90 | best_val_loss: 0.010058424573430482\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 92/100 | Train Loss: 0.005665 | Val Loss: 0.013011 | R2: 0.783299\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 93/100 | Train Loss: 0.006819 | Val Loss: 0.011176 | R2: 0.813977\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 94/100 | Train Loss: 0.005644 | Val Loss: 0.010350 | R2: 0.825920\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 95/100 | Train Loss: 0.004827 | Val Loss: 0.010108 | R2: 0.828896\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 96/100 | Train Loss: 0.004942 | Val Loss: 0.010820 | R2: 0.820024\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 97/100 | Train Loss: 0.006507 | Val Loss: 0.010971 | R2: 0.815861\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 98/100 | Train Loss: 0.004915 | Val Loss: 0.009639 | R2: 0.836964\n",
      "Model Checkpoint | epoch: 97 | best_val_loss: 0.00963855899796666\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 99/100 | Train Loss: 0.004205 | Val Loss: 0.008799 | R2: 0.850693\n",
      "Model Checkpoint | epoch: 98 | best_val_loss: 0.008798959783123185\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 100/100 | Train Loss: 0.004066 | Val Loss: 0.008987 | R2: 0.846860\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 1/100 | Train Loss: 0.076288 | Val Loss: 0.068114 | R2: 0.037247\n",
      "Model Checkpoint | epoch: 0 | best_val_loss: 0.06811379946271579\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 2/100 | Train Loss: 0.070010 | Val Loss: 0.061545 | R2: 0.133817\n",
      "Model Checkpoint | epoch: 1 | best_val_loss: 0.06154469202272594\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 3/100 | Train Loss: 0.062858 | Val Loss: 0.055430 | R2: 0.219984\n",
      "Model Checkpoint | epoch: 2 | best_val_loss: 0.05542980338198443\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 4/100 | Train Loss: 0.052620 | Val Loss: 0.052654 | R2: 0.259842\n",
      "Model Checkpoint | epoch: 3 | best_val_loss: 0.05265439328331397\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 5/100 | Train Loss: 0.044846 | Val Loss: 0.044906 | R2: 0.373061\n",
      "Model Checkpoint | epoch: 4 | best_val_loss: 0.04490559764020145\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 6/100 | Train Loss: 0.039199 | Val Loss: 0.035005 | R2: 0.515450\n",
      "Model Checkpoint | epoch: 5 | best_val_loss: 0.0350048440852099\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 7/100 | Train Loss: 0.033490 | Val Loss: 0.036298 | R2: 0.496325\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 8/100 | Train Loss: 0.032639 | Val Loss: 0.035006 | R2: 0.514283\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 9/100 | Train Loss: 0.030878 | Val Loss: 0.032825 | R2: 0.547796\n",
      "Model Checkpoint | epoch: 8 | best_val_loss: 0.03282493391393736\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 10/100 | Train Loss: 0.028152 | Val Loss: 0.033997 | R2: 0.530743\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 11/100 | Train Loss: 0.029175 | Val Loss: 0.029179 | R2: 0.601697\n",
      "Model Checkpoint | epoch: 10 | best_val_loss: 0.02917883207840431\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 12/100 | Train Loss: 0.027161 | Val Loss: 0.030286 | R2: 0.584862\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 13/100 | Train Loss: 0.025820 | Val Loss: 0.029108 | R2: 0.601503\n",
      "Model Checkpoint | epoch: 12 | best_val_loss: 0.029107611410940686\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 14/100 | Train Loss: 0.025223 | Val Loss: 0.029037 | R2: 0.600208\n",
      "Model Checkpoint | epoch: 13 | best_val_loss: 0.029037064286983674\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 15/100 | Train Loss: 0.024457 | Val Loss: 0.026143 | R2: 0.643249\n",
      "Model Checkpoint | epoch: 14 | best_val_loss: 0.026142759406680448\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 16/100 | Train Loss: 0.022996 | Val Loss: 0.025544 | R2: 0.652010\n",
      "Model Checkpoint | epoch: 15 | best_val_loss: 0.025543613381580347\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 17/100 | Train Loss: 0.021278 | Val Loss: 0.027137 | R2: 0.629382\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 18/100 | Train Loss: 0.021132 | Val Loss: 0.026748 | R2: 0.634107\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 19/100 | Train Loss: 0.020873 | Val Loss: 0.023271 | R2: 0.683008\n",
      "Model Checkpoint | epoch: 18 | best_val_loss: 0.023271133427145995\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 20/100 | Train Loss: 0.018917 | Val Loss: 0.022025 | R2: 0.701139\n",
      "Model Checkpoint | epoch: 19 | best_val_loss: 0.022025294528187564\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 21/100 | Train Loss: 0.017650 | Val Loss: 0.022332 | R2: 0.697031\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 22/100 | Train Loss: 0.017763 | Val Loss: 0.021888 | R2: 0.703304\n",
      "Model Checkpoint | epoch: 21 | best_val_loss: 0.02188792304078945\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 23/100 | Train Loss: 0.017562 | Val Loss: 0.022949 | R2: 0.688769\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 24/100 | Train Loss: 0.020268 | Val Loss: 0.021356 | R2: 0.710309\n",
      "Model Checkpoint | epoch: 23 | best_val_loss: 0.021355570164276288\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 25/100 | Train Loss: 0.016550 | Val Loss: 0.018802 | R2: 0.748213\n",
      "Model Checkpoint | epoch: 24 | best_val_loss: 0.018802213536782398\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 26/100 | Train Loss: 0.015778 | Val Loss: 0.018839 | R2: 0.746147\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 27/100 | Train Loss: 0.016084 | Val Loss: 0.020081 | R2: 0.728664\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 28/100 | Train Loss: 0.016757 | Val Loss: 0.019257 | R2: 0.740576\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 29/100 | Train Loss: 0.015822 | Val Loss: 0.019863 | R2: 0.731826\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 30/100 | Train Loss: 0.016992 | Val Loss: 0.019887 | R2: 0.730681\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 31/100 | Train Loss: 0.016039 | Val Loss: 0.019685 | R2: 0.733534\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 32/100 | Train Loss: 0.013352 | Val Loss: 0.017201 | R2: 0.769247\n",
      "Model Checkpoint | epoch: 31 | best_val_loss: 0.017200959096197038\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 33/100 | Train Loss: 0.012703 | Val Loss: 0.016508 | R2: 0.777802\n",
      "Model Checkpoint | epoch: 32 | best_val_loss: 0.01650762963005238\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 34/100 | Train Loss: 0.012577 | Val Loss: 0.017057 | R2: 0.769029\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 35/100 | Train Loss: 0.011791 | Val Loss: 0.014895 | R2: 0.800370\n",
      "Model Checkpoint | epoch: 34 | best_val_loss: 0.014895447563949145\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 36/100 | Train Loss: 0.011073 | Val Loss: 0.013542 | R2: 0.819582\n",
      "Model Checkpoint | epoch: 35 | best_val_loss: 0.013542211754713207\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 37/100 | Train Loss: 0.010856 | Val Loss: 0.014138 | R2: 0.811231\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 38/100 | Train Loss: 0.010507 | Val Loss: 0.012906 | R2: 0.827215\n",
      "Model Checkpoint | epoch: 37 | best_val_loss: 0.012906112813597753\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 39/100 | Train Loss: 0.010078 | Val Loss: 0.012915 | R2: 0.826615\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 40/100 | Train Loss: 0.009915 | Val Loss: 0.012481 | R2: 0.832442\n",
      "Model Checkpoint | epoch: 39 | best_val_loss: 0.012481269595859986\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 41/100 | Train Loss: 0.009907 | Val Loss: 0.012302 | R2: 0.835466\n",
      "Model Checkpoint | epoch: 40 | best_val_loss: 0.012302385126654472\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 42/100 | Train Loss: 0.009196 | Val Loss: 0.011977 | R2: 0.840357\n",
      "Model Checkpoint | epoch: 41 | best_val_loss: 0.011976665262650284\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 43/100 | Train Loss: 0.008989 | Val Loss: 0.011126 | R2: 0.852054\n",
      "Model Checkpoint | epoch: 42 | best_val_loss: 0.011126136877147171\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 44/100 | Train Loss: 0.008420 | Val Loss: 0.010720 | R2: 0.857307\n",
      "Model Checkpoint | epoch: 43 | best_val_loss: 0.010719728214640378\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 45/100 | Train Loss: 0.008104 | Val Loss: 0.010375 | R2: 0.862332\n",
      "Model Checkpoint | epoch: 44 | best_val_loss: 0.01037530311028887\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 46/100 | Train Loss: 0.008129 | Val Loss: 0.010023 | R2: 0.866476\n",
      "Model Checkpoint | epoch: 45 | best_val_loss: 0.0100234570694156\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 47/100 | Train Loss: 0.007892 | Val Loss: 0.010470 | R2: 0.859797\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 48/100 | Train Loss: 0.007656 | Val Loss: 0.009638 | R2: 0.871863\n",
      "Model Checkpoint | epoch: 47 | best_val_loss: 0.009638242079928104\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 49/100 | Train Loss: 0.007399 | Val Loss: 0.010017 | R2: 0.866558\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 50/100 | Train Loss: 0.006891 | Val Loss: 0.009501 | R2: 0.873298\n",
      "Model Checkpoint | epoch: 49 | best_val_loss: 0.00950138679977196\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 51/100 | Train Loss: 0.007164 | Val Loss: 0.010180 | R2: 0.862490\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 52/100 | Train Loss: 0.007433 | Val Loss: 0.009515 | R2: 0.872970\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 53/100 | Train Loss: 0.006453 | Val Loss: 0.009613 | R2: 0.871316\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 54/100 | Train Loss: 0.006251 | Val Loss: 0.010010 | R2: 0.864508\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 55/100 | Train Loss: 0.006213 | Val Loss: 0.009240 | R2: 0.875971\n",
      "Model Checkpoint | epoch: 54 | best_val_loss: 0.00923970533186932\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 56/100 | Train Loss: 0.006200 | Val Loss: 0.008481 | R2: 0.886288\n",
      "Model Checkpoint | epoch: 55 | best_val_loss: 0.008480546848977812\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 57/100 | Train Loss: 0.006169 | Val Loss: 0.008769 | R2: 0.882368\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 58/100 | Train Loss: 0.006030 | Val Loss: 0.009471 | R2: 0.871593\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 59/100 | Train Loss: 0.005849 | Val Loss: 0.009019 | R2: 0.878290\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 60/100 | Train Loss: 0.005769 | Val Loss: 0.008915 | R2: 0.879405\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 61/100 | Train Loss: 0.005483 | Val Loss: 0.008439 | R2: 0.886113\n",
      "Model Checkpoint | epoch: 60 | best_val_loss: 0.008439088285208628\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 62/100 | Train Loss: 0.005278 | Val Loss: 0.007884 | R2: 0.894208\n",
      "Model Checkpoint | epoch: 61 | best_val_loss: 0.00788351732612096\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 63/100 | Train Loss: 0.005559 | Val Loss: 0.008041 | R2: 0.891129\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 64/100 | Train Loss: 0.005228 | Val Loss: 0.008692 | R2: 0.881790\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 65/100 | Train Loss: 0.005265 | Val Loss: 0.008261 | R2: 0.888535\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 66/100 | Train Loss: 0.005042 | Val Loss: 0.007781 | R2: 0.895466\n",
      "Model Checkpoint | epoch: 65 | best_val_loss: 0.007780924030725145\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 67/100 | Train Loss: 0.004740 | Val Loss: 0.008399 | R2: 0.885651\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 68/100 | Train Loss: 0.004851 | Val Loss: 0.008009 | R2: 0.891687\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 69/100 | Train Loss: 0.005383 | Val Loss: 0.008932 | R2: 0.878360\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 70/100 | Train Loss: 0.004690 | Val Loss: 0.007043 | R2: 0.904822\n",
      "Model Checkpoint | epoch: 69 | best_val_loss: 0.007042811716041696\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 71/100 | Train Loss: 0.004314 | Val Loss: 0.007251 | R2: 0.901755\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 72/100 | Train Loss: 0.005108 | Val Loss: 0.007491 | R2: 0.898154\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 73/100 | Train Loss: 0.004422 | Val Loss: 0.006974 | R2: 0.905988\n",
      "Model Checkpoint | epoch: 72 | best_val_loss: 0.006973776060379654\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 74/100 | Train Loss: 0.004148 | Val Loss: 0.006812 | R2: 0.907925\n",
      "Model Checkpoint | epoch: 73 | best_val_loss: 0.006811825275126871\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 75/100 | Train Loss: 0.004570 | Val Loss: 0.006644 | R2: 0.909785\n",
      "Model Checkpoint | epoch: 74 | best_val_loss: 0.006643916241924873\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 76/100 | Train Loss: 0.004880 | Val Loss: 0.007528 | R2: 0.897391\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 77/100 | Train Loss: 0.004324 | Val Loss: 0.007114 | R2: 0.903242\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 78/100 | Train Loss: 0.004482 | Val Loss: 0.007731 | R2: 0.893842\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 79/100 | Train Loss: 0.004203 | Val Loss: 0.006287 | R2: 0.915467\n",
      "Model Checkpoint | epoch: 78 | best_val_loss: 0.0062873429744860106\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 80/100 | Train Loss: 0.003834 | Val Loss: 0.006311 | R2: 0.914427\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 81/100 | Train Loss: 0.003634 | Val Loss: 0.006943 | R2: 0.904975\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 82/100 | Train Loss: 0.003607 | Val Loss: 0.006718 | R2: 0.907964\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 83/100 | Train Loss: 0.004770 | Val Loss: 0.006449 | R2: 0.912482\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 84/100 | Train Loss: 0.003857 | Val Loss: 0.005653 | R2: 0.923493\n",
      "Model Checkpoint | epoch: 83 | best_val_loss: 0.005652747683165621\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 85/100 | Train Loss: 0.003561 | Val Loss: 0.006071 | R2: 0.917485\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 86/100 | Train Loss: 0.003587 | Val Loss: 0.006197 | R2: 0.915812\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 87/100 | Train Loss: 0.003554 | Val Loss: 0.005938 | R2: 0.918989\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 88/100 | Train Loss: 0.003772 | Val Loss: 0.005307 | R2: 0.928261\n",
      "Model Checkpoint | epoch: 87 | best_val_loss: 0.00530700770955688\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 89/100 | Train Loss: 0.003614 | Val Loss: 0.006272 | R2: 0.914468\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 90/100 | Train Loss: 0.003764 | Val Loss: 0.006175 | R2: 0.916127\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 91/100 | Train Loss: 0.003557 | Val Loss: 0.006535 | R2: 0.910700\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 92/100 | Train Loss: 0.003249 | Val Loss: 0.005957 | R2: 0.919083\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 93/100 | Train Loss: 0.004132 | Val Loss: 0.007934 | R2: 0.890355\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 94/100 | Train Loss: 0.003603 | Val Loss: 0.006234 | R2: 0.914802\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 95/100 | Train Loss: 0.002818 | Val Loss: 0.005605 | R2: 0.923781\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 96/100 | Train Loss: 0.002725 | Val Loss: 0.005573 | R2: 0.923827\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 97/100 | Train Loss: 0.002771 | Val Loss: 0.005375 | R2: 0.926256\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_3 Epoch 98/100 | Train Loss: 0.002705 | Val Loss: 0.005385 | R2: 0.926092\n",
      "Early stopping at epoch 98\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 1/100 | Train Loss: 0.072101 | Val Loss: 0.069486 | R2: 0.056924\n",
      "Model Checkpoint | epoch: 0 | best_val_loss: 0.06948642100145419\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 2/100 | Train Loss: 0.065532 | Val Loss: 0.065841 | R2: 0.106708\n",
      "Model Checkpoint | epoch: 1 | best_val_loss: 0.06584115993231535\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 3/100 | Train Loss: 0.058811 | Val Loss: 0.059570 | R2: 0.191522\n",
      "Model Checkpoint | epoch: 2 | best_val_loss: 0.05957002556117044\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 4/100 | Train Loss: 0.048757 | Val Loss: 0.055650 | R2: 0.244922\n",
      "Model Checkpoint | epoch: 3 | best_val_loss: 0.055649511300855214\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 5/100 | Train Loss: 0.043279 | Val Loss: 0.046650 | R2: 0.366810\n",
      "Model Checkpoint | epoch: 4 | best_val_loss: 0.046650417101672954\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 6/100 | Train Loss: 0.038206 | Val Loss: 0.045413 | R2: 0.383736\n",
      "Model Checkpoint | epoch: 5 | best_val_loss: 0.04541311018551803\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 7/100 | Train Loss: 0.033822 | Val Loss: 0.041723 | R2: 0.433965\n",
      "Model Checkpoint | epoch: 6 | best_val_loss: 0.041722938370994396\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 8/100 | Train Loss: 0.032384 | Val Loss: 0.039259 | R2: 0.467485\n",
      "Model Checkpoint | epoch: 7 | best_val_loss: 0.03925854325377279\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 9/100 | Train Loss: 0.031834 | Val Loss: 0.034998 | R2: 0.525600\n",
      "Model Checkpoint | epoch: 8 | best_val_loss: 0.034998040267887214\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 10/100 | Train Loss: 0.027305 | Val Loss: 0.034826 | R2: 0.528089\n",
      "Model Checkpoint | epoch: 9 | best_val_loss: 0.03482606456677119\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 11/100 | Train Loss: 0.026244 | Val Loss: 0.032591 | R2: 0.558372\n",
      "Model Checkpoint | epoch: 10 | best_val_loss: 0.032590524032867205\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 12/100 | Train Loss: 0.025227 | Val Loss: 0.033857 | R2: 0.541511\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 13/100 | Train Loss: 0.023585 | Val Loss: 0.030023 | R2: 0.593665\n",
      "Model Checkpoint | epoch: 12 | best_val_loss: 0.030023293350098863\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 14/100 | Train Loss: 0.021314 | Val Loss: 0.027714 | R2: 0.625327\n",
      "Model Checkpoint | epoch: 13 | best_val_loss: 0.027713775497550764\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 15/100 | Train Loss: 0.022503 | Val Loss: 0.026987 | R2: 0.635263\n",
      "Model Checkpoint | epoch: 14 | best_val_loss: 0.026986590474016137\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 16/100 | Train Loss: 0.019725 | Val Loss: 0.028650 | R2: 0.612569\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 17/100 | Train Loss: 0.020904 | Val Loss: 0.029171 | R2: 0.605589\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 18/100 | Train Loss: 0.018873 | Val Loss: 0.026618 | R2: 0.640519\n",
      "Model Checkpoint | epoch: 17 | best_val_loss: 0.02661769480801498\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 19/100 | Train Loss: 0.017466 | Val Loss: 0.027776 | R2: 0.624801\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 20/100 | Train Loss: 0.018087 | Val Loss: 0.024895 | R2: 0.663958\n",
      "Model Checkpoint | epoch: 19 | best_val_loss: 0.02489470559037808\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 21/100 | Train Loss: 0.016887 | Val Loss: 0.031489 | R2: 0.573454\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 22/100 | Train Loss: 0.018469 | Val Loss: 0.026993 | R2: 0.635031\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 23/100 | Train Loss: 0.015978 | Val Loss: 0.021331 | R2: 0.712180\n",
      "Model Checkpoint | epoch: 22 | best_val_loss: 0.021331093832022613\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 24/100 | Train Loss: 0.016783 | Val Loss: 0.022488 | R2: 0.696953\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 25/100 | Train Loss: 0.015509 | Val Loss: 0.021244 | R2: 0.713555\n",
      "Model Checkpoint | epoch: 24 | best_val_loss: 0.021243559869627157\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 26/100 | Train Loss: 0.014709 | Val Loss: 0.020413 | R2: 0.724480\n",
      "Model Checkpoint | epoch: 25 | best_val_loss: 0.020413354667524496\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 27/100 | Train Loss: 0.014665 | Val Loss: 0.020249 | R2: 0.726751\n",
      "Model Checkpoint | epoch: 26 | best_val_loss: 0.02024907613180888\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 28/100 | Train Loss: 0.015018 | Val Loss: 0.019789 | R2: 0.732827\n",
      "Model Checkpoint | epoch: 27 | best_val_loss: 0.019789451444211105\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 29/100 | Train Loss: 0.013578 | Val Loss: 0.018726 | R2: 0.747479\n",
      "Model Checkpoint | epoch: 28 | best_val_loss: 0.018726258547562692\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 30/100 | Train Loss: 0.012819 | Val Loss: 0.018098 | R2: 0.755813\n",
      "Model Checkpoint | epoch: 29 | best_val_loss: 0.01809785509414764\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 31/100 | Train Loss: 0.013037 | Val Loss: 0.017450 | R2: 0.764904\n",
      "Model Checkpoint | epoch: 30 | best_val_loss: 0.017449919134119734\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 32/100 | Train Loss: 0.012577 | Val Loss: 0.018370 | R2: 0.751915\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 33/100 | Train Loss: 0.012532 | Val Loss: 0.016309 | R2: 0.780330\n",
      "Model Checkpoint | epoch: 32 | best_val_loss: 0.016309461266526746\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 34/100 | Train Loss: 0.011063 | Val Loss: 0.016112 | R2: 0.782664\n",
      "Model Checkpoint | epoch: 33 | best_val_loss: 0.016111627985645707\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 35/100 | Train Loss: 0.011821 | Val Loss: 0.016142 | R2: 0.782515\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 36/100 | Train Loss: 0.010785 | Val Loss: 0.014656 | R2: 0.802916\n",
      "Model Checkpoint | epoch: 35 | best_val_loss: 0.014656153850697188\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 37/100 | Train Loss: 0.010346 | Val Loss: 0.015388 | R2: 0.792682\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 38/100 | Train Loss: 0.009903 | Val Loss: 0.014870 | R2: 0.799931\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 39/100 | Train Loss: 0.011337 | Val Loss: 0.014142 | R2: 0.809919\n",
      "Model Checkpoint | epoch: 38 | best_val_loss: 0.014142186943001837\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 40/100 | Train Loss: 0.009373 | Val Loss: 0.014750 | R2: 0.801457\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 41/100 | Train Loss: 0.010694 | Val Loss: 0.015898 | R2: 0.785959\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 42/100 | Train Loss: 0.009915 | Val Loss: 0.013924 | R2: 0.812825\n",
      "Model Checkpoint | epoch: 41 | best_val_loss: 0.013923819422799473\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 43/100 | Train Loss: 0.008989 | Val Loss: 0.014183 | R2: 0.809408\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 44/100 | Train Loss: 0.010487 | Val Loss: 0.016528 | R2: 0.777567\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 45/100 | Train Loss: 0.009834 | Val Loss: 0.015106 | R2: 0.796701\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 46/100 | Train Loss: 0.008578 | Val Loss: 0.018342 | R2: 0.752391\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 47/100 | Train Loss: 0.009104 | Val Loss: 0.013520 | R2: 0.818155\n",
      "Model Checkpoint | epoch: 46 | best_val_loss: 0.013519982767653548\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 48/100 | Train Loss: 0.008683 | Val Loss: 0.012645 | R2: 0.830027\n",
      "Model Checkpoint | epoch: 47 | best_val_loss: 0.012644843862366138\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 49/100 | Train Loss: 0.007839 | Val Loss: 0.012587 | R2: 0.830670\n",
      "Model Checkpoint | epoch: 48 | best_val_loss: 0.012587453327891934\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 50/100 | Train Loss: 0.007823 | Val Loss: 0.014457 | R2: 0.805570\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 51/100 | Train Loss: 0.008740 | Val Loss: 0.013902 | R2: 0.812925\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 52/100 | Train Loss: 0.009802 | Val Loss: 0.012762 | R2: 0.828248\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 53/100 | Train Loss: 0.007476 | Val Loss: 0.012290 | R2: 0.834589\n",
      "Model Checkpoint | epoch: 52 | best_val_loss: 0.01229038209688022\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 54/100 | Train Loss: 0.006943 | Val Loss: 0.012329 | R2: 0.834067\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 55/100 | Train Loss: 0.006617 | Val Loss: 0.011814 | R2: 0.841147\n",
      "Model Checkpoint | epoch: 54 | best_val_loss: 0.011813664987062414\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 56/100 | Train Loss: 0.007822 | Val Loss: 0.012681 | R2: 0.829027\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 57/100 | Train Loss: 0.008552 | Val Loss: 0.012104 | R2: 0.836969\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 58/100 | Train Loss: 0.006976 | Val Loss: 0.012862 | R2: 0.826746\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 59/100 | Train Loss: 0.007426 | Val Loss: 0.011780 | R2: 0.841692\n",
      "Model Checkpoint | epoch: 58 | best_val_loss: 0.011779937348794192\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 60/100 | Train Loss: 0.008065 | Val Loss: 0.012268 | R2: 0.834889\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 61/100 | Train Loss: 0.006418 | Val Loss: 0.011641 | R2: 0.843524\n",
      "Model Checkpoint | epoch: 60 | best_val_loss: 0.011640516927465797\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 62/100 | Train Loss: 0.006942 | Val Loss: 0.012011 | R2: 0.838676\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 63/100 | Train Loss: 0.006905 | Val Loss: 0.014505 | R2: 0.804518\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 64/100 | Train Loss: 0.006853 | Val Loss: 0.011435 | R2: 0.846267\n",
      "Model Checkpoint | epoch: 63 | best_val_loss: 0.011434869400173839\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 65/100 | Train Loss: 0.007302 | Val Loss: 0.011016 | R2: 0.852004\n",
      "Model Checkpoint | epoch: 64 | best_val_loss: 0.01101631145965722\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 66/100 | Train Loss: 0.007101 | Val Loss: 0.010439 | R2: 0.859570\n",
      "Model Checkpoint | epoch: 65 | best_val_loss: 0.010438838110046669\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 67/100 | Train Loss: 0.006154 | Val Loss: 0.016480 | R2: 0.777886\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 68/100 | Train Loss: 0.008200 | Val Loss: 0.015427 | R2: 0.792375\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 69/100 | Train Loss: 0.007647 | Val Loss: 0.012876 | R2: 0.826797\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 70/100 | Train Loss: 0.006092 | Val Loss: 0.011585 | R2: 0.844417\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 71/100 | Train Loss: 0.006292 | Val Loss: 0.011018 | R2: 0.852083\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 72/100 | Train Loss: 0.006128 | Val Loss: 0.011280 | R2: 0.848461\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 73/100 | Train Loss: 0.005113 | Val Loss: 0.010226 | R2: 0.862593\n",
      "Model Checkpoint | epoch: 72 | best_val_loss: 0.010225579496602424\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 74/100 | Train Loss: 0.005090 | Val Loss: 0.009966 | R2: 0.866314\n",
      "Model Checkpoint | epoch: 73 | best_val_loss: 0.009965799103012412\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 75/100 | Train Loss: 0.004807 | Val Loss: 0.009844 | R2: 0.867699\n",
      "Model Checkpoint | epoch: 74 | best_val_loss: 0.009844238014374342\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 76/100 | Train Loss: 0.004546 | Val Loss: 0.009753 | R2: 0.868984\n",
      "Model Checkpoint | epoch: 75 | best_val_loss: 0.00975345359415385\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 77/100 | Train Loss: 0.004299 | Val Loss: 0.008948 | R2: 0.879733\n",
      "Model Checkpoint | epoch: 76 | best_val_loss: 0.008948246151739213\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 78/100 | Train Loss: 0.004311 | Val Loss: 0.008933 | R2: 0.880112\n",
      "Model Checkpoint | epoch: 77 | best_val_loss: 0.008932533560657046\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 79/100 | Train Loss: 0.004498 | Val Loss: 0.009326 | R2: 0.874717\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 80/100 | Train Loss: 0.004020 | Val Loss: 0.009240 | R2: 0.875841\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 81/100 | Train Loss: 0.003998 | Val Loss: 0.008819 | R2: 0.881623\n",
      "Model Checkpoint | epoch: 80 | best_val_loss: 0.008819297681834239\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 82/100 | Train Loss: 0.003894 | Val Loss: 0.008857 | R2: 0.881063\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 83/100 | Train Loss: 0.004030 | Val Loss: 0.009021 | R2: 0.878751\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 84/100 | Train Loss: 0.003779 | Val Loss: 0.009282 | R2: 0.875201\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 85/100 | Train Loss: 0.003822 | Val Loss: 0.009343 | R2: 0.874367\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 86/100 | Train Loss: 0.003732 | Val Loss: 0.009203 | R2: 0.876097\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 87/100 | Train Loss: 0.003510 | Val Loss: 0.009052 | R2: 0.878139\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 88/100 | Train Loss: 0.003228 | Val Loss: 0.008862 | R2: 0.880832\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 89/100 | Train Loss: 0.003151 | Val Loss: 0.008568 | R2: 0.884709\n",
      "Model Checkpoint | epoch: 88 | best_val_loss: 0.008568296871859477\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 90/100 | Train Loss: 0.002997 | Val Loss: 0.008561 | R2: 0.884918\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 91/100 | Train Loss: 0.002959 | Val Loss: 0.008275 | R2: 0.888737\n",
      "Model Checkpoint | epoch: 90 | best_val_loss: 0.008274886968784914\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 92/100 | Train Loss: 0.002894 | Val Loss: 0.008605 | R2: 0.884164\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 93/100 | Train Loss: 0.002980 | Val Loss: 0.008810 | R2: 0.881442\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 94/100 | Train Loss: 0.002790 | Val Loss: 0.008636 | R2: 0.883783\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 95/100 | Train Loss: 0.002889 | Val Loss: 0.008229 | R2: 0.889252\n",
      "Model Checkpoint | epoch: 94 | best_val_loss: 0.008229422933376756\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 96/100 | Train Loss: 0.002780 | Val Loss: 0.008203 | R2: 0.889679\n",
      "Model Checkpoint | epoch: 95 | best_val_loss: 0.008203278861403103\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 97/100 | Train Loss: 0.002836 | Val Loss: 0.008003 | R2: 0.892430\n",
      "Model Checkpoint | epoch: 96 | best_val_loss: 0.008002855037558928\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 98/100 | Train Loss: 0.002866 | Val Loss: 0.008322 | R2: 0.888037\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 99/100 | Train Loss: 0.002652 | Val Loss: 0.007815 | R2: 0.894882\n",
      "Model Checkpoint | epoch: 98 | best_val_loss: 0.007814715533931222\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_4 Epoch 100/100 | Train Loss: 0.002601 | Val Loss: 0.008142 | R2: 0.890340\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 1/100 | Train Loss: 0.125707 | Val Loss: 0.108124 | R2: 0.066979\n",
      "Model Checkpoint | epoch: 0 | best_val_loss: 0.10812407014436191\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 2/100 | Train Loss: 0.109817 | Val Loss: 0.102892 | R2: 0.112358\n",
      "Model Checkpoint | epoch: 1 | best_val_loss: 0.10289242733683851\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 3/100 | Train Loss: 0.094543 | Val Loss: 0.089625 | R2: 0.222978\n",
      "Model Checkpoint | epoch: 2 | best_val_loss: 0.08962469796215494\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 4/100 | Train Loss: 0.082618 | Val Loss: 0.083976 | R2: 0.270546\n",
      "Model Checkpoint | epoch: 3 | best_val_loss: 0.08397628389774925\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 5/100 | Train Loss: 0.074744 | Val Loss: 0.079492 | R2: 0.307955\n",
      "Model Checkpoint | epoch: 4 | best_val_loss: 0.07949167597790559\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 6/100 | Train Loss: 0.070371 | Val Loss: 0.078358 | R2: 0.318500\n",
      "Model Checkpoint | epoch: 5 | best_val_loss: 0.07835765512122048\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 7/100 | Train Loss: 0.064463 | Val Loss: 0.072517 | R2: 0.367488\n",
      "Model Checkpoint | epoch: 6 | best_val_loss: 0.07251749597779579\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 8/100 | Train Loss: 0.059198 | Val Loss: 0.069470 | R2: 0.392160\n",
      "Model Checkpoint | epoch: 7 | best_val_loss: 0.06946974747602104\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 9/100 | Train Loss: 0.056718 | Val Loss: 0.062634 | R2: 0.450639\n",
      "Model Checkpoint | epoch: 8 | best_val_loss: 0.0626343541248805\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 10/100 | Train Loss: 0.053805 | Val Loss: 0.068449 | R2: 0.401098\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 11/100 | Train Loss: 0.051320 | Val Loss: 0.062437 | R2: 0.453010\n",
      "Model Checkpoint | epoch: 10 | best_val_loss: 0.06243685094598267\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 12/100 | Train Loss: 0.049156 | Val Loss: 0.060663 | R2: 0.467043\n",
      "Model Checkpoint | epoch: 11 | best_val_loss: 0.060662631591161094\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 13/100 | Train Loss: 0.044901 | Val Loss: 0.060509 | R2: 0.467695\n",
      "Model Checkpoint | epoch: 12 | best_val_loss: 0.06050864104481621\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 14/100 | Train Loss: 0.044156 | Val Loss: 0.055031 | R2: 0.515058\n",
      "Model Checkpoint | epoch: 13 | best_val_loss: 0.05503110666417827\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 15/100 | Train Loss: 0.041263 | Val Loss: 0.055923 | R2: 0.507758\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 16/100 | Train Loss: 0.039765 | Val Loss: 0.052310 | R2: 0.538199\n",
      "Model Checkpoint | epoch: 15 | best_val_loss: 0.052310289009577696\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 17/100 | Train Loss: 0.038074 | Val Loss: 0.049521 | R2: 0.563913\n",
      "Model Checkpoint | epoch: 16 | best_val_loss: 0.049521165891653965\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 18/100 | Train Loss: 0.035849 | Val Loss: 0.049186 | R2: 0.565541\n",
      "Model Checkpoint | epoch: 17 | best_val_loss: 0.04918550078048267\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 19/100 | Train Loss: 0.033429 | Val Loss: 0.048725 | R2: 0.569779\n",
      "Model Checkpoint | epoch: 18 | best_val_loss: 0.048725434469990433\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 20/100 | Train Loss: 0.032896 | Val Loss: 0.043618 | R2: 0.613145\n",
      "Model Checkpoint | epoch: 19 | best_val_loss: 0.0436183568084509\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 21/100 | Train Loss: 0.032221 | Val Loss: 0.042874 | R2: 0.620532\n",
      "Model Checkpoint | epoch: 20 | best_val_loss: 0.04287361618265924\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 22/100 | Train Loss: 0.030137 | Val Loss: 0.044542 | R2: 0.606032\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 23/100 | Train Loss: 0.029175 | Val Loss: 0.046302 | R2: 0.593959\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 24/100 | Train Loss: 0.028585 | Val Loss: 0.041663 | R2: 0.631220\n",
      "Model Checkpoint | epoch: 23 | best_val_loss: 0.04166266426469924\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 25/100 | Train Loss: 0.027372 | Val Loss: 0.044249 | R2: 0.610200\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 26/100 | Train Loss: 0.027462 | Val Loss: 0.041312 | R2: 0.634767\n",
      "Model Checkpoint | epoch: 25 | best_val_loss: 0.04131201778559221\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 27/100 | Train Loss: 0.025975 | Val Loss: 0.040032 | R2: 0.645398\n",
      "Model Checkpoint | epoch: 26 | best_val_loss: 0.040031873621408726\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 28/100 | Train Loss: 0.026302 | Val Loss: 0.040157 | R2: 0.645877\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 29/100 | Train Loss: 0.024110 | Val Loss: 0.039766 | R2: 0.649645\n",
      "Model Checkpoint | epoch: 28 | best_val_loss: 0.03976627959869802\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 30/100 | Train Loss: 0.023380 | Val Loss: 0.038653 | R2: 0.659353\n",
      "Model Checkpoint | epoch: 29 | best_val_loss: 0.03865304797194484\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 31/100 | Train Loss: 0.023345 | Val Loss: 0.037062 | R2: 0.672337\n",
      "Model Checkpoint | epoch: 30 | best_val_loss: 0.03706150235649612\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 32/100 | Train Loss: 0.022307 | Val Loss: 0.035605 | R2: 0.685881\n",
      "Model Checkpoint | epoch: 31 | best_val_loss: 0.03560535524247421\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 33/100 | Train Loss: 0.021969 | Val Loss: 0.035907 | R2: 0.682669\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 34/100 | Train Loss: 0.019591 | Val Loss: 0.037162 | R2: 0.672691\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 35/100 | Train Loss: 0.020757 | Val Loss: 0.034733 | R2: 0.692693\n",
      "Model Checkpoint | epoch: 34 | best_val_loss: 0.03473276792290724\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 36/100 | Train Loss: 0.019695 | Val Loss: 0.033907 | R2: 0.699309\n",
      "Model Checkpoint | epoch: 35 | best_val_loss: 0.033906768582192146\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 37/100 | Train Loss: 0.018977 | Val Loss: 0.034547 | R2: 0.694724\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 38/100 | Train Loss: 0.018235 | Val Loss: 0.033484 | R2: 0.704145\n",
      "Model Checkpoint | epoch: 37 | best_val_loss: 0.03348432963499282\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 39/100 | Train Loss: 0.018258 | Val Loss: 0.031785 | R2: 0.719307\n",
      "Model Checkpoint | epoch: 38 | best_val_loss: 0.031784591213282615\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 40/100 | Train Loss: 0.017701 | Val Loss: 0.031535 | R2: 0.721332\n",
      "Model Checkpoint | epoch: 39 | best_val_loss: 0.03153493424616237\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 41/100 | Train Loss: 0.017027 | Val Loss: 0.030720 | R2: 0.728098\n",
      "Model Checkpoint | epoch: 40 | best_val_loss: 0.03072045224884318\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 42/100 | Train Loss: 0.016967 | Val Loss: 0.032385 | R2: 0.714074\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 43/100 | Train Loss: 0.016707 | Val Loss: 0.032464 | R2: 0.713172\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 44/100 | Train Loss: 0.016070 | Val Loss: 0.030279 | R2: 0.732858\n",
      "Model Checkpoint | epoch: 43 | best_val_loss: 0.030278587276762765\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 45/100 | Train Loss: 0.016139 | Val Loss: 0.030637 | R2: 0.728674\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 46/100 | Train Loss: 0.015995 | Val Loss: 0.031272 | R2: 0.724083\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 47/100 | Train Loss: 0.014809 | Val Loss: 0.030484 | R2: 0.730597\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 48/100 | Train Loss: 0.015919 | Val Loss: 0.032843 | R2: 0.709817\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 49/100 | Train Loss: 0.014943 | Val Loss: 0.027458 | R2: 0.755746\n",
      "Model Checkpoint | epoch: 48 | best_val_loss: 0.027458281566052595\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 50/100 | Train Loss: 0.013644 | Val Loss: 0.030927 | R2: 0.726051\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 51/100 | Train Loss: 0.014676 | Val Loss: 0.029222 | R2: 0.741405\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 52/100 | Train Loss: 0.014024 | Val Loss: 0.029734 | R2: 0.735616\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 53/100 | Train Loss: 0.015714 | Val Loss: 0.027249 | R2: 0.756970\n",
      "Model Checkpoint | epoch: 52 | best_val_loss: 0.027249294065171854\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 54/100 | Train Loss: 0.012733 | Val Loss: 0.027166 | R2: 0.758110\n",
      "Model Checkpoint | epoch: 53 | best_val_loss: 0.02716640626586094\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 55/100 | Train Loss: 0.013376 | Val Loss: 0.026564 | R2: 0.763282\n",
      "Model Checkpoint | epoch: 54 | best_val_loss: 0.026563592884262712\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 56/100 | Train Loss: 0.013545 | Val Loss: 0.027095 | R2: 0.758320\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 57/100 | Train Loss: 0.012624 | Val Loss: 0.027585 | R2: 0.754426\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 58/100 | Train Loss: 0.012786 | Val Loss: 0.025434 | R2: 0.772738\n",
      "Model Checkpoint | epoch: 57 | best_val_loss: 0.02543444681390732\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 59/100 | Train Loss: 0.013012 | Val Loss: 0.027335 | R2: 0.756186\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 60/100 | Train Loss: 0.012340 | Val Loss: 0.024927 | R2: 0.777781\n",
      "Model Checkpoint | epoch: 59 | best_val_loss: 0.024927235684564543\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 61/100 | Train Loss: 0.012683 | Val Loss: 0.025549 | R2: 0.772583\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 62/100 | Train Loss: 0.012053 | Val Loss: 0.023589 | R2: 0.789208\n",
      "Model Checkpoint | epoch: 61 | best_val_loss: 0.023588651482080523\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 63/100 | Train Loss: 0.011751 | Val Loss: 0.025064 | R2: 0.776347\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 64/100 | Train Loss: 0.012311 | Val Loss: 0.026672 | R2: 0.763062\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 65/100 | Train Loss: 0.011035 | Val Loss: 0.024204 | R2: 0.784345\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 66/100 | Train Loss: 0.011080 | Val Loss: 0.023260 | R2: 0.791062\n",
      "Model Checkpoint | epoch: 65 | best_val_loss: 0.02325951250893478\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 67/100 | Train Loss: 0.011032 | Val Loss: 0.028078 | R2: 0.750693\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 68/100 | Train Loss: 0.012092 | Val Loss: 0.025495 | R2: 0.773242\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 69/100 | Train Loss: 0.010900 | Val Loss: 0.023303 | R2: 0.792960\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 70/100 | Train Loss: 0.011014 | Val Loss: 0.026079 | R2: 0.767708\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 71/100 | Train Loss: 0.012757 | Val Loss: 0.023819 | R2: 0.786957\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 72/100 | Train Loss: 0.010930 | Val Loss: 0.023819 | R2: 0.786291\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 73/100 | Train Loss: 0.009318 | Val Loss: 0.022538 | R2: 0.799380\n",
      "Model Checkpoint | epoch: 72 | best_val_loss: 0.0225378786550589\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 74/100 | Train Loss: 0.008677 | Val Loss: 0.021070 | R2: 0.811385\n",
      "Model Checkpoint | epoch: 73 | best_val_loss: 0.021070193650847713\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 75/100 | Train Loss: 0.008004 | Val Loss: 0.021119 | R2: 0.811241\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 76/100 | Train Loss: 0.007832 | Val Loss: 0.020716 | R2: 0.815263\n",
      "Model Checkpoint | epoch: 75 | best_val_loss: 0.02071619605631309\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 77/100 | Train Loss: 0.007936 | Val Loss: 0.019935 | R2: 0.821482\n",
      "Model Checkpoint | epoch: 76 | best_val_loss: 0.019935267424424336\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 78/100 | Train Loss: 0.007588 | Val Loss: 0.020834 | R2: 0.814569\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 79/100 | Train Loss: 0.007605 | Val Loss: 0.019670 | R2: 0.824344\n",
      "Model Checkpoint | epoch: 78 | best_val_loss: 0.019670074900819196\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 80/100 | Train Loss: 0.007441 | Val Loss: 0.019641 | R2: 0.824767\n",
      "Model Checkpoint | epoch: 79 | best_val_loss: 0.01964094445704379\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 81/100 | Train Loss: 0.006987 | Val Loss: 0.019282 | R2: 0.827856\n",
      "Model Checkpoint | epoch: 80 | best_val_loss: 0.019281973299932562\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 82/100 | Train Loss: 0.006948 | Val Loss: 0.019371 | R2: 0.826820\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 83/100 | Train Loss: 0.007004 | Val Loss: 0.019785 | R2: 0.823110\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 84/100 | Train Loss: 0.007073 | Val Loss: 0.019305 | R2: 0.827528\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 85/100 | Train Loss: 0.007011 | Val Loss: 0.019381 | R2: 0.827119\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 86/100 | Train Loss: 0.006515 | Val Loss: 0.018808 | R2: 0.832000\n",
      "Model Checkpoint | epoch: 85 | best_val_loss: 0.01880848885037833\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 87/100 | Train Loss: 0.006235 | Val Loss: 0.018772 | R2: 0.832893\n",
      "Model Checkpoint | epoch: 86 | best_val_loss: 0.018771828226939156\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 88/100 | Train Loss: 0.006225 | Val Loss: 0.018287 | R2: 0.837095\n",
      "Model Checkpoint | epoch: 87 | best_val_loss: 0.018286920263548383\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 89/100 | Train Loss: 0.006506 | Val Loss: 0.018877 | R2: 0.832507\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 90/100 | Train Loss: 0.006350 | Val Loss: 0.018680 | R2: 0.834781\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 91/100 | Train Loss: 0.006151 | Val Loss: 0.018421 | R2: 0.837018\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 92/100 | Train Loss: 0.005998 | Val Loss: 0.017523 | R2: 0.844483\n",
      "Model Checkpoint | epoch: 91 | best_val_loss: 0.017522798661116716\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 93/100 | Train Loss: 0.005829 | Val Loss: 0.018005 | R2: 0.841150\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 94/100 | Train Loss: 0.005921 | Val Loss: 0.018015 | R2: 0.840784\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 95/100 | Train Loss: 0.005689 | Val Loss: 0.017184 | R2: 0.848006\n",
      "Model Checkpoint | epoch: 94 | best_val_loss: 0.017183754553672367\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 96/100 | Train Loss: 0.005420 | Val Loss: 0.016909 | R2: 0.849939\n",
      "Model Checkpoint | epoch: 95 | best_val_loss: 0.016908921063088604\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 97/100 | Train Loss: 0.005484 | Val Loss: 0.016581 | R2: 0.852793\n",
      "Model Checkpoint | epoch: 96 | best_val_loss: 0.01658148513461088\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 98/100 | Train Loss: 0.005581 | Val Loss: 0.016937 | R2: 0.849681\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 99/100 | Train Loss: 0.005391 | Val Loss: 0.016572 | R2: 0.852156\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_5 Epoch 100/100 | Train Loss: 0.005553 | Val Loss: 0.017097 | R2: 0.848913\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 1/100 | Train Loss: 0.075170 | Val Loss: 0.071005 | R2: 0.020663\n",
      "Model Checkpoint | epoch: 0 | best_val_loss: 0.071005458821853\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 2/100 | Train Loss: 0.070413 | Val Loss: 0.069378 | R2: 0.042822\n",
      "Model Checkpoint | epoch: 1 | best_val_loss: 0.06937827128130529\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 3/100 | Train Loss: 0.064840 | Val Loss: 0.063916 | R2: 0.118733\n",
      "Model Checkpoint | epoch: 2 | best_val_loss: 0.06391565628028992\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 4/100 | Train Loss: 0.056253 | Val Loss: 0.055205 | R2: 0.241144\n",
      "Model Checkpoint | epoch: 3 | best_val_loss: 0.05520537642058399\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 5/100 | Train Loss: 0.046086 | Val Loss: 0.052241 | R2: 0.273585\n",
      "Model Checkpoint | epoch: 4 | best_val_loss: 0.05224135902151465\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 6/100 | Train Loss: 0.040216 | Val Loss: 0.041589 | R2: 0.426695\n",
      "Model Checkpoint | epoch: 5 | best_val_loss: 0.04158931521755747\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 7/100 | Train Loss: 0.037470 | Val Loss: 0.042743 | R2: 0.409792\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 8/100 | Train Loss: 0.033763 | Val Loss: 0.040139 | R2: 0.445622\n",
      "Model Checkpoint | epoch: 7 | best_val_loss: 0.04013922726005937\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 9/100 | Train Loss: 0.032002 | Val Loss: 0.038027 | R2: 0.473509\n",
      "Model Checkpoint | epoch: 8 | best_val_loss: 0.03802710894681513\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 10/100 | Train Loss: 0.028947 | Val Loss: 0.031139 | R2: 0.564343\n",
      "Model Checkpoint | epoch: 9 | best_val_loss: 0.0311394920968968\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 11/100 | Train Loss: 0.027999 | Val Loss: 0.032418 | R2: 0.547574\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 12/100 | Train Loss: 0.026930 | Val Loss: 0.030721 | R2: 0.572940\n",
      "Model Checkpoint | epoch: 11 | best_val_loss: 0.030721206926507875\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 13/100 | Train Loss: 0.024810 | Val Loss: 0.030562 | R2: 0.575517\n",
      "Model Checkpoint | epoch: 12 | best_val_loss: 0.03056246276380908\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 14/100 | Train Loss: 0.023514 | Val Loss: 0.029613 | R2: 0.588657\n",
      "Model Checkpoint | epoch: 13 | best_val_loss: 0.029612969524434044\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 15/100 | Train Loss: 0.023656 | Val Loss: 0.027255 | R2: 0.621938\n",
      "Model Checkpoint | epoch: 14 | best_val_loss: 0.027254844597178616\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 16/100 | Train Loss: 0.022981 | Val Loss: 0.029266 | R2: 0.593717\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 17/100 | Train Loss: 0.021998 | Val Loss: 0.025732 | R2: 0.640359\n",
      "Model Checkpoint | epoch: 16 | best_val_loss: 0.025732316167352515\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 18/100 | Train Loss: 0.020205 | Val Loss: 0.027794 | R2: 0.615514\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 19/100 | Train Loss: 0.022778 | Val Loss: 0.026863 | R2: 0.619169\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 20/100 | Train Loss: 0.020253 | Val Loss: 0.024873 | R2: 0.652725\n",
      "Model Checkpoint | epoch: 19 | best_val_loss: 0.024872643766355597\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 21/100 | Train Loss: 0.018706 | Val Loss: 0.024995 | R2: 0.653916\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 22/100 | Train Loss: 0.018658 | Val Loss: 0.024177 | R2: 0.666550\n",
      "Model Checkpoint | epoch: 21 | best_val_loss: 0.02417692185865922\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 23/100 | Train Loss: 0.017373 | Val Loss: 0.023480 | R2: 0.671933\n",
      "Model Checkpoint | epoch: 22 | best_val_loss: 0.023479816071352817\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 24/100 | Train Loss: 0.016536 | Val Loss: 0.023500 | R2: 0.672777\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 25/100 | Train Loss: 0.018014 | Val Loss: 0.022473 | R2: 0.686846\n",
      "Model Checkpoint | epoch: 24 | best_val_loss: 0.022472679414217257\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 26/100 | Train Loss: 0.016364 | Val Loss: 0.023614 | R2: 0.674167\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 27/100 | Train Loss: 0.015858 | Val Loss: 0.022493 | R2: 0.689457\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 28/100 | Train Loss: 0.017092 | Val Loss: 0.021943 | R2: 0.697168\n",
      "Model Checkpoint | epoch: 27 | best_val_loss: 0.021942926523301542\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 29/100 | Train Loss: 0.015964 | Val Loss: 0.021909 | R2: 0.692121\n",
      "Model Checkpoint | epoch: 28 | best_val_loss: 0.021908760077087207\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 30/100 | Train Loss: 0.014925 | Val Loss: 0.020368 | R2: 0.715572\n",
      "Model Checkpoint | epoch: 29 | best_val_loss: 0.020367745922257503\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 31/100 | Train Loss: 0.015338 | Val Loss: 0.020599 | R2: 0.715535\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 32/100 | Train Loss: 0.015186 | Val Loss: 0.022230 | R2: 0.690697\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 33/100 | Train Loss: 0.015814 | Val Loss: 0.022341 | R2: 0.692072\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 34/100 | Train Loss: 0.013477 | Val Loss: 0.019735 | R2: 0.723968\n",
      "Model Checkpoint | epoch: 33 | best_val_loss: 0.019735275872129327\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 35/100 | Train Loss: 0.013510 | Val Loss: 0.018852 | R2: 0.739875\n",
      "Model Checkpoint | epoch: 34 | best_val_loss: 0.018851669073337688\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 36/100 | Train Loss: 0.014831 | Val Loss: 0.020248 | R2: 0.716137\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 37/100 | Train Loss: 0.014112 | Val Loss: 0.022065 | R2: 0.696606\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 38/100 | Train Loss: 0.015137 | Val Loss: 0.020086 | R2: 0.721920\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 39/100 | Train Loss: 0.012340 | Val Loss: 0.018987 | R2: 0.734317\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 40/100 | Train Loss: 0.012123 | Val Loss: 0.018894 | R2: 0.734757\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 41/100 | Train Loss: 0.015336 | Val Loss: 0.019242 | R2: 0.729720\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 42/100 | Train Loss: 0.011693 | Val Loss: 0.017692 | R2: 0.751593\n",
      "Model Checkpoint | epoch: 41 | best_val_loss: 0.017691650908854273\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 43/100 | Train Loss: 0.010672 | Val Loss: 0.017119 | R2: 0.759979\n",
      "Model Checkpoint | epoch: 42 | best_val_loss: 0.017119256749542223\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 44/100 | Train Loss: 0.010496 | Val Loss: 0.016683 | R2: 0.762741\n",
      "Model Checkpoint | epoch: 43 | best_val_loss: 0.016682852978274848\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 45/100 | Train Loss: 0.010300 | Val Loss: 0.016079 | R2: 0.772473\n",
      "Model Checkpoint | epoch: 44 | best_val_loss: 0.016078675711605078\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 46/100 | Train Loss: 0.010075 | Val Loss: 0.015716 | R2: 0.779612\n",
      "Model Checkpoint | epoch: 45 | best_val_loss: 0.015716022308139753\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 47/100 | Train Loss: 0.009953 | Val Loss: 0.016345 | R2: 0.764272\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 48/100 | Train Loss: 0.009437 | Val Loss: 0.015301 | R2: 0.781028\n",
      "Model Checkpoint | epoch: 47 | best_val_loss: 0.015301298703222225\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 49/100 | Train Loss: 0.009140 | Val Loss: 0.014880 | R2: 0.788021\n",
      "Model Checkpoint | epoch: 48 | best_val_loss: 0.014880093214535412\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 50/100 | Train Loss: 0.009052 | Val Loss: 0.014480 | R2: 0.795218\n",
      "Model Checkpoint | epoch: 49 | best_val_loss: 0.014479679937616716\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 51/100 | Train Loss: 0.008645 | Val Loss: 0.014654 | R2: 0.789984\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 52/100 | Train Loss: 0.008443 | Val Loss: 0.014361 | R2: 0.795676\n",
      "Model Checkpoint | epoch: 51 | best_val_loss: 0.01436139094168579\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 53/100 | Train Loss: 0.008122 | Val Loss: 0.013474 | R2: 0.808466\n",
      "Model Checkpoint | epoch: 52 | best_val_loss: 0.013474414956031574\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 54/100 | Train Loss: 0.008435 | Val Loss: 0.013402 | R2: 0.808230\n",
      "Model Checkpoint | epoch: 53 | best_val_loss: 0.013401834402201024\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 55/100 | Train Loss: 0.007841 | Val Loss: 0.012824 | R2: 0.816776\n",
      "Model Checkpoint | epoch: 54 | best_val_loss: 0.012824295980260811\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 56/100 | Train Loss: 0.007803 | Val Loss: 0.012910 | R2: 0.814265\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 57/100 | Train Loss: 0.007486 | Val Loss: 0.012431 | R2: 0.822331\n",
      "Model Checkpoint | epoch: 56 | best_val_loss: 0.012430945396811392\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 58/100 | Train Loss: 0.007767 | Val Loss: 0.013153 | R2: 0.812958\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 59/100 | Train Loss: 0.007727 | Val Loss: 0.011900 | R2: 0.829456\n",
      "Model Checkpoint | epoch: 58 | best_val_loss: 0.011899838104009783\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 60/100 | Train Loss: 0.007050 | Val Loss: 0.012081 | R2: 0.828832\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 61/100 | Train Loss: 0.006852 | Val Loss: 0.011137 | R2: 0.841479\n",
      "Model Checkpoint | epoch: 60 | best_val_loss: 0.011136540887140048\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 62/100 | Train Loss: 0.006605 | Val Loss: 0.010875 | R2: 0.846042\n",
      "Model Checkpoint | epoch: 61 | best_val_loss: 0.01087533396377694\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 63/100 | Train Loss: 0.006472 | Val Loss: 0.011166 | R2: 0.841862\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 64/100 | Train Loss: 0.006184 | Val Loss: 0.010964 | R2: 0.844570\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 65/100 | Train Loss: 0.006209 | Val Loss: 0.011036 | R2: 0.844049\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 66/100 | Train Loss: 0.006403 | Val Loss: 0.010980 | R2: 0.844654\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 67/100 | Train Loss: 0.005880 | Val Loss: 0.011111 | R2: 0.842293\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 68/100 | Train Loss: 0.005836 | Val Loss: 0.010971 | R2: 0.842817\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 69/100 | Train Loss: 0.005642 | Val Loss: 0.010458 | R2: 0.851412\n",
      "Model Checkpoint | epoch: 68 | best_val_loss: 0.010458107260403469\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 70/100 | Train Loss: 0.005188 | Val Loss: 0.009746 | R2: 0.861006\n",
      "Model Checkpoint | epoch: 69 | best_val_loss: 0.009746149106801669\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 71/100 | Train Loss: 0.005096 | Val Loss: 0.010084 | R2: 0.856468\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 72/100 | Train Loss: 0.005153 | Val Loss: 0.010075 | R2: 0.855665\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 73/100 | Train Loss: 0.005009 | Val Loss: 0.009674 | R2: 0.861663\n",
      "Model Checkpoint | epoch: 72 | best_val_loss: 0.009673580127179674\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 74/100 | Train Loss: 0.004903 | Val Loss: 0.009546 | R2: 0.863644\n",
      "Model Checkpoint | epoch: 73 | best_val_loss: 0.00954552800982492\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 75/100 | Train Loss: 0.004666 | Val Loss: 0.009528 | R2: 0.863936\n",
      "Model Checkpoint | epoch: 74 | best_val_loss: 0.009528232152198649\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 76/100 | Train Loss: 0.004600 | Val Loss: 0.009791 | R2: 0.859986\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 77/100 | Train Loss: 0.004709 | Val Loss: 0.009613 | R2: 0.861182\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 78/100 | Train Loss: 0.004581 | Val Loss: 0.009604 | R2: 0.863324\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 79/100 | Train Loss: 0.004589 | Val Loss: 0.008742 | R2: 0.875597\n",
      "Model Checkpoint | epoch: 78 | best_val_loss: 0.008742229809421891\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 80/100 | Train Loss: 0.004484 | Val Loss: 0.008798 | R2: 0.874909\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 81/100 | Train Loss: 0.004389 | Val Loss: 0.008703 | R2: 0.875632\n",
      "Model Checkpoint | epoch: 80 | best_val_loss: 0.008703027283161646\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 82/100 | Train Loss: 0.004289 | Val Loss: 0.008902 | R2: 0.872434\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 83/100 | Train Loss: 0.004298 | Val Loss: 0.009105 | R2: 0.869696\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 84/100 | Train Loss: 0.004201 | Val Loss: 0.008847 | R2: 0.872982\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 85/100 | Train Loss: 0.004091 | Val Loss: 0.008876 | R2: 0.872513\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 86/100 | Train Loss: 0.004118 | Val Loss: 0.009003 | R2: 0.870689\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 87/100 | Train Loss: 0.004111 | Val Loss: 0.008783 | R2: 0.873416\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 88/100 | Train Loss: 0.003870 | Val Loss: 0.008692 | R2: 0.874665\n",
      "Model Checkpoint | epoch: 87 | best_val_loss: 0.008692452798759203\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 89/100 | Train Loss: 0.003896 | Val Loss: 0.008671 | R2: 0.874898\n",
      "Model Checkpoint | epoch: 88 | best_val_loss: 0.008671408106582627\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 90/100 | Train Loss: 0.003799 | Val Loss: 0.008445 | R2: 0.878298\n",
      "Model Checkpoint | epoch: 89 | best_val_loss: 0.008445198294648435\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 91/100 | Train Loss: 0.003630 | Val Loss: 0.008645 | R2: 0.876077\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 92/100 | Train Loss: 0.003650 | Val Loss: 0.008797 | R2: 0.873976\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 93/100 | Train Loss: 0.003575 | Val Loss: 0.008323 | R2: 0.880396\n",
      "Model Checkpoint | epoch: 92 | best_val_loss: 0.008323135937709595\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 94/100 | Train Loss: 0.003517 | Val Loss: 0.008215 | R2: 0.882199\n",
      "Model Checkpoint | epoch: 93 | best_val_loss: 0.008215377378338922\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 95/100 | Train Loss: 0.003504 | Val Loss: 0.008422 | R2: 0.879393\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 96/100 | Train Loss: 0.003478 | Val Loss: 0.008337 | R2: 0.880426\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 97/100 | Train Loss: 0.003408 | Val Loss: 0.008220 | R2: 0.881773\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 98/100 | Train Loss: 0.003424 | Val Loss: 0.007921 | R2: 0.886085\n",
      "Model Checkpoint | epoch: 97 | best_val_loss: 0.007920511949956159\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 99/100 | Train Loss: 0.003436 | Val Loss: 0.008136 | R2: 0.882988\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_6 Epoch 100/100 | Train Loss: 0.003320 | Val Loss: 0.007984 | R2: 0.885180\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 1/100 | Train Loss: 0.071281 | Val Loss: 0.060651 | R2: 0.016505\n",
      "Model Checkpoint | epoch: 0 | best_val_loss: 0.06065103944432404\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 2/100 | Train Loss: 0.066476 | Val Loss: 0.057504 | R2: 0.068269\n",
      "Model Checkpoint | epoch: 1 | best_val_loss: 0.057503768240412076\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 3/100 | Train Loss: 0.057912 | Val Loss: 0.050306 | R2: 0.185991\n",
      "Model Checkpoint | epoch: 2 | best_val_loss: 0.05030641472634549\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 4/100 | Train Loss: 0.048682 | Val Loss: 0.047263 | R2: 0.235250\n",
      "Model Checkpoint | epoch: 3 | best_val_loss: 0.04726315682202888\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 5/100 | Train Loss: 0.039170 | Val Loss: 0.041772 | R2: 0.325196\n",
      "Model Checkpoint | epoch: 4 | best_val_loss: 0.04177239502345522\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 6/100 | Train Loss: 0.035219 | Val Loss: 0.035951 | R2: 0.420061\n",
      "Model Checkpoint | epoch: 5 | best_val_loss: 0.03595057946598778\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 7/100 | Train Loss: 0.032093 | Val Loss: 0.038554 | R2: 0.379017\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 8/100 | Train Loss: 0.029052 | Val Loss: 0.036110 | R2: 0.415284\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 9/100 | Train Loss: 0.027174 | Val Loss: 0.036590 | R2: 0.409584\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 10/100 | Train Loss: 0.026830 | Val Loss: 0.035008 | R2: 0.435086\n",
      "Model Checkpoint | epoch: 9 | best_val_loss: 0.03500842214437822\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 11/100 | Train Loss: 0.024484 | Val Loss: 0.029281 | R2: 0.527439\n",
      "Model Checkpoint | epoch: 10 | best_val_loss: 0.02928133144034008\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 12/100 | Train Loss: 0.023929 | Val Loss: 0.029588 | R2: 0.522848\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 13/100 | Train Loss: 0.022338 | Val Loss: 0.030140 | R2: 0.511432\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 14/100 | Train Loss: 0.020275 | Val Loss: 0.028189 | R2: 0.545705\n",
      "Model Checkpoint | epoch: 13 | best_val_loss: 0.028189021441645715\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 15/100 | Train Loss: 0.018805 | Val Loss: 0.025711 | R2: 0.584767\n",
      "Model Checkpoint | epoch: 14 | best_val_loss: 0.025711498759531725\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 16/100 | Train Loss: 0.016884 | Val Loss: 0.023859 | R2: 0.613047\n",
      "Model Checkpoint | epoch: 15 | best_val_loss: 0.023858539939534643\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 17/100 | Train Loss: 0.018162 | Val Loss: 0.027008 | R2: 0.565323\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 18/100 | Train Loss: 0.015708 | Val Loss: 0.023393 | R2: 0.620587\n",
      "Model Checkpoint | epoch: 17 | best_val_loss: 0.023393482964007287\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 19/100 | Train Loss: 0.015494 | Val Loss: 0.023665 | R2: 0.618027\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 20/100 | Train Loss: 0.017017 | Val Loss: 0.021026 | R2: 0.661316\n",
      "Model Checkpoint | epoch: 19 | best_val_loss: 0.021025570768759484\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 21/100 | Train Loss: 0.015640 | Val Loss: 0.018818 | R2: 0.695845\n",
      "Model Checkpoint | epoch: 20 | best_val_loss: 0.01881831808424451\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 22/100 | Train Loss: 0.013730 | Val Loss: 0.019721 | R2: 0.680213\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 23/100 | Train Loss: 0.014884 | Val Loss: 0.019754 | R2: 0.680373\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 24/100 | Train Loss: 0.013597 | Val Loss: 0.018517 | R2: 0.701657\n",
      "Model Checkpoint | epoch: 23 | best_val_loss: 0.0185165642692009\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 25/100 | Train Loss: 0.014888 | Val Loss: 0.018580 | R2: 0.700246\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 26/100 | Train Loss: 0.012367 | Val Loss: 0.016783 | R2: 0.729649\n",
      "Model Checkpoint | epoch: 25 | best_val_loss: 0.016782912348106038\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 27/100 | Train Loss: 0.012441 | Val Loss: 0.018982 | R2: 0.693002\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 28/100 | Train Loss: 0.013327 | Val Loss: 0.017274 | R2: 0.719716\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 29/100 | Train Loss: 0.011116 | Val Loss: 0.017563 | R2: 0.712677\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 30/100 | Train Loss: 0.012835 | Val Loss: 0.017946 | R2: 0.708766\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 31/100 | Train Loss: 0.012099 | Val Loss: 0.015730 | R2: 0.746305\n",
      "Model Checkpoint | epoch: 30 | best_val_loss: 0.01573022463886688\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 32/100 | Train Loss: 0.011407 | Val Loss: 0.017479 | R2: 0.718437\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 33/100 | Train Loss: 0.011496 | Val Loss: 0.017811 | R2: 0.712701\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 34/100 | Train Loss: 0.010635 | Val Loss: 0.016533 | R2: 0.733631\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 35/100 | Train Loss: 0.011634 | Val Loss: 0.015837 | R2: 0.745247\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 36/100 | Train Loss: 0.010200 | Val Loss: 0.015655 | R2: 0.748713\n",
      "Model Checkpoint | epoch: 35 | best_val_loss: 0.015654821509792884\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 37/100 | Train Loss: 0.010225 | Val Loss: 0.014777 | R2: 0.763311\n",
      "Model Checkpoint | epoch: 36 | best_val_loss: 0.01477670134130555\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 38/100 | Train Loss: 0.010238 | Val Loss: 0.014236 | R2: 0.770595\n",
      "Model Checkpoint | epoch: 37 | best_val_loss: 0.014236361880683236\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 39/100 | Train Loss: 0.009449 | Val Loss: 0.013438 | R2: 0.783396\n",
      "Model Checkpoint | epoch: 38 | best_val_loss: 0.013438224064186216\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 40/100 | Train Loss: 0.010294 | Val Loss: 0.014477 | R2: 0.766659\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 41/100 | Train Loss: 0.010352 | Val Loss: 0.014515 | R2: 0.767462\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 42/100 | Train Loss: 0.009007 | Val Loss: 0.013661 | R2: 0.780840\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 43/100 | Train Loss: 0.009511 | Val Loss: 0.014261 | R2: 0.770740\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 44/100 | Train Loss: 0.008965 | Val Loss: 0.013486 | R2: 0.783012\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 45/100 | Train Loss: 0.008873 | Val Loss: 0.014511 | R2: 0.766710\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 46/100 | Train Loss: 0.008268 | Val Loss: 0.011194 | R2: 0.820418\n",
      "Model Checkpoint | epoch: 45 | best_val_loss: 0.011193983930447656\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 47/100 | Train Loss: 0.007631 | Val Loss: 0.011796 | R2: 0.809132\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 48/100 | Train Loss: 0.007464 | Val Loss: 0.011389 | R2: 0.816077\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 49/100 | Train Loss: 0.006825 | Val Loss: 0.010527 | R2: 0.830670\n",
      "Model Checkpoint | epoch: 48 | best_val_loss: 0.010526733472606995\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 50/100 | Train Loss: 0.006443 | Val Loss: 0.010126 | R2: 0.837386\n",
      "Model Checkpoint | epoch: 49 | best_val_loss: 0.010126023214100213\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 51/100 | Train Loss: 0.006347 | Val Loss: 0.010023 | R2: 0.839447\n",
      "Model Checkpoint | epoch: 50 | best_val_loss: 0.010023365915448975\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 52/100 | Train Loss: 0.006169 | Val Loss: 0.009754 | R2: 0.843665\n",
      "Model Checkpoint | epoch: 51 | best_val_loss: 0.009754260006535332\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 53/100 | Train Loss: 0.006091 | Val Loss: 0.010256 | R2: 0.834778\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 54/100 | Train Loss: 0.006142 | Val Loss: 0.009610 | R2: 0.845662\n",
      "Model Checkpoint | epoch: 53 | best_val_loss: 0.009610016723690528\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 55/100 | Train Loss: 0.006357 | Val Loss: 0.008432 | R2: 0.863922\n",
      "Model Checkpoint | epoch: 54 | best_val_loss: 0.008431860417224622\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 56/100 | Train Loss: 0.006140 | Val Loss: 0.008492 | R2: 0.863270\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 57/100 | Train Loss: 0.006327 | Val Loss: 0.008179 | R2: 0.868291\n",
      "Model Checkpoint | epoch: 56 | best_val_loss: 0.00817871420330549\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 58/100 | Train Loss: 0.005543 | Val Loss: 0.008121 | R2: 0.869077\n",
      "Model Checkpoint | epoch: 57 | best_val_loss: 0.008120798046584242\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 59/100 | Train Loss: 0.005651 | Val Loss: 0.008463 | R2: 0.863319\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 60/100 | Train Loss: 0.005471 | Val Loss: 0.008398 | R2: 0.864221\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 61/100 | Train Loss: 0.005698 | Val Loss: 0.008628 | R2: 0.860009\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 62/100 | Train Loss: 0.005437 | Val Loss: 0.008288 | R2: 0.864919\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 63/100 | Train Loss: 0.005109 | Val Loss: 0.007982 | R2: 0.869868\n",
      "Model Checkpoint | epoch: 62 | best_val_loss: 0.007982190114368374\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 64/100 | Train Loss: 0.005102 | Val Loss: 0.007645 | R2: 0.875893\n",
      "Model Checkpoint | epoch: 63 | best_val_loss: 0.007644790456403926\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 65/100 | Train Loss: 0.005334 | Val Loss: 0.008241 | R2: 0.864773\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 66/100 | Train Loss: 0.004770 | Val Loss: 0.007526 | R2: 0.876627\n",
      "Model Checkpoint | epoch: 65 | best_val_loss: 0.007526355314909273\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 67/100 | Train Loss: 0.004573 | Val Loss: 0.007478 | R2: 0.877763\n",
      "Model Checkpoint | epoch: 66 | best_val_loss: 0.0074783591016118105\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 68/100 | Train Loss: 0.004891 | Val Loss: 0.007409 | R2: 0.879424\n",
      "Model Checkpoint | epoch: 67 | best_val_loss: 0.0074093424287340085\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 69/100 | Train Loss: 0.004585 | Val Loss: 0.007315 | R2: 0.880572\n",
      "Model Checkpoint | epoch: 68 | best_val_loss: 0.007314735045320251\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 70/100 | Train Loss: 0.004495 | Val Loss: 0.008131 | R2: 0.866543\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 71/100 | Train Loss: 0.004415 | Val Loss: 0.008105 | R2: 0.866512\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 72/100 | Train Loss: 0.004350 | Val Loss: 0.008291 | R2: 0.864019\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 73/100 | Train Loss: 0.004043 | Val Loss: 0.007265 | R2: 0.881487\n",
      "Model Checkpoint | epoch: 72 | best_val_loss: 0.0072649195253616195\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 74/100 | Train Loss: 0.004197 | Val Loss: 0.007517 | R2: 0.877140\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 75/100 | Train Loss: 0.004298 | Val Loss: 0.008475 | R2: 0.862145\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 76/100 | Train Loss: 0.004637 | Val Loss: 0.006531 | R2: 0.893554\n",
      "Model Checkpoint | epoch: 75 | best_val_loss: 0.006531323756789788\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 77/100 | Train Loss: 0.003800 | Val Loss: 0.006865 | R2: 0.887866\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 78/100 | Train Loss: 0.003773 | Val Loss: 0.006808 | R2: 0.888920\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 79/100 | Train Loss: 0.004101 | Val Loss: 0.007052 | R2: 0.885093\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 80/100 | Train Loss: 0.004607 | Val Loss: 0.011283 | R2: 0.814271\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 81/100 | Train Loss: 0.004376 | Val Loss: 0.007009 | R2: 0.885119\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 82/100 | Train Loss: 0.003660 | Val Loss: 0.006734 | R2: 0.890173\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 83/100 | Train Loss: 0.003490 | Val Loss: 0.006802 | R2: 0.888850\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 84/100 | Train Loss: 0.003422 | Val Loss: 0.006554 | R2: 0.892807\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 85/100 | Train Loss: 0.003419 | Val Loss: 0.007530 | R2: 0.876344\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 86/100 | Train Loss: 0.003351 | Val Loss: 0.006368 | R2: 0.896174\n",
      "Model Checkpoint | epoch: 85 | best_val_loss: 0.006368314184659135\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 87/100 | Train Loss: 0.003242 | Val Loss: 0.006126 | R2: 0.900286\n",
      "Model Checkpoint | epoch: 86 | best_val_loss: 0.006125613913706426\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 88/100 | Train Loss: 0.003222 | Val Loss: 0.006511 | R2: 0.893348\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 89/100 | Train Loss: 0.003426 | Val Loss: 0.005989 | R2: 0.902629\n",
      "Model Checkpoint | epoch: 88 | best_val_loss: 0.005989015041215882\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 90/100 | Train Loss: 0.003267 | Val Loss: 0.005876 | R2: 0.904616\n",
      "Model Checkpoint | epoch: 89 | best_val_loss: 0.005875993417464391\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 91/100 | Train Loss: 0.003142 | Val Loss: 0.006073 | R2: 0.900951\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 92/100 | Train Loss: 0.003160 | Val Loss: 0.006124 | R2: 0.899924\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 93/100 | Train Loss: 0.003051 | Val Loss: 0.005787 | R2: 0.906073\n",
      "Model Checkpoint | epoch: 92 | best_val_loss: 0.0057868152519093\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 94/100 | Train Loss: 0.002995 | Val Loss: 0.005843 | R2: 0.904742\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 95/100 | Train Loss: 0.002962 | Val Loss: 0.006108 | R2: 0.899718\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 96/100 | Train Loss: 0.002956 | Val Loss: 0.006484 | R2: 0.893223\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 97/100 | Train Loss: 0.002849 | Val Loss: 0.006510 | R2: 0.892838\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 98/100 | Train Loss: 0.003003 | Val Loss: 0.005570 | R2: 0.908987\n",
      "Model Checkpoint | epoch: 97 | best_val_loss: 0.005569558818402584\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 99/100 | Train Loss: 0.002855 | Val Loss: 0.005619 | R2: 0.908307\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_7 Epoch 100/100 | Train Loss: 0.002788 | Val Loss: 0.005548 | R2: 0.909637\n",
      "Model Checkpoint | epoch: 99 | best_val_loss: 0.005548278828704497\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 1/100 | Train Loss: 0.080346 | Val Loss: 0.064144 | R2: 0.118088\n",
      "Model Checkpoint | epoch: 0 | best_val_loss: 0.06414421742321509\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 2/100 | Train Loss: 0.070015 | Val Loss: 0.061333 | R2: 0.159976\n",
      "Model Checkpoint | epoch: 1 | best_val_loss: 0.06133264040527865\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 3/100 | Train Loss: 0.061525 | Val Loss: 0.055992 | R2: 0.238740\n",
      "Model Checkpoint | epoch: 2 | best_val_loss: 0.055991953250124224\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 4/100 | Train Loss: 0.053073 | Val Loss: 0.055759 | R2: 0.244822\n",
      "Model Checkpoint | epoch: 3 | best_val_loss: 0.05575854160067522\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 5/100 | Train Loss: 0.048231 | Val Loss: 0.045951 | R2: 0.383238\n",
      "Model Checkpoint | epoch: 4 | best_val_loss: 0.04595108948306491\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 6/100 | Train Loss: 0.042033 | Val Loss: 0.045339 | R2: 0.393182\n",
      "Model Checkpoint | epoch: 5 | best_val_loss: 0.04533862213759372\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 7/100 | Train Loss: 0.040575 | Val Loss: 0.045612 | R2: 0.386887\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 8/100 | Train Loss: 0.038219 | Val Loss: 0.047758 | R2: 0.353898\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 9/100 | Train Loss: 0.033510 | Val Loss: 0.046405 | R2: 0.370656\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 10/100 | Train Loss: 0.032010 | Val Loss: 0.041451 | R2: 0.439159\n",
      "Model Checkpoint | epoch: 9 | best_val_loss: 0.04145061528755145\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 11/100 | Train Loss: 0.031832 | Val Loss: 0.036785 | R2: 0.505656\n",
      "Model Checkpoint | epoch: 10 | best_val_loss: 0.036785357812471275\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 12/100 | Train Loss: 0.029380 | Val Loss: 0.040139 | R2: 0.466509\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 13/100 | Train Loss: 0.027267 | Val Loss: 0.035278 | R2: 0.531418\n",
      "Model Checkpoint | epoch: 12 | best_val_loss: 0.035277830918552355\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 14/100 | Train Loss: 0.024888 | Val Loss: 0.034502 | R2: 0.540614\n",
      "Model Checkpoint | epoch: 13 | best_val_loss: 0.03450164695422993\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 15/100 | Train Loss: 0.026130 | Val Loss: 0.037090 | R2: 0.504962\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 16/100 | Train Loss: 0.023239 | Val Loss: 0.031268 | R2: 0.583939\n",
      "Model Checkpoint | epoch: 15 | best_val_loss: 0.031267876144847835\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 17/100 | Train Loss: 0.022690 | Val Loss: 0.029038 | R2: 0.616810\n",
      "Model Checkpoint | epoch: 16 | best_val_loss: 0.029038087610155344\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 18/100 | Train Loss: 0.021111 | Val Loss: 0.033710 | R2: 0.559985\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 19/100 | Train Loss: 0.020387 | Val Loss: 0.027558 | R2: 0.636118\n",
      "Model Checkpoint | epoch: 18 | best_val_loss: 0.027558068490742396\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 20/100 | Train Loss: 0.019382 | Val Loss: 0.029963 | R2: 0.605343\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 21/100 | Train Loss: 0.019267 | Val Loss: 0.027203 | R2: 0.640589\n",
      "Model Checkpoint | epoch: 20 | best_val_loss: 0.027203241495156866\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 22/100 | Train Loss: 0.018348 | Val Loss: 0.024835 | R2: 0.672073\n",
      "Model Checkpoint | epoch: 21 | best_val_loss: 0.024834699214229154\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 23/100 | Train Loss: 0.017141 | Val Loss: 0.025258 | R2: 0.666341\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 24/100 | Train Loss: 0.018669 | Val Loss: 0.026837 | R2: 0.643093\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 25/100 | Train Loss: 0.016661 | Val Loss: 0.023646 | R2: 0.687573\n",
      "Model Checkpoint | epoch: 24 | best_val_loss: 0.02364608336211596\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 26/100 | Train Loss: 0.015836 | Val Loss: 0.022765 | R2: 0.699014\n",
      "Model Checkpoint | epoch: 25 | best_val_loss: 0.022765333412784257\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 27/100 | Train Loss: 0.015720 | Val Loss: 0.024255 | R2: 0.677692\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 28/100 | Train Loss: 0.016333 | Val Loss: 0.026705 | R2: 0.643809\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 29/100 | Train Loss: 0.014841 | Val Loss: 0.022667 | R2: 0.699836\n",
      "Model Checkpoint | epoch: 28 | best_val_loss: 0.02266693982259474\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 30/100 | Train Loss: 0.015074 | Val Loss: 0.024229 | R2: 0.676957\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 31/100 | Train Loss: 0.014108 | Val Loss: 0.025548 | R2: 0.659920\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 32/100 | Train Loss: 0.015352 | Val Loss: 0.026106 | R2: 0.646973\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 33/100 | Train Loss: 0.013177 | Val Loss: 0.023854 | R2: 0.681513\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 34/100 | Train Loss: 0.011992 | Val Loss: 0.021003 | R2: 0.719919\n",
      "Model Checkpoint | epoch: 33 | best_val_loss: 0.021003131423973375\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 35/100 | Train Loss: 0.012078 | Val Loss: 0.021091 | R2: 0.719524\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 36/100 | Train Loss: 0.012391 | Val Loss: 0.019921 | R2: 0.736669\n",
      "Model Checkpoint | epoch: 35 | best_val_loss: 0.019921050750935036\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 37/100 | Train Loss: 0.012871 | Val Loss: 0.023922 | R2: 0.678773\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 38/100 | Train Loss: 0.013531 | Val Loss: 0.022205 | R2: 0.704064\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 39/100 | Train Loss: 0.010719 | Val Loss: 0.021848 | R2: 0.708170\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 40/100 | Train Loss: 0.011407 | Val Loss: 0.020454 | R2: 0.726643\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 41/100 | Train Loss: 0.011198 | Val Loss: 0.019888 | R2: 0.735525\n",
      "Model Checkpoint | epoch: 40 | best_val_loss: 0.01988819506050398\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 42/100 | Train Loss: 0.010167 | Val Loss: 0.020189 | R2: 0.733971\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 43/100 | Train Loss: 0.011082 | Val Loss: 0.024442 | R2: 0.671733\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 44/100 | Train Loss: 0.011586 | Val Loss: 0.020116 | R2: 0.733273\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 45/100 | Train Loss: 0.011142 | Val Loss: 0.023328 | R2: 0.689139\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 46/100 | Train Loss: 0.010755 | Val Loss: 0.019123 | R2: 0.744996\n",
      "Model Checkpoint | epoch: 45 | best_val_loss: 0.01912307140412223\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 47/100 | Train Loss: 0.009206 | Val Loss: 0.016562 | R2: 0.779257\n",
      "Model Checkpoint | epoch: 46 | best_val_loss: 0.01656159052212024\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 48/100 | Train Loss: 0.009931 | Val Loss: 0.018105 | R2: 0.757961\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 49/100 | Train Loss: 0.011246 | Val Loss: 0.018978 | R2: 0.747217\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 50/100 | Train Loss: 0.009361 | Val Loss: 0.018778 | R2: 0.748040\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 51/100 | Train Loss: 0.009142 | Val Loss: 0.023279 | R2: 0.683839\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 52/100 | Train Loss: 0.010601 | Val Loss: 0.019610 | R2: 0.741152\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 53/100 | Train Loss: 0.008849 | Val Loss: 0.020529 | R2: 0.723905\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 54/100 | Train Loss: 0.007792 | Val Loss: 0.018033 | R2: 0.758728\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 55/100 | Train Loss: 0.007248 | Val Loss: 0.016431 | R2: 0.782319\n",
      "Model Checkpoint | epoch: 54 | best_val_loss: 0.01643135684625142\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 56/100 | Train Loss: 0.006926 | Val Loss: 0.016844 | R2: 0.776212\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 57/100 | Train Loss: 0.006696 | Val Loss: 0.017634 | R2: 0.762926\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 58/100 | Train Loss: 0.006546 | Val Loss: 0.017468 | R2: 0.766578\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 59/100 | Train Loss: 0.006386 | Val Loss: 0.018427 | R2: 0.751387\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 60/100 | Train Loss: 0.006153 | Val Loss: 0.016103 | R2: 0.784713\n",
      "Model Checkpoint | epoch: 59 | best_val_loss: 0.016102699261281588\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 61/100 | Train Loss: 0.006294 | Val Loss: 0.017074 | R2: 0.771072\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 62/100 | Train Loss: 0.005860 | Val Loss: 0.016307 | R2: 0.781268\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 63/100 | Train Loss: 0.005774 | Val Loss: 0.016751 | R2: 0.776420\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 64/100 | Train Loss: 0.006135 | Val Loss: 0.016507 | R2: 0.778193\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 65/100 | Train Loss: 0.005899 | Val Loss: 0.015948 | R2: 0.786334\n",
      "Model Checkpoint | epoch: 64 | best_val_loss: 0.015947580102934605\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 66/100 | Train Loss: 0.005449 | Val Loss: 0.016408 | R2: 0.779911\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 67/100 | Train Loss: 0.006160 | Val Loss: 0.015096 | R2: 0.797328\n",
      "Model Checkpoint | epoch: 66 | best_val_loss: 0.015095949368137452\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 68/100 | Train Loss: 0.005372 | Val Loss: 0.014842 | R2: 0.801407\n",
      "Model Checkpoint | epoch: 67 | best_val_loss: 0.014842123573924053\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 69/100 | Train Loss: 0.005514 | Val Loss: 0.014108 | R2: 0.812293\n",
      "Model Checkpoint | epoch: 68 | best_val_loss: 0.014108427950873092\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 70/100 | Train Loss: 0.005468 | Val Loss: 0.014260 | R2: 0.808027\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 71/100 | Train Loss: 0.005452 | Val Loss: 0.014094 | R2: 0.809843\n",
      "Model Checkpoint | epoch: 70 | best_val_loss: 0.014093538824533526\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 72/100 | Train Loss: 0.004773 | Val Loss: 0.014438 | R2: 0.805545\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 73/100 | Train Loss: 0.004917 | Val Loss: 0.014380 | R2: 0.807664\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 74/100 | Train Loss: 0.004679 | Val Loss: 0.013519 | R2: 0.818573\n",
      "Model Checkpoint | epoch: 73 | best_val_loss: 0.013518975791767135\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 75/100 | Train Loss: 0.004660 | Val Loss: 0.014527 | R2: 0.805453\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 76/100 | Train Loss: 0.004805 | Val Loss: 0.014025 | R2: 0.813900\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 77/100 | Train Loss: 0.004377 | Val Loss: 0.013939 | R2: 0.813649\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 78/100 | Train Loss: 0.004335 | Val Loss: 0.014239 | R2: 0.809380\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 79/100 | Train Loss: 0.004496 | Val Loss: 0.015381 | R2: 0.791659\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 80/100 | Train Loss: 0.004474 | Val Loss: 0.014447 | R2: 0.805939\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 81/100 | Train Loss: 0.003967 | Val Loss: 0.013844 | R2: 0.813848\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 82/100 | Train Loss: 0.003836 | Val Loss: 0.013902 | R2: 0.813582\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 83/100 | Train Loss: 0.003900 | Val Loss: 0.014001 | R2: 0.812181\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_8 Epoch 84/100 | Train Loss: 0.003717 | Val Loss: 0.013626 | R2: 0.817632\n",
      "Early stopping at epoch 84\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 1/100 | Train Loss: 0.052560 | Val Loss: 0.054916 | R2: 0.010796\n",
      "Model Checkpoint | epoch: 0 | best_val_loss: 0.05491604898414678\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 2/100 | Train Loss: 0.050737 | Val Loss: 0.054324 | R2: 0.021452\n",
      "Model Checkpoint | epoch: 1 | best_val_loss: 0.05432414013975197\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 3/100 | Train Loss: 0.049323 | Val Loss: 0.051946 | R2: 0.064119\n",
      "Model Checkpoint | epoch: 2 | best_val_loss: 0.05194561412671788\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 4/100 | Train Loss: 0.044794 | Val Loss: 0.048600 | R2: 0.124021\n",
      "Model Checkpoint | epoch: 3 | best_val_loss: 0.048600052577443424\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 5/100 | Train Loss: 0.036733 | Val Loss: 0.042391 | R2: 0.235341\n",
      "Model Checkpoint | epoch: 4 | best_val_loss: 0.042390948433144435\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 6/100 | Train Loss: 0.031379 | Val Loss: 0.040409 | R2: 0.271668\n",
      "Model Checkpoint | epoch: 5 | best_val_loss: 0.040409347405346734\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 7/100 | Train Loss: 0.027437 | Val Loss: 0.032116 | R2: 0.422064\n",
      "Model Checkpoint | epoch: 6 | best_val_loss: 0.032115566309179285\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 8/100 | Train Loss: 0.025887 | Val Loss: 0.032411 | R2: 0.416515\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 9/100 | Train Loss: 0.023490 | Val Loss: 0.032043 | R2: 0.423464\n",
      "Model Checkpoint | epoch: 8 | best_val_loss: 0.03204349646476718\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 10/100 | Train Loss: 0.022543 | Val Loss: 0.032350 | R2: 0.418320\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 11/100 | Train Loss: 0.021315 | Val Loss: 0.027358 | R2: 0.507782\n",
      "Model Checkpoint | epoch: 10 | best_val_loss: 0.027358406965879517\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 12/100 | Train Loss: 0.019562 | Val Loss: 0.028763 | R2: 0.482902\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 13/100 | Train Loss: 0.019428 | Val Loss: 0.024760 | R2: 0.553603\n",
      "Model Checkpoint | epoch: 12 | best_val_loss: 0.024759548268662508\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 14/100 | Train Loss: 0.018490 | Val Loss: 0.024164 | R2: 0.566361\n",
      "Model Checkpoint | epoch: 13 | best_val_loss: 0.024163675012843062\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 15/100 | Train Loss: 0.017888 | Val Loss: 0.022581 | R2: 0.594263\n",
      "Model Checkpoint | epoch: 14 | best_val_loss: 0.02258115579915789\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 16/100 | Train Loss: 0.016873 | Val Loss: 0.024167 | R2: 0.563847\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 17/100 | Train Loss: 0.016791 | Val Loss: 0.022360 | R2: 0.597912\n",
      "Model Checkpoint | epoch: 16 | best_val_loss: 0.022360096407378377\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 18/100 | Train Loss: 0.015032 | Val Loss: 0.021381 | R2: 0.616046\n",
      "Model Checkpoint | epoch: 17 | best_val_loss: 0.021380593031953644\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 19/100 | Train Loss: 0.014489 | Val Loss: 0.021044 | R2: 0.622052\n",
      "Model Checkpoint | epoch: 18 | best_val_loss: 0.021044110361881193\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 20/100 | Train Loss: 0.014181 | Val Loss: 0.020798 | R2: 0.627386\n",
      "Model Checkpoint | epoch: 19 | best_val_loss: 0.020797921373288977\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 21/100 | Train Loss: 0.013686 | Val Loss: 0.018435 | R2: 0.670501\n",
      "Model Checkpoint | epoch: 20 | best_val_loss: 0.018434640468437766\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 22/100 | Train Loss: 0.013750 | Val Loss: 0.022837 | R2: 0.590000\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 23/100 | Train Loss: 0.013167 | Val Loss: 0.017080 | R2: 0.692975\n",
      "Model Checkpoint | epoch: 22 | best_val_loss: 0.017080008636183468\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 24/100 | Train Loss: 0.012147 | Val Loss: 0.019620 | R2: 0.648026\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 25/100 | Train Loss: 0.012480 | Val Loss: 0.018933 | R2: 0.659132\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 26/100 | Train Loss: 0.011320 | Val Loss: 0.018164 | R2: 0.672910\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 27/100 | Train Loss: 0.011797 | Val Loss: 0.017600 | R2: 0.683344\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 28/100 | Train Loss: 0.011330 | Val Loss: 0.017099 | R2: 0.693632\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 29/100 | Train Loss: 0.010710 | Val Loss: 0.016986 | R2: 0.694779\n",
      "Model Checkpoint | epoch: 28 | best_val_loss: 0.016986093033160107\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 30/100 | Train Loss: 0.010579 | Val Loss: 0.018682 | R2: 0.664098\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 31/100 | Train Loss: 0.010124 | Val Loss: 0.019046 | R2: 0.658093\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 32/100 | Train Loss: 0.009228 | Val Loss: 0.016964 | R2: 0.694787\n",
      "Model Checkpoint | epoch: 31 | best_val_loss: 0.01696404493493416\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 33/100 | Train Loss: 0.009687 | Val Loss: 0.015810 | R2: 0.716043\n",
      "Model Checkpoint | epoch: 32 | best_val_loss: 0.015810003351006244\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 34/100 | Train Loss: 0.009882 | Val Loss: 0.015808 | R2: 0.714705\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 35/100 | Train Loss: 0.009488 | Val Loss: 0.015837 | R2: 0.715185\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 36/100 | Train Loss: 0.008493 | Val Loss: 0.015275 | R2: 0.723681\n",
      "Model Checkpoint | epoch: 35 | best_val_loss: 0.015274606764659337\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 37/100 | Train Loss: 0.008446 | Val Loss: 0.015836 | R2: 0.713951\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 38/100 | Train Loss: 0.008542 | Val Loss: 0.015761 | R2: 0.716621\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 39/100 | Train Loss: 0.007469 | Val Loss: 0.014970 | R2: 0.730340\n",
      "Model Checkpoint | epoch: 38 | best_val_loss: 0.014970203239406045\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 40/100 | Train Loss: 0.008809 | Val Loss: 0.015812 | R2: 0.716561\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 41/100 | Train Loss: 0.008274 | Val Loss: 0.014631 | R2: 0.736500\n",
      "Model Checkpoint | epoch: 40 | best_val_loss: 0.01463128277975031\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 42/100 | Train Loss: 0.006845 | Val Loss: 0.015810 | R2: 0.714574\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 43/100 | Train Loss: 0.008303 | Val Loss: 0.016032 | R2: 0.710036\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 44/100 | Train Loss: 0.008428 | Val Loss: 0.014887 | R2: 0.730846\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 45/100 | Train Loss: 0.007009 | Val Loss: 0.015566 | R2: 0.720308\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 46/100 | Train Loss: 0.008381 | Val Loss: 0.014778 | R2: 0.733631\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 47/100 | Train Loss: 0.007540 | Val Loss: 0.013923 | R2: 0.749178\n",
      "Model Checkpoint | epoch: 46 | best_val_loss: 0.013923060982080642\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 48/100 | Train Loss: 0.006689 | Val Loss: 0.014229 | R2: 0.742909\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 49/100 | Train Loss: 0.006286 | Val Loss: 0.013812 | R2: 0.750218\n",
      "Model Checkpoint | epoch: 48 | best_val_loss: 0.013811905751431671\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 50/100 | Train Loss: 0.007475 | Val Loss: 0.015416 | R2: 0.721652\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 51/100 | Train Loss: 0.007696 | Val Loss: 0.015306 | R2: 0.724319\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 52/100 | Train Loss: 0.006702 | Val Loss: 0.013539 | R2: 0.755653\n",
      "Model Checkpoint | epoch: 51 | best_val_loss: 0.013538835492496017\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 53/100 | Train Loss: 0.006473 | Val Loss: 0.014537 | R2: 0.737342\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 54/100 | Train Loss: 0.006628 | Val Loss: 0.014243 | R2: 0.742759\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 55/100 | Train Loss: 0.007168 | Val Loss: 0.015420 | R2: 0.721493\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 56/100 | Train Loss: 0.007055 | Val Loss: 0.014532 | R2: 0.738279\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 57/100 | Train Loss: 0.006826 | Val Loss: 0.014193 | R2: 0.744730\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 58/100 | Train Loss: 0.006167 | Val Loss: 0.014563 | R2: 0.737009\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 59/100 | Train Loss: 0.005548 | Val Loss: 0.013653 | R2: 0.753414\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 60/100 | Train Loss: 0.005161 | Val Loss: 0.013065 | R2: 0.764779\n",
      "Model Checkpoint | epoch: 59 | best_val_loss: 0.013065052032548314\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 61/100 | Train Loss: 0.005120 | Val Loss: 0.012784 | R2: 0.769714\n",
      "Model Checkpoint | epoch: 60 | best_val_loss: 0.012783618025999102\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 62/100 | Train Loss: 0.005168 | Val Loss: 0.012806 | R2: 0.768844\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 63/100 | Train Loss: 0.004853 | Val Loss: 0.010961 | R2: 0.801850\n",
      "Model Checkpoint | epoch: 62 | best_val_loss: 0.010961246757819836\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 64/100 | Train Loss: 0.004778 | Val Loss: 0.011720 | R2: 0.788809\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 65/100 | Train Loss: 0.005124 | Val Loss: 0.012090 | R2: 0.781970\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 66/100 | Train Loss: 0.004681 | Val Loss: 0.011148 | R2: 0.798933\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 67/100 | Train Loss: 0.004546 | Val Loss: 0.010890 | R2: 0.803351\n",
      "Model Checkpoint | epoch: 66 | best_val_loss: 0.010889708975779488\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 68/100 | Train Loss: 0.004604 | Val Loss: 0.010440 | R2: 0.812205\n",
      "Model Checkpoint | epoch: 67 | best_val_loss: 0.010440008754948698\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 69/100 | Train Loss: 0.004441 | Val Loss: 0.010091 | R2: 0.818373\n",
      "Model Checkpoint | epoch: 68 | best_val_loss: 0.0100909799445621\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 70/100 | Train Loss: 0.004325 | Val Loss: 0.010614 | R2: 0.809203\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 71/100 | Train Loss: 0.004491 | Val Loss: 0.010851 | R2: 0.804616\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 72/100 | Train Loss: 0.004521 | Val Loss: 0.010064 | R2: 0.819095\n",
      "Model Checkpoint | epoch: 71 | best_val_loss: 0.010064226518228275\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 73/100 | Train Loss: 0.004341 | Val Loss: 0.011596 | R2: 0.791641\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 74/100 | Train Loss: 0.004587 | Val Loss: 0.010018 | R2: 0.820273\n",
      "Model Checkpoint | epoch: 73 | best_val_loss: 0.01001761941582663\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 75/100 | Train Loss: 0.004128 | Val Loss: 0.010131 | R2: 0.817219\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 76/100 | Train Loss: 0.004046 | Val Loss: 0.010207 | R2: 0.816030\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 77/100 | Train Loss: 0.004168 | Val Loss: 0.010312 | R2: 0.814181\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 78/100 | Train Loss: 0.004297 | Val Loss: 0.010378 | R2: 0.812716\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 79/100 | Train Loss: 0.004259 | Val Loss: 0.010333 | R2: 0.813819\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 80/100 | Train Loss: 0.004126 | Val Loss: 0.010400 | R2: 0.812899\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 81/100 | Train Loss: 0.003866 | Val Loss: 0.010265 | R2: 0.815208\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 82/100 | Train Loss: 0.003719 | Val Loss: 0.010248 | R2: 0.815480\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 83/100 | Train Loss: 0.003734 | Val Loss: 0.010109 | R2: 0.818083\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 84/100 | Train Loss: 0.003667 | Val Loss: 0.009964 | R2: 0.820373\n",
      "Model Checkpoint | epoch: 83 | best_val_loss: 0.00996376366436986\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 85/100 | Train Loss: 0.003512 | Val Loss: 0.010099 | R2: 0.818091\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 86/100 | Train Loss: 0.003547 | Val Loss: 0.009947 | R2: 0.820847\n",
      "Model Checkpoint | epoch: 85 | best_val_loss: 0.009946965507836365\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 87/100 | Train Loss: 0.003444 | Val Loss: 0.010325 | R2: 0.813955\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 88/100 | Train Loss: 0.003590 | Val Loss: 0.010327 | R2: 0.814302\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 89/100 | Train Loss: 0.003427 | Val Loss: 0.009950 | R2: 0.820925\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 90/100 | Train Loss: 0.003471 | Val Loss: 0.009910 | R2: 0.821763\n",
      "Model Checkpoint | epoch: 89 | best_val_loss: 0.009909935843290037\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 91/100 | Train Loss: 0.003422 | Val Loss: 0.010211 | R2: 0.816410\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 92/100 | Train Loss: 0.003438 | Val Loss: 0.010098 | R2: 0.818178\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 93/100 | Train Loss: 0.003375 | Val Loss: 0.009927 | R2: 0.821335\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 94/100 | Train Loss: 0.003408 | Val Loss: 0.010455 | R2: 0.811350\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 95/100 | Train Loss: 0.003365 | Val Loss: 0.009592 | R2: 0.826889\n",
      "Model Checkpoint | epoch: 94 | best_val_loss: 0.009592064929890006\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 96/100 | Train Loss: 0.003273 | Val Loss: 0.009827 | R2: 0.822597\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 97/100 | Train Loss: 0.003311 | Val Loss: 0.010991 | R2: 0.801593\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 98/100 | Train Loss: 0.003253 | Val Loss: 0.010110 | R2: 0.817374\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 99/100 | Train Loss: 0.003230 | Val Loss: 0.009902 | R2: 0.821118\n",
      "Hybrid_CNN_LSTM_contralateral_3_output_session_9 Epoch 100/100 | Train Loss: 0.003283 | Val Loss: 0.010190 | R2: 0.816074\n"
     ]
    }
   ],
   "source": [
    "for index in range(len(processed_data_l_X)):\n",
    "    X = np.load(processed_data_l_X[index])\n",
    "    y = np.load(processed_data_l_y[index])\n",
    "\n",
    "    # Creating Train and Validation Sets\n",
    "    dataset = EcogMotionDataset(X, y)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size \n",
    "    train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=64)\n",
    "    \n",
    "    # 2. CNN_LSTM Hybrid Model\n",
    "    hybrid_model = EcogToMotionNet()\n",
    "    Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 79/100 | Train Loss: 0.007047 | Val Loss: 0.011237 | R2: 0.813044\n",
    "    Model Checkpoint | epoch: 78 | best_val_loss: 0.011236593591857753\n",
    "    Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 80/100 | Train Loss: 0.006915 | Val Loss: 0.011991 | R2: 0.801362\n",
    "    Hybrid_CNN_LSTM_contralateral_3_output_session_2 Epoch 81/100 | Train Loss: 0.007074 | Val Loss: 0.012819 | R2: 0.788208\n",
    "    criterion = nn.MSELoss()\n",
    "    hybrid_train_losses, hybrid_val_losses, hybrid_r2 = train_model(hybrid_model, device, train_loader, val_loader, epochs=100, model_name=f\"Hybrid_CNN_LSTM_contralateral_3_output_session_{index}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb78e68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define K-fold sets\n",
    "test_list_X = []\n",
    "train_list_X = []\n",
    "test_list_y = []\n",
    "train_list_y = []\n",
    "\n",
    "for i in range(len(processed_data_l_X)):\n",
    "    test_list_X.append(processed_data_l_X[i])\n",
    "    test_list_y.append(processed_data_l_y[i])\n",
    "    train_X = [x for idx, x in enumerate(processed_data_l_X) if idx != i]\n",
    "    train_y = [y for idx, y in enumerate(processed_data_l_y) if idx != i]\n",
    "    train_list_X.append(train_X)\n",
    "    train_list_y.append(train_y)\n",
    "\n",
    "# Load a single specific dataset\n",
    "# K-Fold 0 uses session 7 as the test set:\n",
    "KFOLD = 0\n",
    "SESSION_SET = 6\n",
    "\n",
    "\"\"\"\n",
    "['/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-24_(S10)/X.npy',0\n",
    " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S1)/X.npy', 1\n",
    " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-06_(S6)/X.npy', 2\n",
    " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S5)/X.npy', 3\n",
    " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S3)/X.npy', 4\n",
    " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S4)/X.npy', 5\n",
    " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-10_(S8)/X.npy', 6 \n",
    " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-17_(S9)/X.npy', 7\n",
    " '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S2)/X.npy'] 8\n",
    "\"\"\"\n",
    "\n",
    "X = np.load(train_list_X[KFOLD][SESSION_SET])\n",
    "y = np.load(train_list_y[KFOLD][SESSION_SET])\n",
    "\n",
    "# X = np.load(test_list_X[0]) # Identify the test set\n",
    "# y = np.load(test_list_y[0]) # Identify the test set\n",
    "\n",
    "# Creating Train and Validation Sets\n",
    "dataset = EcogMotionDataset(X, y)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=64)\n",
    "\n",
    "# Assuming train_loader, val_loader, criterion are defined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb524472",
   "metadata": {},
   "source": [
    "### K-Fold Cross Validation Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7dae6a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 0 ===\n",
      "Training on Session 1/9\n",
      "Session 0 Epoch 1 - Train Loss: 0.051085\n",
      "Checkpoint saved for fold 0, session 0, epoch 1\n",
      "Session 0 Epoch 2 - Train Loss: 0.050288\n",
      "Checkpoint saved for fold 0, session 0, epoch 2\n",
      "Session 0 Epoch 3 - Train Loss: 0.048895\n",
      "Checkpoint saved for fold 0, session 0, epoch 3\n",
      "Session 0 Epoch 4 - Train Loss: 0.045292\n",
      "Checkpoint saved for fold 0, session 0, epoch 4\n",
      "Session 0 Epoch 5 - Train Loss: 0.035813\n",
      "Checkpoint saved for fold 0, session 0, epoch 5\n",
      "Session 0 Epoch 6 - Train Loss: 0.029496\n",
      "Checkpoint saved for fold 0, session 0, epoch 6\n",
      "Session 0 Epoch 7 - Train Loss: 0.026231\n",
      "Checkpoint saved for fold 0, session 0, epoch 7\n",
      "Session 0 Epoch 8 - Train Loss: 0.024574\n",
      "Checkpoint saved for fold 0, session 0, epoch 8\n",
      "Session 0 Epoch 9 - Train Loss: 0.021405\n",
      "Checkpoint saved for fold 0, session 0, epoch 9\n",
      "Session 0 Epoch 10 - Train Loss: 0.021470\n",
      "Session 0 Epoch 11 - Train Loss: 0.019468\n",
      "Checkpoint saved for fold 0, session 0, epoch 11\n",
      "Session 0 Epoch 12 - Train Loss: 0.018339\n",
      "Checkpoint saved for fold 0, session 0, epoch 12\n",
      "Session 0 Epoch 13 - Train Loss: 0.018660\n",
      "Session 0 Epoch 14 - Train Loss: 0.018522\n",
      "Session 0 Epoch 15 - Train Loss: 0.018618\n",
      "Session 0 Epoch 16 - Train Loss: 0.017098\n",
      "Checkpoint saved for fold 0, session 0, epoch 16\n",
      "Session 0 Epoch 17 - Train Loss: 0.018791\n",
      "Session 0 Epoch 18 - Train Loss: 0.017463\n",
      "Session 0 Epoch 19 - Train Loss: 0.016912\n",
      "Checkpoint saved for fold 0, session 0, epoch 19\n",
      "Session 0 Epoch 20 - Train Loss: 0.017233\n",
      "Session 0 Epoch 21 - Train Loss: 0.016274\n",
      "Checkpoint saved for fold 0, session 0, epoch 21\n",
      "Session 0 Epoch 22 - Train Loss: 0.016271\n",
      "Session 0 Epoch 23 - Train Loss: 0.015846\n",
      "Checkpoint saved for fold 0, session 0, epoch 23\n",
      "Session 0 Epoch 24 - Train Loss: 0.016039\n",
      "Session 0 Epoch 25 - Train Loss: 0.015255\n",
      "Checkpoint saved for fold 0, session 0, epoch 25\n",
      "Session 0 Epoch 26 - Train Loss: 0.015343\n",
      "Session 0 Epoch 27 - Train Loss: 0.014881\n",
      "Checkpoint saved for fold 0, session 0, epoch 27\n",
      "Session 0 Epoch 28 - Train Loss: 0.015078\n",
      "Session 0 Epoch 29 - Train Loss: 0.014845\n",
      "Session 0 Epoch 30 - Train Loss: 0.014999\n",
      "Session 0 Epoch 31 - Train Loss: 0.013478\n",
      "Checkpoint saved for fold 0, session 0, epoch 31\n",
      "Session 0 Epoch 32 - Train Loss: 0.014068\n",
      "Session 0 Epoch 33 - Train Loss: 0.013514\n",
      "Session 0 Epoch 34 - Train Loss: 0.013046\n",
      "Checkpoint saved for fold 0, session 0, epoch 34\n",
      "Session 0 Epoch 35 - Train Loss: 0.012155\n",
      "Checkpoint saved for fold 0, session 0, epoch 35\n",
      "Session 0 Epoch 36 - Train Loss: 0.011673\n",
      "Checkpoint saved for fold 0, session 0, epoch 36\n",
      "Session 0 Epoch 37 - Train Loss: 0.013161\n",
      "Session 0 Epoch 38 - Train Loss: 0.011901\n",
      "Session 0 Epoch 39 - Train Loss: 0.011465\n",
      "Checkpoint saved for fold 0, session 0, epoch 39\n",
      "Session 0 Epoch 40 - Train Loss: 0.010798\n",
      "Checkpoint saved for fold 0, session 0, epoch 40\n",
      "Session 0 Epoch 41 - Train Loss: 0.011726\n",
      "Session 0 Epoch 42 - Train Loss: 0.011152\n",
      "Session 0 Epoch 43 - Train Loss: 0.010396\n",
      "Checkpoint saved for fold 0, session 0, epoch 43\n",
      "Session 0 Epoch 44 - Train Loss: 0.011075\n",
      "Session 0 Epoch 45 - Train Loss: 0.010406\n",
      "Session 0 Epoch 46 - Train Loss: 0.010838\n",
      "Session 0 Epoch 47 - Train Loss: 0.009687\n",
      "Checkpoint saved for fold 0, session 0, epoch 47\n",
      "Session 0 Epoch 48 - Train Loss: 0.010780\n",
      "Session 0 Epoch 49 - Train Loss: 0.010163\n",
      "Session 0 Epoch 50 - Train Loss: 0.010102\n",
      "Session 0 Epoch 51 - Train Loss: 0.009844\n",
      "Session 0 Epoch 52 - Train Loss: 0.010862\n",
      "Session 0 Epoch 53 - Train Loss: 0.009603\n",
      "Session 0 Epoch 54 - Train Loss: 0.009588\n",
      "Session 0 Epoch 55 - Train Loss: 0.009636\n",
      "Session 0 Epoch 56 - Train Loss: 0.009952\n",
      "Session 0 Epoch 57 - Train Loss: 0.009346\n",
      "Checkpoint saved for fold 0, session 0, epoch 57\n",
      "Session 0 Epoch 58 - Train Loss: 0.009971\n",
      "Session 0 Epoch 59 - Train Loss: 0.009399\n",
      "Session 0 Epoch 60 - Train Loss: 0.008831\n",
      "Checkpoint saved for fold 0, session 0, epoch 60\n",
      "Session 0 Epoch 61 - Train Loss: 0.008940\n",
      "Session 0 Epoch 62 - Train Loss: 0.008461\n",
      "Checkpoint saved for fold 0, session 0, epoch 62\n",
      "Session 0 Epoch 63 - Train Loss: 0.007998\n",
      "Checkpoint saved for fold 0, session 0, epoch 63\n",
      "Session 0 Epoch 64 - Train Loss: 0.007601\n",
      "Checkpoint saved for fold 0, session 0, epoch 64\n",
      "Session 0 Epoch 65 - Train Loss: 0.009482\n",
      "Session 0 Epoch 66 - Train Loss: 0.007745\n",
      "Session 0 Epoch 67 - Train Loss: 0.007926\n",
      "Session 0 Epoch 68 - Train Loss: 0.008242\n",
      "Session 0 Epoch 69 - Train Loss: 0.007765\n",
      "Session 0 Epoch 70 - Train Loss: 0.008422\n",
      "Session 0 Epoch 71 - Train Loss: 0.007159\n",
      "Checkpoint saved for fold 0, session 0, epoch 71\n",
      "Session 0 Epoch 72 - Train Loss: 0.006881\n",
      "Checkpoint saved for fold 0, session 0, epoch 72\n",
      "Session 0 Epoch 73 - Train Loss: 0.006786\n",
      "Session 0 Epoch 74 - Train Loss: 0.006854\n",
      "Session 0 Epoch 75 - Train Loss: 0.006829\n",
      "Session 0 Epoch 76 - Train Loss: 0.006647\n",
      "Checkpoint saved for fold 0, session 0, epoch 76\n",
      "Session 0 Epoch 77 - Train Loss: 0.006737\n",
      "Session 0 Epoch 78 - Train Loss: 0.006410\n",
      "Checkpoint saved for fold 0, session 0, epoch 78\n",
      "Session 0 Epoch 79 - Train Loss: 0.006632\n",
      "Session 0 Epoch 80 - Train Loss: 0.006662\n",
      "Training on Session 2/9\n",
      "Session 1 Epoch 1 - Train Loss: 0.074380\n",
      "Checkpoint saved for fold 0, session 1, epoch 1\n",
      "Session 1 Epoch 2 - Train Loss: 0.064895\n",
      "Checkpoint saved for fold 0, session 1, epoch 2\n",
      "Session 1 Epoch 3 - Train Loss: 0.063376\n",
      "Checkpoint saved for fold 0, session 1, epoch 3\n",
      "Session 1 Epoch 4 - Train Loss: 0.056860\n",
      "Checkpoint saved for fold 0, session 1, epoch 4\n",
      "Session 1 Epoch 5 - Train Loss: 0.049620\n",
      "Checkpoint saved for fold 0, session 1, epoch 5\n",
      "Session 1 Epoch 6 - Train Loss: 0.045912\n",
      "Checkpoint saved for fold 0, session 1, epoch 6\n",
      "Session 1 Epoch 7 - Train Loss: 0.042962\n",
      "Checkpoint saved for fold 0, session 1, epoch 7\n",
      "Session 1 Epoch 8 - Train Loss: 0.041040\n",
      "Checkpoint saved for fold 0, session 1, epoch 8\n",
      "Session 1 Epoch 9 - Train Loss: 0.039078\n",
      "Checkpoint saved for fold 0, session 1, epoch 9\n",
      "Session 1 Epoch 10 - Train Loss: 0.037256\n",
      "Checkpoint saved for fold 0, session 1, epoch 10\n",
      "Session 1 Epoch 11 - Train Loss: 0.035427\n",
      "Checkpoint saved for fold 0, session 1, epoch 11\n",
      "Session 1 Epoch 12 - Train Loss: 0.034705\n",
      "Checkpoint saved for fold 0, session 1, epoch 12\n",
      "Session 1 Epoch 13 - Train Loss: 0.033625\n",
      "Checkpoint saved for fold 0, session 1, epoch 13\n",
      "Session 1 Epoch 14 - Train Loss: 0.032693\n",
      "Checkpoint saved for fold 0, session 1, epoch 14\n",
      "Session 1 Epoch 15 - Train Loss: 0.031760\n",
      "Checkpoint saved for fold 0, session 1, epoch 15\n",
      "Session 1 Epoch 16 - Train Loss: 0.031157\n",
      "Checkpoint saved for fold 0, session 1, epoch 16\n",
      "Session 1 Epoch 17 - Train Loss: 0.030728\n",
      "Checkpoint saved for fold 0, session 1, epoch 17\n",
      "Session 1 Epoch 18 - Train Loss: 0.030156\n",
      "Checkpoint saved for fold 0, session 1, epoch 18\n",
      "Session 1 Epoch 19 - Train Loss: 0.029782\n",
      "Checkpoint saved for fold 0, session 1, epoch 19\n",
      "Session 1 Epoch 20 - Train Loss: 0.029277\n",
      "Checkpoint saved for fold 0, session 1, epoch 20\n",
      "Session 1 Epoch 21 - Train Loss: 0.028999\n",
      "Checkpoint saved for fold 0, session 1, epoch 21\n",
      "Session 1 Epoch 22 - Train Loss: 0.029011\n",
      "Session 1 Epoch 23 - Train Loss: 0.028241\n",
      "Checkpoint saved for fold 0, session 1, epoch 23\n",
      "Session 1 Epoch 24 - Train Loss: 0.028356\n",
      "Session 1 Epoch 25 - Train Loss: 0.028034\n",
      "Checkpoint saved for fold 0, session 1, epoch 25\n",
      "Session 1 Epoch 26 - Train Loss: 0.027948\n",
      "Session 1 Epoch 27 - Train Loss: 0.027835\n",
      "Checkpoint saved for fold 0, session 1, epoch 27\n",
      "Session 1 Epoch 28 - Train Loss: 0.028137\n",
      "Session 1 Epoch 29 - Train Loss: 0.027788\n",
      "Session 1 Epoch 30 - Train Loss: 0.027633\n",
      "Checkpoint saved for fold 0, session 1, epoch 30\n",
      "Session 1 Epoch 31 - Train Loss: 0.027441\n",
      "Checkpoint saved for fold 0, session 1, epoch 31\n",
      "Session 1 Epoch 32 - Train Loss: 0.027413\n",
      "Session 1 Epoch 33 - Train Loss: 0.027040\n",
      "Checkpoint saved for fold 0, session 1, epoch 33\n",
      "Session 1 Epoch 34 - Train Loss: 0.027223\n",
      "Session 1 Epoch 35 - Train Loss: 0.027173\n",
      "Session 1 Epoch 36 - Train Loss: 0.027124\n",
      "Session 1 Epoch 37 - Train Loss: 0.027075\n",
      "Session 1 Epoch 38 - Train Loss: 0.027093\n",
      "Session 1 Epoch 39 - Train Loss: 0.027323\n",
      "Session 1 Epoch 40 - Train Loss: 0.027052\n",
      "Session 1 Epoch 41 - Train Loss: 0.027085\n",
      "Session 1 Epoch 42 - Train Loss: 0.026940\n",
      "Session 1 Epoch 43 - Train Loss: 0.027026\n",
      "Early stopping at epoch 43 for session 1\n",
      "Training on Session 3/9\n",
      "Session 2 Epoch 1 - Train Loss: 0.524119\n",
      "Checkpoint saved for fold 0, session 2, epoch 1\n",
      "Session 2 Epoch 2 - Train Loss: 0.374437\n",
      "Checkpoint saved for fold 0, session 2, epoch 2\n",
      "Session 2 Epoch 3 - Train Loss: 0.222258\n",
      "Checkpoint saved for fold 0, session 2, epoch 3\n",
      "Session 2 Epoch 4 - Train Loss: 0.169177\n",
      "Checkpoint saved for fold 0, session 2, epoch 4\n",
      "Session 2 Epoch 5 - Train Loss: 0.144383\n",
      "Checkpoint saved for fold 0, session 2, epoch 5\n",
      "Session 2 Epoch 6 - Train Loss: 0.126864\n",
      "Checkpoint saved for fold 0, session 2, epoch 6\n",
      "Session 2 Epoch 7 - Train Loss: 0.115601\n",
      "Checkpoint saved for fold 0, session 2, epoch 7\n",
      "Session 2 Epoch 8 - Train Loss: 0.109556\n",
      "Checkpoint saved for fold 0, session 2, epoch 8\n",
      "Session 2 Epoch 9 - Train Loss: 0.105671\n",
      "Checkpoint saved for fold 0, session 2, epoch 9\n",
      "Session 2 Epoch 10 - Train Loss: 0.103307\n",
      "Checkpoint saved for fold 0, session 2, epoch 10\n",
      "Session 2 Epoch 11 - Train Loss: 0.102003\n",
      "Checkpoint saved for fold 0, session 2, epoch 11\n",
      "Session 2 Epoch 12 - Train Loss: 0.100944\n",
      "Checkpoint saved for fold 0, session 2, epoch 12\n",
      "Session 2 Epoch 13 - Train Loss: 0.099090\n",
      "Checkpoint saved for fold 0, session 2, epoch 13\n",
      "Session 2 Epoch 14 - Train Loss: 0.097964\n",
      "Checkpoint saved for fold 0, session 2, epoch 14\n",
      "Session 2 Epoch 15 - Train Loss: 0.097608\n",
      "Checkpoint saved for fold 0, session 2, epoch 15\n",
      "Session 2 Epoch 16 - Train Loss: 0.096018\n",
      "Checkpoint saved for fold 0, session 2, epoch 16\n",
      "Session 2 Epoch 17 - Train Loss: 0.096243\n",
      "Session 2 Epoch 18 - Train Loss: 0.095667\n",
      "Checkpoint saved for fold 0, session 2, epoch 18\n",
      "Session 2 Epoch 19 - Train Loss: 0.095521\n",
      "Checkpoint saved for fold 0, session 2, epoch 19\n",
      "Session 2 Epoch 20 - Train Loss: 0.095004\n",
      "Checkpoint saved for fold 0, session 2, epoch 20\n",
      "Session 2 Epoch 21 - Train Loss: 0.094392\n",
      "Checkpoint saved for fold 0, session 2, epoch 21\n",
      "Session 2 Epoch 22 - Train Loss: 0.093939\n",
      "Checkpoint saved for fold 0, session 2, epoch 22\n",
      "Session 2 Epoch 23 - Train Loss: 0.093913\n",
      "Session 2 Epoch 24 - Train Loss: 0.093931\n",
      "Session 2 Epoch 25 - Train Loss: 0.093593\n",
      "Checkpoint saved for fold 0, session 2, epoch 25\n",
      "Session 2 Epoch 26 - Train Loss: 0.093461\n",
      "Checkpoint saved for fold 0, session 2, epoch 26\n",
      "Session 2 Epoch 27 - Train Loss: 0.093221\n",
      "Checkpoint saved for fold 0, session 2, epoch 27\n",
      "Session 2 Epoch 28 - Train Loss: 0.092876\n",
      "Checkpoint saved for fold 0, session 2, epoch 28\n",
      "Session 2 Epoch 29 - Train Loss: 0.093017\n",
      "Session 2 Epoch 30 - Train Loss: 0.092581\n",
      "Checkpoint saved for fold 0, session 2, epoch 30\n",
      "Session 2 Epoch 31 - Train Loss: 0.092750\n",
      "Session 2 Epoch 32 - Train Loss: 0.092443\n",
      "Checkpoint saved for fold 0, session 2, epoch 32\n",
      "Session 2 Epoch 33 - Train Loss: 0.092649\n",
      "Session 2 Epoch 34 - Train Loss: 0.092474\n",
      "Session 2 Epoch 35 - Train Loss: 0.092403\n",
      "Session 2 Epoch 36 - Train Loss: 0.092594\n",
      "Session 2 Epoch 37 - Train Loss: 0.092366\n",
      "Session 2 Epoch 38 - Train Loss: 0.092438\n",
      "Session 2 Epoch 39 - Train Loss: 0.092334\n",
      "Checkpoint saved for fold 0, session 2, epoch 39\n",
      "Session 2 Epoch 40 - Train Loss: 0.092363\n",
      "Session 2 Epoch 41 - Train Loss: 0.092243\n",
      "Session 2 Epoch 42 - Train Loss: 0.092113\n",
      "Checkpoint saved for fold 0, session 2, epoch 42\n",
      "Session 2 Epoch 43 - Train Loss: 0.092154\n",
      "Session 2 Epoch 44 - Train Loss: 0.091894\n",
      "Checkpoint saved for fold 0, session 2, epoch 44\n",
      "Session 2 Epoch 45 - Train Loss: 0.092384\n",
      "Session 2 Epoch 46 - Train Loss: 0.091893\n",
      "Session 2 Epoch 47 - Train Loss: 0.092412\n",
      "Session 2 Epoch 48 - Train Loss: 0.091839\n",
      "Session 2 Epoch 49 - Train Loss: 0.092166\n",
      "Session 2 Epoch 50 - Train Loss: 0.092376\n",
      "Session 2 Epoch 51 - Train Loss: 0.092265\n",
      "Session 2 Epoch 52 - Train Loss: 0.091914\n",
      "Session 2 Epoch 53 - Train Loss: 0.092145\n",
      "Session 2 Epoch 54 - Train Loss: 0.091936\n",
      "Early stopping at epoch 54 for session 2\n",
      "Training on Session 4/9\n",
      "Session 3 Epoch 1 - Train Loss: 0.144050\n",
      "Checkpoint saved for fold 0, session 3, epoch 1\n",
      "Session 3 Epoch 2 - Train Loss: 0.143423\n",
      "Checkpoint saved for fold 0, session 3, epoch 2\n",
      "Session 3 Epoch 3 - Train Loss: 0.141925\n",
      "Checkpoint saved for fold 0, session 3, epoch 3\n",
      "Session 3 Epoch 4 - Train Loss: 0.141819\n",
      "Checkpoint saved for fold 0, session 3, epoch 4\n",
      "Session 3 Epoch 5 - Train Loss: 0.141491\n",
      "Checkpoint saved for fold 0, session 3, epoch 5\n",
      "Session 3 Epoch 6 - Train Loss: 0.140632\n",
      "Checkpoint saved for fold 0, session 3, epoch 6\n",
      "Session 3 Epoch 7 - Train Loss: 0.140060\n",
      "Checkpoint saved for fold 0, session 3, epoch 7\n",
      "Session 3 Epoch 8 - Train Loss: 0.138488\n",
      "Checkpoint saved for fold 0, session 3, epoch 8\n",
      "Session 3 Epoch 9 - Train Loss: 0.138654\n",
      "Session 3 Epoch 10 - Train Loss: 0.137616\n",
      "Checkpoint saved for fold 0, session 3, epoch 10\n",
      "Session 3 Epoch 11 - Train Loss: 0.136722\n",
      "Checkpoint saved for fold 0, session 3, epoch 11\n",
      "Session 3 Epoch 12 - Train Loss: 0.136085\n",
      "Checkpoint saved for fold 0, session 3, epoch 12\n",
      "Session 3 Epoch 13 - Train Loss: 0.135353\n",
      "Checkpoint saved for fold 0, session 3, epoch 13\n",
      "Session 3 Epoch 14 - Train Loss: 0.134888\n",
      "Checkpoint saved for fold 0, session 3, epoch 14\n",
      "Session 3 Epoch 15 - Train Loss: 0.134363\n",
      "Checkpoint saved for fold 0, session 3, epoch 15\n",
      "Session 3 Epoch 16 - Train Loss: 0.133246\n",
      "Checkpoint saved for fold 0, session 3, epoch 16\n",
      "Session 3 Epoch 17 - Train Loss: 0.132485\n",
      "Checkpoint saved for fold 0, session 3, epoch 17\n",
      "Session 3 Epoch 18 - Train Loss: 0.132421\n",
      "Session 3 Epoch 19 - Train Loss: 0.131562\n",
      "Checkpoint saved for fold 0, session 3, epoch 19\n",
      "Session 3 Epoch 20 - Train Loss: 0.130942\n",
      "Checkpoint saved for fold 0, session 3, epoch 20\n",
      "Session 3 Epoch 21 - Train Loss: 0.130422\n",
      "Checkpoint saved for fold 0, session 3, epoch 21\n",
      "Session 3 Epoch 22 - Train Loss: 0.129484\n",
      "Checkpoint saved for fold 0, session 3, epoch 22\n",
      "Session 3 Epoch 23 - Train Loss: 0.128944\n",
      "Checkpoint saved for fold 0, session 3, epoch 23\n",
      "Session 3 Epoch 24 - Train Loss: 0.128764\n",
      "Checkpoint saved for fold 0, session 3, epoch 24\n",
      "Session 3 Epoch 25 - Train Loss: 0.127753\n",
      "Checkpoint saved for fold 0, session 3, epoch 25\n",
      "Session 3 Epoch 26 - Train Loss: 0.127555\n",
      "Checkpoint saved for fold 0, session 3, epoch 26\n",
      "Session 3 Epoch 27 - Train Loss: 0.126555\n",
      "Checkpoint saved for fold 0, session 3, epoch 27\n",
      "Session 3 Epoch 28 - Train Loss: 0.126126\n",
      "Checkpoint saved for fold 0, session 3, epoch 28\n",
      "Session 3 Epoch 29 - Train Loss: 0.125430\n",
      "Checkpoint saved for fold 0, session 3, epoch 29\n",
      "Session 3 Epoch 30 - Train Loss: 0.124815\n",
      "Checkpoint saved for fold 0, session 3, epoch 30\n",
      "Session 3 Epoch 31 - Train Loss: 0.124412\n",
      "Checkpoint saved for fold 0, session 3, epoch 31\n",
      "Session 3 Epoch 32 - Train Loss: 0.123792\n",
      "Checkpoint saved for fold 0, session 3, epoch 32\n",
      "Session 3 Epoch 33 - Train Loss: 0.122864\n",
      "Checkpoint saved for fold 0, session 3, epoch 33\n",
      "Session 3 Epoch 34 - Train Loss: 0.122646\n",
      "Checkpoint saved for fold 0, session 3, epoch 34\n",
      "Session 3 Epoch 35 - Train Loss: 0.121951\n",
      "Checkpoint saved for fold 0, session 3, epoch 35\n",
      "Session 3 Epoch 36 - Train Loss: 0.121393\n",
      "Checkpoint saved for fold 0, session 3, epoch 36\n",
      "Session 3 Epoch 37 - Train Loss: 0.120722\n",
      "Checkpoint saved for fold 0, session 3, epoch 37\n",
      "Session 3 Epoch 38 - Train Loss: 0.119961\n",
      "Checkpoint saved for fold 0, session 3, epoch 38\n",
      "Session 3 Epoch 39 - Train Loss: 0.119773\n",
      "Checkpoint saved for fold 0, session 3, epoch 39\n",
      "Session 3 Epoch 40 - Train Loss: 0.119368\n",
      "Checkpoint saved for fold 0, session 3, epoch 40\n",
      "Session 3 Epoch 41 - Train Loss: 0.118989\n",
      "Checkpoint saved for fold 0, session 3, epoch 41\n",
      "Session 3 Epoch 42 - Train Loss: 0.117862\n",
      "Checkpoint saved for fold 0, session 3, epoch 42\n",
      "Session 3 Epoch 43 - Train Loss: 0.117898\n",
      "Session 3 Epoch 44 - Train Loss: 0.117293\n",
      "Checkpoint saved for fold 0, session 3, epoch 44\n",
      "Session 3 Epoch 45 - Train Loss: 0.117186\n",
      "Checkpoint saved for fold 0, session 3, epoch 45\n",
      "Session 3 Epoch 46 - Train Loss: 0.115816\n",
      "Checkpoint saved for fold 0, session 3, epoch 46\n",
      "Session 3 Epoch 47 - Train Loss: 0.115505\n",
      "Checkpoint saved for fold 0, session 3, epoch 47\n",
      "Session 3 Epoch 48 - Train Loss: 0.115052\n",
      "Checkpoint saved for fold 0, session 3, epoch 48\n",
      "Session 3 Epoch 49 - Train Loss: 0.114628\n",
      "Checkpoint saved for fold 0, session 3, epoch 49\n",
      "Session 3 Epoch 50 - Train Loss: 0.113753\n",
      "Checkpoint saved for fold 0, session 3, epoch 50\n",
      "Session 3 Epoch 51 - Train Loss: 0.113802\n",
      "Session 3 Epoch 52 - Train Loss: 0.113216\n",
      "Checkpoint saved for fold 0, session 3, epoch 52\n",
      "Session 3 Epoch 53 - Train Loss: 0.112733\n",
      "Checkpoint saved for fold 0, session 3, epoch 53\n",
      "Session 3 Epoch 54 - Train Loss: 0.112320\n",
      "Checkpoint saved for fold 0, session 3, epoch 54\n",
      "Session 3 Epoch 55 - Train Loss: 0.111650\n",
      "Checkpoint saved for fold 0, session 3, epoch 55\n",
      "Session 3 Epoch 56 - Train Loss: 0.111233\n",
      "Checkpoint saved for fold 0, session 3, epoch 56\n",
      "Session 3 Epoch 57 - Train Loss: 0.110717\n",
      "Checkpoint saved for fold 0, session 3, epoch 57\n",
      "Session 3 Epoch 58 - Train Loss: 0.110345\n",
      "Checkpoint saved for fold 0, session 3, epoch 58\n",
      "Session 3 Epoch 59 - Train Loss: 0.109654\n",
      "Checkpoint saved for fold 0, session 3, epoch 59\n",
      "Session 3 Epoch 60 - Train Loss: 0.109356\n",
      "Checkpoint saved for fold 0, session 3, epoch 60\n",
      "Session 3 Epoch 61 - Train Loss: 0.109336\n",
      "Session 3 Epoch 62 - Train Loss: 0.108378\n",
      "Checkpoint saved for fold 0, session 3, epoch 62\n",
      "Session 3 Epoch 63 - Train Loss: 0.107852\n",
      "Checkpoint saved for fold 0, session 3, epoch 63\n",
      "Session 3 Epoch 64 - Train Loss: 0.107473\n",
      "Checkpoint saved for fold 0, session 3, epoch 64\n",
      "Session 3 Epoch 65 - Train Loss: 0.107141\n",
      "Checkpoint saved for fold 0, session 3, epoch 65\n",
      "Session 3 Epoch 66 - Train Loss: 0.106633\n",
      "Checkpoint saved for fold 0, session 3, epoch 66\n",
      "Session 3 Epoch 67 - Train Loss: 0.106280\n",
      "Checkpoint saved for fold 0, session 3, epoch 67\n",
      "Session 3 Epoch 68 - Train Loss: 0.105929\n",
      "Checkpoint saved for fold 0, session 3, epoch 68\n",
      "Session 3 Epoch 69 - Train Loss: 0.105763\n",
      "Checkpoint saved for fold 0, session 3, epoch 69\n",
      "Session 3 Epoch 70 - Train Loss: 0.104524\n",
      "Checkpoint saved for fold 0, session 3, epoch 70\n",
      "Session 3 Epoch 71 - Train Loss: 0.104445\n",
      "Session 3 Epoch 72 - Train Loss: 0.104624\n",
      "Session 3 Epoch 73 - Train Loss: 0.103851\n",
      "Checkpoint saved for fold 0, session 3, epoch 73\n",
      "Session 3 Epoch 74 - Train Loss: 0.103215\n",
      "Checkpoint saved for fold 0, session 3, epoch 74\n",
      "Session 3 Epoch 75 - Train Loss: 0.103040\n",
      "Checkpoint saved for fold 0, session 3, epoch 75\n",
      "Session 3 Epoch 76 - Train Loss: 0.102460\n",
      "Checkpoint saved for fold 0, session 3, epoch 76\n",
      "Session 3 Epoch 77 - Train Loss: 0.102276\n",
      "Checkpoint saved for fold 0, session 3, epoch 77\n",
      "Session 3 Epoch 78 - Train Loss: 0.101702\n",
      "Checkpoint saved for fold 0, session 3, epoch 78\n",
      "Session 3 Epoch 79 - Train Loss: 0.101111\n",
      "Checkpoint saved for fold 0, session 3, epoch 79\n",
      "Session 3 Epoch 80 - Train Loss: 0.100884\n",
      "Checkpoint saved for fold 0, session 3, epoch 80\n",
      "Training on Session 5/9\n",
      "Session 4 Epoch 1 - Train Loss: 0.199184\n",
      "Checkpoint saved for fold 0, session 4, epoch 1\n",
      "Session 4 Epoch 2 - Train Loss: 0.198320\n",
      "Checkpoint saved for fold 0, session 4, epoch 2\n",
      "Session 4 Epoch 3 - Train Loss: 0.198191\n",
      "Checkpoint saved for fold 0, session 4, epoch 3\n",
      "Session 4 Epoch 4 - Train Loss: 0.196989\n",
      "Checkpoint saved for fold 0, session 4, epoch 4\n",
      "Session 4 Epoch 5 - Train Loss: 0.196666\n",
      "Checkpoint saved for fold 0, session 4, epoch 5\n",
      "Session 4 Epoch 6 - Train Loss: 0.195859\n",
      "Checkpoint saved for fold 0, session 4, epoch 6\n",
      "Session 4 Epoch 7 - Train Loss: 0.195545\n",
      "Checkpoint saved for fold 0, session 4, epoch 7\n",
      "Session 4 Epoch 8 - Train Loss: 0.195272\n",
      "Checkpoint saved for fold 0, session 4, epoch 8\n",
      "Session 4 Epoch 9 - Train Loss: 0.194167\n",
      "Checkpoint saved for fold 0, session 4, epoch 9\n",
      "Session 4 Epoch 10 - Train Loss: 0.193649\n",
      "Checkpoint saved for fold 0, session 4, epoch 10\n",
      "Session 4 Epoch 11 - Train Loss: 0.192930\n",
      "Checkpoint saved for fold 0, session 4, epoch 11\n",
      "Session 4 Epoch 12 - Train Loss: 0.192806\n",
      "Checkpoint saved for fold 0, session 4, epoch 12\n",
      "Session 4 Epoch 13 - Train Loss: 0.191392\n",
      "Checkpoint saved for fold 0, session 4, epoch 13\n",
      "Session 4 Epoch 14 - Train Loss: 0.190995\n",
      "Checkpoint saved for fold 0, session 4, epoch 14\n",
      "Session 4 Epoch 15 - Train Loss: 0.190266\n",
      "Checkpoint saved for fold 0, session 4, epoch 15\n",
      "Session 4 Epoch 16 - Train Loss: 0.189571\n",
      "Checkpoint saved for fold 0, session 4, epoch 16\n",
      "Session 4 Epoch 17 - Train Loss: 0.189649\n",
      "Session 4 Epoch 18 - Train Loss: 0.188496\n",
      "Checkpoint saved for fold 0, session 4, epoch 18\n",
      "Session 4 Epoch 19 - Train Loss: 0.188292\n",
      "Checkpoint saved for fold 0, session 4, epoch 19\n",
      "Session 4 Epoch 20 - Train Loss: 0.187554\n",
      "Checkpoint saved for fold 0, session 4, epoch 20\n",
      "Session 4 Epoch 21 - Train Loss: 0.187369\n",
      "Checkpoint saved for fold 0, session 4, epoch 21\n",
      "Session 4 Epoch 22 - Train Loss: 0.186227\n",
      "Checkpoint saved for fold 0, session 4, epoch 22\n",
      "Session 4 Epoch 23 - Train Loss: 0.186363\n",
      "Session 4 Epoch 24 - Train Loss: 0.185459\n",
      "Checkpoint saved for fold 0, session 4, epoch 24\n",
      "Session 4 Epoch 25 - Train Loss: 0.184372\n",
      "Checkpoint saved for fold 0, session 4, epoch 25\n",
      "Session 4 Epoch 26 - Train Loss: 0.184223\n",
      "Checkpoint saved for fold 0, session 4, epoch 26\n",
      "Session 4 Epoch 27 - Train Loss: 0.183691\n",
      "Checkpoint saved for fold 0, session 4, epoch 27\n",
      "Session 4 Epoch 28 - Train Loss: 0.183117\n",
      "Checkpoint saved for fold 0, session 4, epoch 28\n",
      "Session 4 Epoch 29 - Train Loss: 0.183183\n",
      "Session 4 Epoch 30 - Train Loss: 0.182392\n",
      "Checkpoint saved for fold 0, session 4, epoch 30\n",
      "Session 4 Epoch 31 - Train Loss: 0.181549\n",
      "Checkpoint saved for fold 0, session 4, epoch 31\n",
      "Session 4 Epoch 32 - Train Loss: 0.180971\n",
      "Checkpoint saved for fold 0, session 4, epoch 32\n",
      "Session 4 Epoch 33 - Train Loss: 0.180590\n",
      "Checkpoint saved for fold 0, session 4, epoch 33\n",
      "Session 4 Epoch 34 - Train Loss: 0.180299\n",
      "Checkpoint saved for fold 0, session 4, epoch 34\n",
      "Session 4 Epoch 35 - Train Loss: 0.179304\n",
      "Checkpoint saved for fold 0, session 4, epoch 35\n",
      "Session 4 Epoch 36 - Train Loss: 0.178934\n",
      "Checkpoint saved for fold 0, session 4, epoch 36\n",
      "Session 4 Epoch 37 - Train Loss: 0.177987\n",
      "Checkpoint saved for fold 0, session 4, epoch 37\n",
      "Session 4 Epoch 38 - Train Loss: 0.178179\n",
      "Session 4 Epoch 39 - Train Loss: 0.177573\n",
      "Checkpoint saved for fold 0, session 4, epoch 39\n",
      "Session 4 Epoch 40 - Train Loss: 0.177240\n",
      "Checkpoint saved for fold 0, session 4, epoch 40\n",
      "Session 4 Epoch 41 - Train Loss: 0.176286\n",
      "Checkpoint saved for fold 0, session 4, epoch 41\n",
      "Session 4 Epoch 42 - Train Loss: 0.175868\n",
      "Checkpoint saved for fold 0, session 4, epoch 42\n",
      "Session 4 Epoch 43 - Train Loss: 0.175554\n",
      "Checkpoint saved for fold 0, session 4, epoch 43\n",
      "Session 4 Epoch 44 - Train Loss: 0.174754\n",
      "Checkpoint saved for fold 0, session 4, epoch 44\n",
      "Session 4 Epoch 45 - Train Loss: 0.174269\n",
      "Checkpoint saved for fold 0, session 4, epoch 45\n",
      "Session 4 Epoch 46 - Train Loss: 0.173853\n",
      "Checkpoint saved for fold 0, session 4, epoch 46\n",
      "Session 4 Epoch 47 - Train Loss: 0.173020\n",
      "Checkpoint saved for fold 0, session 4, epoch 47\n",
      "Session 4 Epoch 48 - Train Loss: 0.172401\n",
      "Checkpoint saved for fold 0, session 4, epoch 48\n",
      "Session 4 Epoch 49 - Train Loss: 0.172489\n",
      "Session 4 Epoch 50 - Train Loss: 0.171642\n",
      "Checkpoint saved for fold 0, session 4, epoch 50\n",
      "Session 4 Epoch 51 - Train Loss: 0.171732\n",
      "Session 4 Epoch 52 - Train Loss: 0.170722\n",
      "Checkpoint saved for fold 0, session 4, epoch 52\n",
      "Session 4 Epoch 53 - Train Loss: 0.170567\n",
      "Checkpoint saved for fold 0, session 4, epoch 53\n",
      "Session 4 Epoch 54 - Train Loss: 0.169942\n",
      "Checkpoint saved for fold 0, session 4, epoch 54\n",
      "Session 4 Epoch 55 - Train Loss: 0.169558\n",
      "Checkpoint saved for fold 0, session 4, epoch 55\n",
      "Session 4 Epoch 56 - Train Loss: 0.168554\n",
      "Checkpoint saved for fold 0, session 4, epoch 56\n",
      "Session 4 Epoch 57 - Train Loss: 0.168479\n",
      "Session 4 Epoch 58 - Train Loss: 0.168311\n",
      "Checkpoint saved for fold 0, session 4, epoch 58\n",
      "Session 4 Epoch 59 - Train Loss: 0.167649\n",
      "Checkpoint saved for fold 0, session 4, epoch 59\n",
      "Session 4 Epoch 60 - Train Loss: 0.167367\n",
      "Checkpoint saved for fold 0, session 4, epoch 60\n",
      "Session 4 Epoch 61 - Train Loss: 0.166387\n",
      "Checkpoint saved for fold 0, session 4, epoch 61\n",
      "Session 4 Epoch 62 - Train Loss: 0.166467\n",
      "Session 4 Epoch 63 - Train Loss: 0.166117\n",
      "Checkpoint saved for fold 0, session 4, epoch 63\n",
      "Session 4 Epoch 64 - Train Loss: 0.165505\n",
      "Checkpoint saved for fold 0, session 4, epoch 64\n",
      "Session 4 Epoch 65 - Train Loss: 0.165063\n",
      "Checkpoint saved for fold 0, session 4, epoch 65\n",
      "Session 4 Epoch 66 - Train Loss: 0.164598\n",
      "Checkpoint saved for fold 0, session 4, epoch 66\n",
      "Session 4 Epoch 67 - Train Loss: 0.163987\n",
      "Checkpoint saved for fold 0, session 4, epoch 67\n",
      "Session 4 Epoch 68 - Train Loss: 0.163524\n",
      "Checkpoint saved for fold 0, session 4, epoch 68\n",
      "Session 4 Epoch 69 - Train Loss: 0.163398\n",
      "Checkpoint saved for fold 0, session 4, epoch 69\n",
      "Session 4 Epoch 70 - Train Loss: 0.162815\n",
      "Checkpoint saved for fold 0, session 4, epoch 70\n",
      "Session 4 Epoch 71 - Train Loss: 0.162045\n",
      "Checkpoint saved for fold 0, session 4, epoch 71\n",
      "Session 4 Epoch 72 - Train Loss: 0.161865\n",
      "Checkpoint saved for fold 0, session 4, epoch 72\n",
      "Session 4 Epoch 73 - Train Loss: 0.161690\n",
      "Checkpoint saved for fold 0, session 4, epoch 73\n",
      "Session 4 Epoch 74 - Train Loss: 0.161142\n",
      "Checkpoint saved for fold 0, session 4, epoch 74\n",
      "Session 4 Epoch 75 - Train Loss: 0.160614\n",
      "Checkpoint saved for fold 0, session 4, epoch 75\n",
      "Session 4 Epoch 76 - Train Loss: 0.160606\n",
      "Session 4 Epoch 77 - Train Loss: 0.160099\n",
      "Checkpoint saved for fold 0, session 4, epoch 77\n",
      "Session 4 Epoch 78 - Train Loss: 0.159229\n",
      "Checkpoint saved for fold 0, session 4, epoch 78\n",
      "Session 4 Epoch 79 - Train Loss: 0.158881\n",
      "Checkpoint saved for fold 0, session 4, epoch 79\n",
      "Session 4 Epoch 80 - Train Loss: 0.158847\n",
      "Training on Session 6/9\n",
      "Session 5 Epoch 1 - Train Loss: 0.099853\n",
      "Checkpoint saved for fold 0, session 5, epoch 1\n",
      "Session 5 Epoch 2 - Train Loss: 0.099787\n",
      "Session 5 Epoch 3 - Train Loss: 0.099966\n",
      "Session 5 Epoch 4 - Train Loss: 0.099818\n",
      "Session 5 Epoch 5 - Train Loss: 0.099792\n",
      "Session 5 Epoch 6 - Train Loss: 0.100298\n",
      "Session 5 Epoch 7 - Train Loss: 0.099447\n",
      "Checkpoint saved for fold 0, session 5, epoch 7\n",
      "Session 5 Epoch 8 - Train Loss: 0.099402\n",
      "Session 5 Epoch 9 - Train Loss: 0.099900\n",
      "Session 5 Epoch 10 - Train Loss: 0.099397\n",
      "Session 5 Epoch 11 - Train Loss: 0.099579\n",
      "Session 5 Epoch 12 - Train Loss: 0.099362\n",
      "Session 5 Epoch 13 - Train Loss: 0.099334\n",
      "Checkpoint saved for fold 0, session 5, epoch 13\n",
      "Session 5 Epoch 14 - Train Loss: 0.099221\n",
      "Checkpoint saved for fold 0, session 5, epoch 14\n",
      "Session 5 Epoch 15 - Train Loss: 0.099371\n",
      "Session 5 Epoch 16 - Train Loss: 0.099423\n",
      "Session 5 Epoch 17 - Train Loss: 0.099219\n",
      "Session 5 Epoch 18 - Train Loss: 0.098817\n",
      "Checkpoint saved for fold 0, session 5, epoch 18\n",
      "Session 5 Epoch 19 - Train Loss: 0.099024\n",
      "Session 5 Epoch 20 - Train Loss: 0.098855\n",
      "Session 5 Epoch 21 - Train Loss: 0.098799\n",
      "Session 5 Epoch 22 - Train Loss: 0.098539\n",
      "Checkpoint saved for fold 0, session 5, epoch 22\n",
      "Session 5 Epoch 23 - Train Loss: 0.098479\n",
      "Session 5 Epoch 24 - Train Loss: 0.098670\n",
      "Session 5 Epoch 25 - Train Loss: 0.098464\n",
      "Session 5 Epoch 26 - Train Loss: 0.098559\n",
      "Session 5 Epoch 27 - Train Loss: 0.098558\n",
      "Session 5 Epoch 28 - Train Loss: 0.098347\n",
      "Checkpoint saved for fold 0, session 5, epoch 28\n",
      "Session 5 Epoch 29 - Train Loss: 0.098547\n",
      "Session 5 Epoch 30 - Train Loss: 0.098444\n",
      "Session 5 Epoch 31 - Train Loss: 0.098185\n",
      "Checkpoint saved for fold 0, session 5, epoch 31\n",
      "Session 5 Epoch 32 - Train Loss: 0.098096\n",
      "Session 5 Epoch 33 - Train Loss: 0.098244\n",
      "Session 5 Epoch 34 - Train Loss: 0.098001\n",
      "Checkpoint saved for fold 0, session 5, epoch 34\n",
      "Session 5 Epoch 35 - Train Loss: 0.098250\n",
      "Session 5 Epoch 36 - Train Loss: 0.098330\n",
      "Session 5 Epoch 37 - Train Loss: 0.097644\n",
      "Checkpoint saved for fold 0, session 5, epoch 37\n",
      "Session 5 Epoch 38 - Train Loss: 0.098149\n",
      "Session 5 Epoch 39 - Train Loss: 0.097824\n",
      "Session 5 Epoch 40 - Train Loss: 0.097877\n",
      "Session 5 Epoch 41 - Train Loss: 0.098085\n",
      "Session 5 Epoch 42 - Train Loss: 0.097979\n",
      "Session 5 Epoch 43 - Train Loss: 0.097485\n",
      "Checkpoint saved for fold 0, session 5, epoch 43\n",
      "Session 5 Epoch 44 - Train Loss: 0.097567\n",
      "Session 5 Epoch 45 - Train Loss: 0.097630\n",
      "Session 5 Epoch 46 - Train Loss: 0.097263\n",
      "Checkpoint saved for fold 0, session 5, epoch 46\n",
      "Session 5 Epoch 47 - Train Loss: 0.097124\n",
      "Checkpoint saved for fold 0, session 5, epoch 47\n",
      "Session 5 Epoch 48 - Train Loss: 0.097403\n",
      "Session 5 Epoch 49 - Train Loss: 0.097591\n",
      "Session 5 Epoch 50 - Train Loss: 0.097458\n",
      "Session 5 Epoch 51 - Train Loss: 0.097392\n",
      "Session 5 Epoch 52 - Train Loss: 0.097173\n",
      "Session 5 Epoch 53 - Train Loss: 0.097214\n",
      "Session 5 Epoch 54 - Train Loss: 0.097412\n",
      "Session 5 Epoch 55 - Train Loss: 0.097198\n",
      "Session 5 Epoch 56 - Train Loss: 0.097171\n",
      "Session 5 Epoch 57 - Train Loss: 0.097260\n",
      "Early stopping at epoch 57 for session 5\n",
      "Training on Session 7/9\n",
      "Session 6 Epoch 1 - Train Loss: 0.126774\n",
      "Checkpoint saved for fold 0, session 6, epoch 1\n",
      "Session 6 Epoch 2 - Train Loss: 0.125813\n",
      "Checkpoint saved for fold 0, session 6, epoch 2\n",
      "Session 6 Epoch 3 - Train Loss: 0.125745\n",
      "Session 6 Epoch 4 - Train Loss: 0.124638\n",
      "Checkpoint saved for fold 0, session 6, epoch 4\n",
      "Session 6 Epoch 5 - Train Loss: 0.124323\n",
      "Checkpoint saved for fold 0, session 6, epoch 5\n",
      "Session 6 Epoch 6 - Train Loss: 0.123472\n",
      "Checkpoint saved for fold 0, session 6, epoch 6\n",
      "Session 6 Epoch 7 - Train Loss: 0.122803\n",
      "Checkpoint saved for fold 0, session 6, epoch 7\n",
      "Session 6 Epoch 8 - Train Loss: 0.122405\n",
      "Checkpoint saved for fold 0, session 6, epoch 8\n",
      "Session 6 Epoch 9 - Train Loss: 0.121867\n",
      "Checkpoint saved for fold 0, session 6, epoch 9\n",
      "Session 6 Epoch 10 - Train Loss: 0.121311\n",
      "Checkpoint saved for fold 0, session 6, epoch 10\n",
      "Session 6 Epoch 11 - Train Loss: 0.120813\n",
      "Checkpoint saved for fold 0, session 6, epoch 11\n",
      "Session 6 Epoch 12 - Train Loss: 0.119763\n",
      "Checkpoint saved for fold 0, session 6, epoch 12\n",
      "Session 6 Epoch 13 - Train Loss: 0.119513\n",
      "Checkpoint saved for fold 0, session 6, epoch 13\n",
      "Session 6 Epoch 14 - Train Loss: 0.118886\n",
      "Checkpoint saved for fold 0, session 6, epoch 14\n",
      "Session 6 Epoch 15 - Train Loss: 0.118276\n",
      "Checkpoint saved for fold 0, session 6, epoch 15\n",
      "Session 6 Epoch 16 - Train Loss: 0.117858\n",
      "Checkpoint saved for fold 0, session 6, epoch 16\n",
      "Session 6 Epoch 17 - Train Loss: 0.117073\n",
      "Checkpoint saved for fold 0, session 6, epoch 17\n",
      "Session 6 Epoch 18 - Train Loss: 0.116734\n",
      "Checkpoint saved for fold 0, session 6, epoch 18\n",
      "Session 6 Epoch 19 - Train Loss: 0.115876\n",
      "Checkpoint saved for fold 0, session 6, epoch 19\n",
      "Session 6 Epoch 20 - Train Loss: 0.115541\n",
      "Checkpoint saved for fold 0, session 6, epoch 20\n",
      "Session 6 Epoch 21 - Train Loss: 0.115303\n",
      "Checkpoint saved for fold 0, session 6, epoch 21\n",
      "Session 6 Epoch 22 - Train Loss: 0.114315\n",
      "Checkpoint saved for fold 0, session 6, epoch 22\n",
      "Session 6 Epoch 23 - Train Loss: 0.113997\n",
      "Checkpoint saved for fold 0, session 6, epoch 23\n",
      "Session 6 Epoch 24 - Train Loss: 0.113616\n",
      "Checkpoint saved for fold 0, session 6, epoch 24\n",
      "Session 6 Epoch 25 - Train Loss: 0.113239\n",
      "Checkpoint saved for fold 0, session 6, epoch 25\n",
      "Session 6 Epoch 26 - Train Loss: 0.112533\n",
      "Checkpoint saved for fold 0, session 6, epoch 26\n",
      "Session 6 Epoch 27 - Train Loss: 0.112326\n",
      "Checkpoint saved for fold 0, session 6, epoch 27\n",
      "Session 6 Epoch 28 - Train Loss: 0.112085\n",
      "Checkpoint saved for fold 0, session 6, epoch 28\n",
      "Session 6 Epoch 29 - Train Loss: 0.111184\n",
      "Checkpoint saved for fold 0, session 6, epoch 29\n",
      "Session 6 Epoch 30 - Train Loss: 0.111186\n",
      "Session 6 Epoch 31 - Train Loss: 0.109838\n",
      "Checkpoint saved for fold 0, session 6, epoch 31\n",
      "Session 6 Epoch 32 - Train Loss: 0.109983\n",
      "Session 6 Epoch 33 - Train Loss: 0.109324\n",
      "Checkpoint saved for fold 0, session 6, epoch 33\n",
      "Session 6 Epoch 34 - Train Loss: 0.108876\n",
      "Checkpoint saved for fold 0, session 6, epoch 34\n",
      "Session 6 Epoch 35 - Train Loss: 0.108403\n",
      "Checkpoint saved for fold 0, session 6, epoch 35\n",
      "Session 6 Epoch 36 - Train Loss: 0.107726\n",
      "Checkpoint saved for fold 0, session 6, epoch 36\n",
      "Session 6 Epoch 37 - Train Loss: 0.107510\n",
      "Checkpoint saved for fold 0, session 6, epoch 37\n",
      "Session 6 Epoch 38 - Train Loss: 0.107928\n",
      "Session 6 Epoch 39 - Train Loss: 0.107118\n",
      "Checkpoint saved for fold 0, session 6, epoch 39\n",
      "Session 6 Epoch 40 - Train Loss: 0.106179\n",
      "Checkpoint saved for fold 0, session 6, epoch 40\n",
      "Session 6 Epoch 41 - Train Loss: 0.105548\n",
      "Checkpoint saved for fold 0, session 6, epoch 41\n",
      "Session 6 Epoch 42 - Train Loss: 0.105605\n",
      "Session 6 Epoch 43 - Train Loss: 0.105176\n",
      "Checkpoint saved for fold 0, session 6, epoch 43\n",
      "Session 6 Epoch 44 - Train Loss: 0.104778\n",
      "Checkpoint saved for fold 0, session 6, epoch 44\n",
      "Session 6 Epoch 45 - Train Loss: 0.104389\n",
      "Checkpoint saved for fold 0, session 6, epoch 45\n",
      "Session 6 Epoch 46 - Train Loss: 0.103867\n",
      "Checkpoint saved for fold 0, session 6, epoch 46\n",
      "Session 6 Epoch 47 - Train Loss: 0.103657\n",
      "Checkpoint saved for fold 0, session 6, epoch 47\n",
      "Session 6 Epoch 48 - Train Loss: 0.103010\n",
      "Checkpoint saved for fold 0, session 6, epoch 48\n",
      "Session 6 Epoch 49 - Train Loss: 0.102298\n",
      "Checkpoint saved for fold 0, session 6, epoch 49\n",
      "Session 6 Epoch 50 - Train Loss: 0.102358\n",
      "Session 6 Epoch 51 - Train Loss: 0.101678\n",
      "Checkpoint saved for fold 0, session 6, epoch 51\n",
      "Session 6 Epoch 52 - Train Loss: 0.101637\n",
      "Session 6 Epoch 53 - Train Loss: 0.101350\n",
      "Checkpoint saved for fold 0, session 6, epoch 53\n",
      "Session 6 Epoch 54 - Train Loss: 0.100900\n",
      "Checkpoint saved for fold 0, session 6, epoch 54\n",
      "Session 6 Epoch 55 - Train Loss: 0.100595\n",
      "Checkpoint saved for fold 0, session 6, epoch 55\n",
      "Session 6 Epoch 56 - Train Loss: 0.099968\n",
      "Checkpoint saved for fold 0, session 6, epoch 56\n",
      "Session 6 Epoch 57 - Train Loss: 0.099646\n",
      "Checkpoint saved for fold 0, session 6, epoch 57\n",
      "Session 6 Epoch 58 - Train Loss: 0.099300\n",
      "Checkpoint saved for fold 0, session 6, epoch 58\n",
      "Session 6 Epoch 59 - Train Loss: 0.098838\n",
      "Checkpoint saved for fold 0, session 6, epoch 59\n",
      "Session 6 Epoch 60 - Train Loss: 0.098872\n",
      "Session 6 Epoch 61 - Train Loss: 0.098445\n",
      "Checkpoint saved for fold 0, session 6, epoch 61\n",
      "Session 6 Epoch 62 - Train Loss: 0.097917\n",
      "Checkpoint saved for fold 0, session 6, epoch 62\n",
      "Session 6 Epoch 63 - Train Loss: 0.097642\n",
      "Checkpoint saved for fold 0, session 6, epoch 63\n",
      "Session 6 Epoch 64 - Train Loss: 0.097207\n",
      "Checkpoint saved for fold 0, session 6, epoch 64\n",
      "Session 6 Epoch 65 - Train Loss: 0.097287\n",
      "Session 6 Epoch 66 - Train Loss: 0.096537\n",
      "Checkpoint saved for fold 0, session 6, epoch 66\n",
      "Session 6 Epoch 67 - Train Loss: 0.096078\n",
      "Checkpoint saved for fold 0, session 6, epoch 67\n",
      "Session 6 Epoch 68 - Train Loss: 0.096174\n",
      "Session 6 Epoch 69 - Train Loss: 0.095491\n",
      "Checkpoint saved for fold 0, session 6, epoch 69\n",
      "Session 6 Epoch 70 - Train Loss: 0.095229\n",
      "Checkpoint saved for fold 0, session 6, epoch 70\n",
      "Session 6 Epoch 71 - Train Loss: 0.095205\n",
      "Session 6 Epoch 72 - Train Loss: 0.094624\n",
      "Checkpoint saved for fold 0, session 6, epoch 72\n",
      "Session 6 Epoch 73 - Train Loss: 0.094376\n",
      "Checkpoint saved for fold 0, session 6, epoch 73\n",
      "Session 6 Epoch 74 - Train Loss: 0.094195\n",
      "Checkpoint saved for fold 0, session 6, epoch 74\n",
      "Session 6 Epoch 75 - Train Loss: 0.093859\n",
      "Checkpoint saved for fold 0, session 6, epoch 75\n",
      "Session 6 Epoch 76 - Train Loss: 0.093670\n",
      "Checkpoint saved for fold 0, session 6, epoch 76\n",
      "Session 6 Epoch 77 - Train Loss: 0.093486\n",
      "Checkpoint saved for fold 0, session 6, epoch 77\n",
      "Session 6 Epoch 78 - Train Loss: 0.092881\n",
      "Checkpoint saved for fold 0, session 6, epoch 78\n",
      "Session 6 Epoch 79 - Train Loss: 0.092561\n",
      "Checkpoint saved for fold 0, session 6, epoch 79\n",
      "Session 6 Epoch 80 - Train Loss: 0.092164\n",
      "Checkpoint saved for fold 0, session 6, epoch 80\n",
      "Training on Session 8/9\n",
      "Session 7 Epoch 1 - Train Loss: 0.146299\n",
      "Checkpoint saved for fold 0, session 7, epoch 1\n",
      "Session 7 Epoch 2 - Train Loss: 0.146236\n",
      "Session 7 Epoch 3 - Train Loss: 0.145992\n",
      "Checkpoint saved for fold 0, session 7, epoch 3\n",
      "Session 7 Epoch 4 - Train Loss: 0.145622\n",
      "Checkpoint saved for fold 0, session 7, epoch 4\n",
      "Session 7 Epoch 5 - Train Loss: 0.145230\n",
      "Checkpoint saved for fold 0, session 7, epoch 5\n",
      "Session 7 Epoch 6 - Train Loss: 0.145180\n",
      "Session 7 Epoch 7 - Train Loss: 0.145149\n",
      "Session 7 Epoch 8 - Train Loss: 0.144407\n",
      "Checkpoint saved for fold 0, session 7, epoch 8\n",
      "Session 7 Epoch 9 - Train Loss: 0.144560\n",
      "Session 7 Epoch 10 - Train Loss: 0.144423\n",
      "Session 7 Epoch 11 - Train Loss: 0.143820\n",
      "Checkpoint saved for fold 0, session 7, epoch 11\n",
      "Session 7 Epoch 12 - Train Loss: 0.143627\n",
      "Checkpoint saved for fold 0, session 7, epoch 12\n",
      "Session 7 Epoch 13 - Train Loss: 0.143343\n",
      "Checkpoint saved for fold 0, session 7, epoch 13\n",
      "Session 7 Epoch 14 - Train Loss: 0.143071\n",
      "Checkpoint saved for fold 0, session 7, epoch 14\n",
      "Session 7 Epoch 15 - Train Loss: 0.142939\n",
      "Checkpoint saved for fold 0, session 7, epoch 15\n",
      "Session 7 Epoch 16 - Train Loss: 0.142549\n",
      "Checkpoint saved for fold 0, session 7, epoch 16\n",
      "Session 7 Epoch 17 - Train Loss: 0.142474\n",
      "Session 7 Epoch 18 - Train Loss: 0.142206\n",
      "Checkpoint saved for fold 0, session 7, epoch 18\n",
      "Session 7 Epoch 19 - Train Loss: 0.142080\n",
      "Checkpoint saved for fold 0, session 7, epoch 19\n",
      "Session 7 Epoch 20 - Train Loss: 0.141497\n",
      "Checkpoint saved for fold 0, session 7, epoch 20\n",
      "Session 7 Epoch 21 - Train Loss: 0.141572\n",
      "Session 7 Epoch 22 - Train Loss: 0.141479\n",
      "Session 7 Epoch 23 - Train Loss: 0.141187\n",
      "Checkpoint saved for fold 0, session 7, epoch 23\n",
      "Session 7 Epoch 24 - Train Loss: 0.141296\n",
      "Session 7 Epoch 25 - Train Loss: 0.140919\n",
      "Checkpoint saved for fold 0, session 7, epoch 25\n",
      "Session 7 Epoch 26 - Train Loss: 0.140518\n",
      "Checkpoint saved for fold 0, session 7, epoch 26\n",
      "Session 7 Epoch 27 - Train Loss: 0.140406\n",
      "Checkpoint saved for fold 0, session 7, epoch 27\n",
      "Session 7 Epoch 28 - Train Loss: 0.140239\n",
      "Checkpoint saved for fold 0, session 7, epoch 28\n",
      "Session 7 Epoch 29 - Train Loss: 0.139858\n",
      "Checkpoint saved for fold 0, session 7, epoch 29\n",
      "Session 7 Epoch 30 - Train Loss: 0.139781\n",
      "Session 7 Epoch 31 - Train Loss: 0.139276\n",
      "Checkpoint saved for fold 0, session 7, epoch 31\n",
      "Session 7 Epoch 32 - Train Loss: 0.139314\n",
      "Session 7 Epoch 33 - Train Loss: 0.139065\n",
      "Checkpoint saved for fold 0, session 7, epoch 33\n",
      "Session 7 Epoch 34 - Train Loss: 0.138716\n",
      "Checkpoint saved for fold 0, session 7, epoch 34\n",
      "Session 7 Epoch 35 - Train Loss: 0.138584\n",
      "Checkpoint saved for fold 0, session 7, epoch 35\n",
      "Session 7 Epoch 36 - Train Loss: 0.138476\n",
      "Checkpoint saved for fold 0, session 7, epoch 36\n",
      "Session 7 Epoch 37 - Train Loss: 0.138190\n",
      "Checkpoint saved for fold 0, session 7, epoch 37\n",
      "Session 7 Epoch 38 - Train Loss: 0.137934\n",
      "Checkpoint saved for fold 0, session 7, epoch 38\n",
      "Session 7 Epoch 39 - Train Loss: 0.137510\n",
      "Checkpoint saved for fold 0, session 7, epoch 39\n",
      "Session 7 Epoch 40 - Train Loss: 0.137306\n",
      "Checkpoint saved for fold 0, session 7, epoch 40\n",
      "Session 7 Epoch 41 - Train Loss: 0.137188\n",
      "Checkpoint saved for fold 0, session 7, epoch 41\n",
      "Session 7 Epoch 42 - Train Loss: 0.137302\n",
      "Session 7 Epoch 43 - Train Loss: 0.137043\n",
      "Checkpoint saved for fold 0, session 7, epoch 43\n",
      "Session 7 Epoch 44 - Train Loss: 0.136870\n",
      "Checkpoint saved for fold 0, session 7, epoch 44\n",
      "Session 7 Epoch 45 - Train Loss: 0.136571\n",
      "Checkpoint saved for fold 0, session 7, epoch 45\n",
      "Session 7 Epoch 46 - Train Loss: 0.136292\n",
      "Checkpoint saved for fold 0, session 7, epoch 46\n",
      "Session 7 Epoch 47 - Train Loss: 0.136083\n",
      "Checkpoint saved for fold 0, session 7, epoch 47\n",
      "Session 7 Epoch 48 - Train Loss: 0.135966\n",
      "Checkpoint saved for fold 0, session 7, epoch 48\n",
      "Session 7 Epoch 49 - Train Loss: 0.135405\n",
      "Checkpoint saved for fold 0, session 7, epoch 49\n",
      "Session 7 Epoch 50 - Train Loss: 0.135214\n",
      "Checkpoint saved for fold 0, session 7, epoch 50\n",
      "Session 7 Epoch 51 - Train Loss: 0.135420\n",
      "Session 7 Epoch 52 - Train Loss: 0.135082\n",
      "Checkpoint saved for fold 0, session 7, epoch 52\n",
      "Session 7 Epoch 53 - Train Loss: 0.134957\n",
      "Checkpoint saved for fold 0, session 7, epoch 53\n",
      "Session 7 Epoch 54 - Train Loss: 0.134533\n",
      "Checkpoint saved for fold 0, session 7, epoch 54\n",
      "Session 7 Epoch 55 - Train Loss: 0.134470\n",
      "Session 7 Epoch 56 - Train Loss: 0.134248\n",
      "Checkpoint saved for fold 0, session 7, epoch 56\n",
      "Session 7 Epoch 57 - Train Loss: 0.133646\n",
      "Checkpoint saved for fold 0, session 7, epoch 57\n",
      "Session 7 Epoch 58 - Train Loss: 0.133756\n",
      "Session 7 Epoch 59 - Train Loss: 0.133732\n",
      "Session 7 Epoch 60 - Train Loss: 0.133513\n",
      "Checkpoint saved for fold 0, session 7, epoch 60\n",
      "Session 7 Epoch 61 - Train Loss: 0.133141\n",
      "Checkpoint saved for fold 0, session 7, epoch 61\n",
      "Session 7 Epoch 62 - Train Loss: 0.133178\n",
      "Session 7 Epoch 63 - Train Loss: 0.132776\n",
      "Checkpoint saved for fold 0, session 7, epoch 63\n",
      "Session 7 Epoch 64 - Train Loss: 0.132596\n",
      "Checkpoint saved for fold 0, session 7, epoch 64\n",
      "Session 7 Epoch 65 - Train Loss: 0.132251\n",
      "Checkpoint saved for fold 0, session 7, epoch 65\n",
      "Session 7 Epoch 66 - Train Loss: 0.132059\n",
      "Checkpoint saved for fold 0, session 7, epoch 66\n",
      "Session 7 Epoch 67 - Train Loss: 0.132136\n",
      "Session 7 Epoch 68 - Train Loss: 0.131611\n",
      "Checkpoint saved for fold 0, session 7, epoch 68\n",
      "Session 7 Epoch 69 - Train Loss: 0.131796\n",
      "Session 7 Epoch 70 - Train Loss: 0.131485\n",
      "Checkpoint saved for fold 0, session 7, epoch 70\n",
      "Session 7 Epoch 71 - Train Loss: 0.131309\n",
      "Checkpoint saved for fold 0, session 7, epoch 71\n",
      "Session 7 Epoch 72 - Train Loss: 0.130949\n",
      "Checkpoint saved for fold 0, session 7, epoch 72\n",
      "Session 7 Epoch 73 - Train Loss: 0.130893\n",
      "Session 7 Epoch 74 - Train Loss: 0.130793\n",
      "Checkpoint saved for fold 0, session 7, epoch 74\n",
      "Session 7 Epoch 75 - Train Loss: 0.130577\n",
      "Checkpoint saved for fold 0, session 7, epoch 75\n",
      "Session 7 Epoch 76 - Train Loss: 0.130556\n",
      "Session 7 Epoch 77 - Train Loss: 0.129881\n",
      "Checkpoint saved for fold 0, session 7, epoch 77\n",
      "Session 7 Epoch 78 - Train Loss: 0.130044\n",
      "Session 7 Epoch 79 - Train Loss: 0.129700\n",
      "Checkpoint saved for fold 0, session 7, epoch 79\n",
      "Session 7 Epoch 80 - Train Loss: 0.129466\n",
      "Checkpoint saved for fold 0, session 7, epoch 80\n",
      "Training on Session 9/9\n",
      "Session 8 Epoch 1 - Train Loss: 0.079198\n",
      "Checkpoint saved for fold 0, session 8, epoch 1\n",
      "Session 8 Epoch 2 - Train Loss: 0.079175\n",
      "Session 8 Epoch 3 - Train Loss: 0.078977\n",
      "Checkpoint saved for fold 0, session 8, epoch 3\n",
      "Session 8 Epoch 4 - Train Loss: 0.079129\n",
      "Session 8 Epoch 5 - Train Loss: 0.078908\n",
      "Session 8 Epoch 6 - Train Loss: 0.078855\n",
      "Checkpoint saved for fold 0, session 8, epoch 6\n",
      "Session 8 Epoch 7 - Train Loss: 0.078921\n",
      "Session 8 Epoch 8 - Train Loss: 0.078865\n",
      "Session 8 Epoch 9 - Train Loss: 0.078765\n",
      "Session 8 Epoch 10 - Train Loss: 0.078948\n",
      "Session 8 Epoch 11 - Train Loss: 0.078868\n",
      "Session 8 Epoch 12 - Train Loss: 0.078752\n",
      "Checkpoint saved for fold 0, session 8, epoch 12\n",
      "Session 8 Epoch 13 - Train Loss: 0.078610\n",
      "Checkpoint saved for fold 0, session 8, epoch 13\n",
      "Session 8 Epoch 14 - Train Loss: 0.078674\n",
      "Session 8 Epoch 15 - Train Loss: 0.078789\n",
      "Session 8 Epoch 16 - Train Loss: 0.078437\n",
      "Checkpoint saved for fold 0, session 8, epoch 16\n",
      "Session 8 Epoch 17 - Train Loss: 0.078811\n",
      "Session 8 Epoch 18 - Train Loss: 0.078819\n",
      "Session 8 Epoch 19 - Train Loss: 0.078300\n",
      "Checkpoint saved for fold 0, session 8, epoch 19\n",
      "Session 8 Epoch 20 - Train Loss: 0.078662\n",
      "Session 8 Epoch 21 - Train Loss: 0.078548\n",
      "Session 8 Epoch 22 - Train Loss: 0.078851\n",
      "Session 8 Epoch 23 - Train Loss: 0.078639\n",
      "Session 8 Epoch 24 - Train Loss: 0.078228\n",
      "Session 8 Epoch 25 - Train Loss: 0.078467\n",
      "Session 8 Epoch 26 - Train Loss: 0.078358\n",
      "Session 8 Epoch 27 - Train Loss: 0.078414\n",
      "Session 8 Epoch 28 - Train Loss: 0.078607\n",
      "Session 8 Epoch 29 - Train Loss: 0.078515\n",
      "Early stopping at epoch 29 for session 8\n",
      "Fold 0 - Test Loss: 0.0845, R^2: -349153187188641.9375\n",
      "\n",
      "=== Fold 1 ===\n",
      "Training on Session 1/9\n",
      "Session 0 Epoch 1 - Train Loss: 0.054543\n",
      "Checkpoint saved for fold 1, session 0, epoch 1\n",
      "Session 0 Epoch 2 - Train Loss: 0.053295\n",
      "Checkpoint saved for fold 1, session 0, epoch 2\n",
      "Session 0 Epoch 3 - Train Loss: 0.051444\n",
      "Checkpoint saved for fold 1, session 0, epoch 3\n",
      "Session 0 Epoch 4 - Train Loss: 0.046754\n",
      "Checkpoint saved for fold 1, session 0, epoch 4\n",
      "Session 0 Epoch 5 - Train Loss: 0.040622\n",
      "Checkpoint saved for fold 1, session 0, epoch 5\n",
      "Session 0 Epoch 6 - Train Loss: 0.037168\n",
      "Checkpoint saved for fold 1, session 0, epoch 6\n",
      "Session 0 Epoch 7 - Train Loss: 0.034288\n",
      "Checkpoint saved for fold 1, session 0, epoch 7\n",
      "Session 0 Epoch 8 - Train Loss: 0.032707\n",
      "Checkpoint saved for fold 1, session 0, epoch 8\n",
      "Session 0 Epoch 9 - Train Loss: 0.031815\n",
      "Checkpoint saved for fold 1, session 0, epoch 9\n",
      "Session 0 Epoch 10 - Train Loss: 0.029965\n",
      "Checkpoint saved for fold 1, session 0, epoch 10\n",
      "Session 0 Epoch 11 - Train Loss: 0.029220\n",
      "Checkpoint saved for fold 1, session 0, epoch 11\n",
      "Session 0 Epoch 12 - Train Loss: 0.028130\n",
      "Checkpoint saved for fold 1, session 0, epoch 12\n",
      "Session 0 Epoch 13 - Train Loss: 0.028382\n",
      "Session 0 Epoch 14 - Train Loss: 0.027719\n",
      "Checkpoint saved for fold 1, session 0, epoch 14\n",
      "Session 0 Epoch 15 - Train Loss: 0.026157\n",
      "Checkpoint saved for fold 1, session 0, epoch 15\n",
      "Session 0 Epoch 16 - Train Loss: 0.025984\n",
      "Checkpoint saved for fold 1, session 0, epoch 16\n",
      "Session 0 Epoch 17 - Train Loss: 0.024291\n",
      "Checkpoint saved for fold 1, session 0, epoch 17\n",
      "Session 0 Epoch 18 - Train Loss: 0.023833\n",
      "Checkpoint saved for fold 1, session 0, epoch 18\n",
      "Session 0 Epoch 19 - Train Loss: 0.024368\n",
      "Session 0 Epoch 20 - Train Loss: 0.024248\n",
      "Session 0 Epoch 21 - Train Loss: 0.023570\n",
      "Checkpoint saved for fold 1, session 0, epoch 21\n",
      "Session 0 Epoch 22 - Train Loss: 0.022933\n",
      "Checkpoint saved for fold 1, session 0, epoch 22\n",
      "Session 0 Epoch 23 - Train Loss: 0.022159\n",
      "Checkpoint saved for fold 1, session 0, epoch 23\n",
      "Session 0 Epoch 24 - Train Loss: 0.021020\n",
      "Checkpoint saved for fold 1, session 0, epoch 24\n",
      "Session 0 Epoch 25 - Train Loss: 0.021157\n",
      "Session 0 Epoch 26 - Train Loss: 0.020718\n",
      "Checkpoint saved for fold 1, session 0, epoch 26\n",
      "Session 0 Epoch 27 - Train Loss: 0.019294\n",
      "Checkpoint saved for fold 1, session 0, epoch 27\n",
      "Session 0 Epoch 28 - Train Loss: 0.018898\n",
      "Checkpoint saved for fold 1, session 0, epoch 28\n",
      "Session 0 Epoch 29 - Train Loss: 0.018033\n",
      "Checkpoint saved for fold 1, session 0, epoch 29\n",
      "Session 0 Epoch 30 - Train Loss: 0.017636\n",
      "Checkpoint saved for fold 1, session 0, epoch 30\n",
      "Session 0 Epoch 31 - Train Loss: 0.016964\n",
      "Checkpoint saved for fold 1, session 0, epoch 31\n",
      "Session 0 Epoch 32 - Train Loss: 0.016308\n",
      "Checkpoint saved for fold 1, session 0, epoch 32\n",
      "Session 0 Epoch 33 - Train Loss: 0.017982\n",
      "Session 0 Epoch 34 - Train Loss: 0.014927\n",
      "Checkpoint saved for fold 1, session 0, epoch 34\n",
      "Session 0 Epoch 35 - Train Loss: 0.015531\n",
      "Session 0 Epoch 36 - Train Loss: 0.014915\n",
      "Session 0 Epoch 37 - Train Loss: 0.013588\n",
      "Checkpoint saved for fold 1, session 0, epoch 37\n",
      "Session 0 Epoch 38 - Train Loss: 0.013047\n",
      "Checkpoint saved for fold 1, session 0, epoch 38\n",
      "Session 0 Epoch 39 - Train Loss: 0.012330\n",
      "Checkpoint saved for fold 1, session 0, epoch 39\n",
      "Session 0 Epoch 40 - Train Loss: 0.012426\n",
      "Session 0 Epoch 41 - Train Loss: 0.012031\n",
      "Checkpoint saved for fold 1, session 0, epoch 41\n",
      "Session 0 Epoch 42 - Train Loss: 0.011867\n",
      "Checkpoint saved for fold 1, session 0, epoch 42\n",
      "Session 0 Epoch 43 - Train Loss: 0.011649\n",
      "Checkpoint saved for fold 1, session 0, epoch 43\n",
      "Session 0 Epoch 44 - Train Loss: 0.012133\n",
      "Session 0 Epoch 45 - Train Loss: 0.011030\n",
      "Checkpoint saved for fold 1, session 0, epoch 45\n",
      "Session 0 Epoch 46 - Train Loss: 0.010933\n",
      "Session 0 Epoch 47 - Train Loss: 0.011634\n",
      "Session 0 Epoch 48 - Train Loss: 0.010818\n",
      "Checkpoint saved for fold 1, session 0, epoch 48\n",
      "Session 0 Epoch 49 - Train Loss: 0.011206\n",
      "Session 0 Epoch 50 - Train Loss: 0.010573\n",
      "Checkpoint saved for fold 1, session 0, epoch 50\n",
      "Session 0 Epoch 51 - Train Loss: 0.010125\n",
      "Checkpoint saved for fold 1, session 0, epoch 51\n",
      "Session 0 Epoch 52 - Train Loss: 0.010499\n",
      "Session 0 Epoch 53 - Train Loss: 0.010078\n",
      "Session 0 Epoch 54 - Train Loss: 0.009965\n",
      "Checkpoint saved for fold 1, session 0, epoch 54\n",
      "Session 0 Epoch 55 - Train Loss: 0.010275\n",
      "Session 0 Epoch 56 - Train Loss: 0.009953\n",
      "Session 0 Epoch 57 - Train Loss: 0.009855\n",
      "Checkpoint saved for fold 1, session 0, epoch 57\n",
      "Session 0 Epoch 58 - Train Loss: 0.009787\n",
      "Session 0 Epoch 59 - Train Loss: 0.009370\n",
      "Checkpoint saved for fold 1, session 0, epoch 59\n",
      "Session 0 Epoch 60 - Train Loss: 0.009388\n",
      "Session 0 Epoch 61 - Train Loss: 0.009959\n",
      "Session 0 Epoch 62 - Train Loss: 0.009390\n",
      "Session 0 Epoch 63 - Train Loss: 0.009072\n",
      "Checkpoint saved for fold 1, session 0, epoch 63\n",
      "Session 0 Epoch 64 - Train Loss: 0.008465\n",
      "Checkpoint saved for fold 1, session 0, epoch 64\n",
      "Session 0 Epoch 65 - Train Loss: 0.008874\n",
      "Session 0 Epoch 66 - Train Loss: 0.008298\n",
      "Checkpoint saved for fold 1, session 0, epoch 66\n",
      "Session 0 Epoch 67 - Train Loss: 0.007630\n",
      "Checkpoint saved for fold 1, session 0, epoch 67\n",
      "Session 0 Epoch 68 - Train Loss: 0.007882\n",
      "Session 0 Epoch 69 - Train Loss: 0.006850\n",
      "Checkpoint saved for fold 1, session 0, epoch 69\n",
      "Session 0 Epoch 70 - Train Loss: 0.006779\n",
      "Session 0 Epoch 71 - Train Loss: 0.006769\n",
      "Session 0 Epoch 72 - Train Loss: 0.007123\n",
      "Session 0 Epoch 73 - Train Loss: 0.007241\n",
      "Session 0 Epoch 74 - Train Loss: 0.006204\n",
      "Checkpoint saved for fold 1, session 0, epoch 74\n",
      "Session 0 Epoch 75 - Train Loss: 0.005805\n",
      "Checkpoint saved for fold 1, session 0, epoch 75\n",
      "Session 0 Epoch 76 - Train Loss: 0.006837\n",
      "Session 0 Epoch 77 - Train Loss: 0.005750\n",
      "Session 0 Epoch 78 - Train Loss: 0.006808\n",
      "Session 0 Epoch 79 - Train Loss: 0.005696\n",
      "Checkpoint saved for fold 1, session 0, epoch 79\n",
      "Session 0 Epoch 80 - Train Loss: 0.005654\n",
      "Training on Session 2/9\n",
      "Session 1 Epoch 1 - Train Loss: 0.068007\n",
      "Checkpoint saved for fold 1, session 1, epoch 1\n",
      "Session 1 Epoch 2 - Train Loss: 0.063720\n",
      "Checkpoint saved for fold 1, session 1, epoch 2\n",
      "Session 1 Epoch 3 - Train Loss: 0.061291\n",
      "Checkpoint saved for fold 1, session 1, epoch 3\n",
      "Session 1 Epoch 4 - Train Loss: 0.051371\n",
      "Checkpoint saved for fold 1, session 1, epoch 4\n",
      "Session 1 Epoch 5 - Train Loss: 0.043836\n",
      "Checkpoint saved for fold 1, session 1, epoch 5\n",
      "Session 1 Epoch 6 - Train Loss: 0.040140\n",
      "Checkpoint saved for fold 1, session 1, epoch 6\n",
      "Session 1 Epoch 7 - Train Loss: 0.035899\n",
      "Checkpoint saved for fold 1, session 1, epoch 7\n",
      "Session 1 Epoch 8 - Train Loss: 0.032572\n",
      "Checkpoint saved for fold 1, session 1, epoch 8\n",
      "Session 1 Epoch 9 - Train Loss: 0.030534\n",
      "Checkpoint saved for fold 1, session 1, epoch 9\n",
      "Session 1 Epoch 10 - Train Loss: 0.028036\n",
      "Checkpoint saved for fold 1, session 1, epoch 10\n",
      "Session 1 Epoch 11 - Train Loss: 0.026126\n",
      "Checkpoint saved for fold 1, session 1, epoch 11\n",
      "Session 1 Epoch 12 - Train Loss: 0.024510\n",
      "Checkpoint saved for fold 1, session 1, epoch 12\n",
      "Session 1 Epoch 13 - Train Loss: 0.022814\n",
      "Checkpoint saved for fold 1, session 1, epoch 13\n",
      "Session 1 Epoch 14 - Train Loss: 0.021885\n",
      "Checkpoint saved for fold 1, session 1, epoch 14\n",
      "Session 1 Epoch 15 - Train Loss: 0.020958\n",
      "Checkpoint saved for fold 1, session 1, epoch 15\n",
      "Session 1 Epoch 16 - Train Loss: 0.020299\n",
      "Checkpoint saved for fold 1, session 1, epoch 16\n",
      "Session 1 Epoch 17 - Train Loss: 0.019181\n",
      "Checkpoint saved for fold 1, session 1, epoch 17\n",
      "Session 1 Epoch 18 - Train Loss: 0.018989\n",
      "Checkpoint saved for fold 1, session 1, epoch 18\n",
      "Session 1 Epoch 19 - Train Loss: 0.017915\n",
      "Checkpoint saved for fold 1, session 1, epoch 19\n",
      "Session 1 Epoch 20 - Train Loss: 0.017256\n",
      "Checkpoint saved for fold 1, session 1, epoch 20\n",
      "Session 1 Epoch 21 - Train Loss: 0.016776\n",
      "Checkpoint saved for fold 1, session 1, epoch 21\n",
      "Session 1 Epoch 22 - Train Loss: 0.016504\n",
      "Checkpoint saved for fold 1, session 1, epoch 22\n",
      "Session 1 Epoch 23 - Train Loss: 0.016456\n",
      "Session 1 Epoch 24 - Train Loss: 0.015720\n",
      "Checkpoint saved for fold 1, session 1, epoch 24\n",
      "Session 1 Epoch 25 - Train Loss: 0.015251\n",
      "Checkpoint saved for fold 1, session 1, epoch 25\n",
      "Session 1 Epoch 26 - Train Loss: 0.014935\n",
      "Checkpoint saved for fold 1, session 1, epoch 26\n",
      "Session 1 Epoch 27 - Train Loss: 0.014763\n",
      "Checkpoint saved for fold 1, session 1, epoch 27\n",
      "Session 1 Epoch 28 - Train Loss: 0.014415\n",
      "Checkpoint saved for fold 1, session 1, epoch 28\n",
      "Session 1 Epoch 29 - Train Loss: 0.014236\n",
      "Checkpoint saved for fold 1, session 1, epoch 29\n",
      "Session 1 Epoch 30 - Train Loss: 0.014431\n",
      "Session 1 Epoch 31 - Train Loss: 0.014148\n",
      "Session 1 Epoch 32 - Train Loss: 0.013904\n",
      "Checkpoint saved for fold 1, session 1, epoch 32\n",
      "Session 1 Epoch 33 - Train Loss: 0.013824\n",
      "Session 1 Epoch 34 - Train Loss: 0.013765\n",
      "Checkpoint saved for fold 1, session 1, epoch 34\n",
      "Session 1 Epoch 35 - Train Loss: 0.013435\n",
      "Checkpoint saved for fold 1, session 1, epoch 35\n",
      "Session 1 Epoch 36 - Train Loss: 0.013552\n",
      "Session 1 Epoch 37 - Train Loss: 0.013676\n",
      "Session 1 Epoch 38 - Train Loss: 0.013702\n",
      "Session 1 Epoch 39 - Train Loss: 0.013416\n",
      "Session 1 Epoch 40 - Train Loss: 0.013672\n",
      "Session 1 Epoch 41 - Train Loss: 0.013331\n",
      "Checkpoint saved for fold 1, session 1, epoch 41\n",
      "Session 1 Epoch 42 - Train Loss: 0.013435\n",
      "Session 1 Epoch 43 - Train Loss: 0.013313\n",
      "Session 1 Epoch 44 - Train Loss: 0.013263\n",
      "Session 1 Epoch 45 - Train Loss: 0.013282\n",
      "Session 1 Epoch 46 - Train Loss: 0.013398\n",
      "Session 1 Epoch 47 - Train Loss: 0.013352\n",
      "Session 1 Epoch 48 - Train Loss: 0.013028\n",
      "Checkpoint saved for fold 1, session 1, epoch 48\n",
      "Session 1 Epoch 49 - Train Loss: 0.013345\n",
      "Session 1 Epoch 50 - Train Loss: 0.013524\n",
      "Session 1 Epoch 51 - Train Loss: 0.013235\n",
      "Session 1 Epoch 52 - Train Loss: 0.013286\n",
      "Session 1 Epoch 53 - Train Loss: 0.013208\n",
      "Session 1 Epoch 54 - Train Loss: 0.013240\n",
      "Session 1 Epoch 55 - Train Loss: 0.013091\n",
      "Session 1 Epoch 56 - Train Loss: 0.013163\n",
      "Session 1 Epoch 57 - Train Loss: 0.013135\n",
      "Session 1 Epoch 58 - Train Loss: 0.013250\n",
      "Early stopping at epoch 58 for session 1\n",
      "Training on Session 3/9\n",
      "Session 2 Epoch 1 - Train Loss: 0.559701\n",
      "Checkpoint saved for fold 1, session 2, epoch 1\n",
      "Session 2 Epoch 2 - Train Loss: 0.532032\n",
      "Checkpoint saved for fold 1, session 2, epoch 2\n",
      "Session 2 Epoch 3 - Train Loss: 0.504357\n",
      "Checkpoint saved for fold 1, session 2, epoch 3\n",
      "Session 2 Epoch 4 - Train Loss: 0.480755\n",
      "Checkpoint saved for fold 1, session 2, epoch 4\n",
      "Session 2 Epoch 5 - Train Loss: 0.454763\n",
      "Checkpoint saved for fold 1, session 2, epoch 5\n",
      "Session 2 Epoch 6 - Train Loss: 0.425939\n",
      "Checkpoint saved for fold 1, session 2, epoch 6\n",
      "Session 2 Epoch 7 - Train Loss: 0.395188\n",
      "Checkpoint saved for fold 1, session 2, epoch 7\n",
      "Session 2 Epoch 8 - Train Loss: 0.361504\n",
      "Checkpoint saved for fold 1, session 2, epoch 8\n",
      "Session 2 Epoch 9 - Train Loss: 0.336311\n",
      "Checkpoint saved for fold 1, session 2, epoch 9\n",
      "Session 2 Epoch 10 - Train Loss: 0.320971\n",
      "Checkpoint saved for fold 1, session 2, epoch 10\n",
      "Session 2 Epoch 11 - Train Loss: 0.303628\n",
      "Checkpoint saved for fold 1, session 2, epoch 11\n",
      "Session 2 Epoch 12 - Train Loss: 0.287447\n",
      "Checkpoint saved for fold 1, session 2, epoch 12\n",
      "Session 2 Epoch 13 - Train Loss: 0.272100\n",
      "Checkpoint saved for fold 1, session 2, epoch 13\n",
      "Session 2 Epoch 14 - Train Loss: 0.257770\n",
      "Checkpoint saved for fold 1, session 2, epoch 14\n",
      "Session 2 Epoch 15 - Train Loss: 0.246731\n",
      "Checkpoint saved for fold 1, session 2, epoch 15\n",
      "Session 2 Epoch 16 - Train Loss: 0.239904\n",
      "Checkpoint saved for fold 1, session 2, epoch 16\n",
      "Session 2 Epoch 17 - Train Loss: 0.232505\n",
      "Checkpoint saved for fold 1, session 2, epoch 17\n",
      "Session 2 Epoch 18 - Train Loss: 0.226359\n",
      "Checkpoint saved for fold 1, session 2, epoch 18\n",
      "Session 2 Epoch 19 - Train Loss: 0.219408\n",
      "Checkpoint saved for fold 1, session 2, epoch 19\n",
      "Session 2 Epoch 20 - Train Loss: 0.213430\n",
      "Checkpoint saved for fold 1, session 2, epoch 20\n",
      "Session 2 Epoch 21 - Train Loss: 0.209363\n",
      "Checkpoint saved for fold 1, session 2, epoch 21\n",
      "Session 2 Epoch 22 - Train Loss: 0.206312\n",
      "Checkpoint saved for fold 1, session 2, epoch 22\n",
      "Session 2 Epoch 23 - Train Loss: 0.203023\n",
      "Checkpoint saved for fold 1, session 2, epoch 23\n",
      "Session 2 Epoch 24 - Train Loss: 0.200561\n",
      "Checkpoint saved for fold 1, session 2, epoch 24\n",
      "Session 2 Epoch 25 - Train Loss: 0.198001\n",
      "Checkpoint saved for fold 1, session 2, epoch 25\n",
      "Session 2 Epoch 26 - Train Loss: 0.195355\n",
      "Checkpoint saved for fold 1, session 2, epoch 26\n",
      "Session 2 Epoch 27 - Train Loss: 0.192890\n",
      "Checkpoint saved for fold 1, session 2, epoch 27\n",
      "Session 2 Epoch 28 - Train Loss: 0.191519\n",
      "Checkpoint saved for fold 1, session 2, epoch 28\n",
      "Session 2 Epoch 29 - Train Loss: 0.190262\n",
      "Checkpoint saved for fold 1, session 2, epoch 29\n",
      "Session 2 Epoch 30 - Train Loss: 0.189209\n",
      "Checkpoint saved for fold 1, session 2, epoch 30\n",
      "Session 2 Epoch 31 - Train Loss: 0.188253\n",
      "Checkpoint saved for fold 1, session 2, epoch 31\n",
      "Session 2 Epoch 32 - Train Loss: 0.186569\n",
      "Checkpoint saved for fold 1, session 2, epoch 32\n",
      "Session 2 Epoch 33 - Train Loss: 0.186220\n",
      "Checkpoint saved for fold 1, session 2, epoch 33\n",
      "Session 2 Epoch 34 - Train Loss: 0.186350\n",
      "Session 2 Epoch 35 - Train Loss: 0.185526\n",
      "Checkpoint saved for fold 1, session 2, epoch 35\n",
      "Session 2 Epoch 36 - Train Loss: 0.184376\n",
      "Checkpoint saved for fold 1, session 2, epoch 36\n",
      "Session 2 Epoch 37 - Train Loss: 0.183326\n",
      "Checkpoint saved for fold 1, session 2, epoch 37\n",
      "Session 2 Epoch 38 - Train Loss: 0.182746\n",
      "Checkpoint saved for fold 1, session 2, epoch 38\n",
      "Session 2 Epoch 39 - Train Loss: 0.182601\n",
      "Checkpoint saved for fold 1, session 2, epoch 39\n",
      "Session 2 Epoch 40 - Train Loss: 0.181721\n",
      "Checkpoint saved for fold 1, session 2, epoch 40\n",
      "Session 2 Epoch 41 - Train Loss: 0.182952\n",
      "Session 2 Epoch 42 - Train Loss: 0.181892\n",
      "Session 2 Epoch 43 - Train Loss: 0.181328\n",
      "Checkpoint saved for fold 1, session 2, epoch 43\n",
      "Session 2 Epoch 44 - Train Loss: 0.181119\n",
      "Checkpoint saved for fold 1, session 2, epoch 44\n",
      "Session 2 Epoch 45 - Train Loss: 0.180883\n",
      "Checkpoint saved for fold 1, session 2, epoch 45\n",
      "Session 2 Epoch 46 - Train Loss: 0.181196\n",
      "Session 2 Epoch 47 - Train Loss: 0.180608\n",
      "Checkpoint saved for fold 1, session 2, epoch 47\n",
      "Session 2 Epoch 48 - Train Loss: 0.180226\n",
      "Checkpoint saved for fold 1, session 2, epoch 48\n",
      "Session 2 Epoch 49 - Train Loss: 0.180298\n",
      "Session 2 Epoch 50 - Train Loss: 0.180332\n",
      "Session 2 Epoch 51 - Train Loss: 0.180172\n",
      "Session 2 Epoch 52 - Train Loss: 0.180337\n",
      "Session 2 Epoch 53 - Train Loss: 0.179543\n",
      "Checkpoint saved for fold 1, session 2, epoch 53\n",
      "Session 2 Epoch 54 - Train Loss: 0.179402\n",
      "Checkpoint saved for fold 1, session 2, epoch 54\n",
      "Session 2 Epoch 55 - Train Loss: 0.179371\n",
      "Session 2 Epoch 56 - Train Loss: 0.179252\n",
      "Checkpoint saved for fold 1, session 2, epoch 56\n",
      "Session 2 Epoch 57 - Train Loss: 0.179045\n",
      "Checkpoint saved for fold 1, session 2, epoch 57\n",
      "Session 2 Epoch 58 - Train Loss: 0.177792\n",
      "Checkpoint saved for fold 1, session 2, epoch 58\n",
      "Session 2 Epoch 59 - Train Loss: 0.178896\n",
      "Session 2 Epoch 60 - Train Loss: 0.177822\n",
      "Session 2 Epoch 61 - Train Loss: 0.177537\n",
      "Checkpoint saved for fold 1, session 2, epoch 61\n",
      "Session 2 Epoch 62 - Train Loss: 0.177876\n",
      "Session 2 Epoch 63 - Train Loss: 0.177718\n",
      "Session 2 Epoch 64 - Train Loss: 0.177148\n",
      "Checkpoint saved for fold 1, session 2, epoch 64\n",
      "Session 2 Epoch 65 - Train Loss: 0.176969\n",
      "Checkpoint saved for fold 1, session 2, epoch 65\n",
      "Session 2 Epoch 66 - Train Loss: 0.177519\n",
      "Session 2 Epoch 67 - Train Loss: 0.175695\n",
      "Checkpoint saved for fold 1, session 2, epoch 67\n",
      "Session 2 Epoch 68 - Train Loss: 0.176579\n",
      "Session 2 Epoch 69 - Train Loss: 0.176118\n",
      "Session 2 Epoch 70 - Train Loss: 0.176343\n",
      "Session 2 Epoch 71 - Train Loss: 0.175762\n",
      "Session 2 Epoch 72 - Train Loss: 0.175942\n",
      "Session 2 Epoch 73 - Train Loss: 0.175879\n",
      "Session 2 Epoch 74 - Train Loss: 0.175647\n",
      "Session 2 Epoch 75 - Train Loss: 0.174318\n",
      "Checkpoint saved for fold 1, session 2, epoch 75\n",
      "Session 2 Epoch 76 - Train Loss: 0.174574\n",
      "Session 2 Epoch 77 - Train Loss: 0.173931\n",
      "Checkpoint saved for fold 1, session 2, epoch 77\n",
      "Session 2 Epoch 78 - Train Loss: 0.174982\n",
      "Session 2 Epoch 79 - Train Loss: 0.173858\n",
      "Session 2 Epoch 80 - Train Loss: 0.173916\n",
      "Training on Session 4/9\n",
      "Session 3 Epoch 1 - Train Loss: 0.197266\n",
      "Checkpoint saved for fold 1, session 3, epoch 1\n",
      "Session 3 Epoch 2 - Train Loss: 0.196566\n",
      "Checkpoint saved for fold 1, session 3, epoch 2\n",
      "Session 3 Epoch 3 - Train Loss: 0.197346\n",
      "Session 3 Epoch 4 - Train Loss: 0.195827\n",
      "Checkpoint saved for fold 1, session 3, epoch 4\n",
      "Session 3 Epoch 5 - Train Loss: 0.194733\n",
      "Checkpoint saved for fold 1, session 3, epoch 5\n",
      "Session 3 Epoch 6 - Train Loss: 0.194378\n",
      "Checkpoint saved for fold 1, session 3, epoch 6\n",
      "Session 3 Epoch 7 - Train Loss: 0.193504\n",
      "Checkpoint saved for fold 1, session 3, epoch 7\n",
      "Session 3 Epoch 8 - Train Loss: 0.192709\n",
      "Checkpoint saved for fold 1, session 3, epoch 8\n",
      "Session 3 Epoch 9 - Train Loss: 0.192692\n",
      "Session 3 Epoch 10 - Train Loss: 0.192126\n",
      "Checkpoint saved for fold 1, session 3, epoch 10\n",
      "Session 3 Epoch 11 - Train Loss: 0.192183\n",
      "Session 3 Epoch 12 - Train Loss: 0.191202\n",
      "Checkpoint saved for fold 1, session 3, epoch 12\n",
      "Session 3 Epoch 13 - Train Loss: 0.191499\n",
      "Session 3 Epoch 14 - Train Loss: 0.190216\n",
      "Checkpoint saved for fold 1, session 3, epoch 14\n",
      "Session 3 Epoch 15 - Train Loss: 0.189407\n",
      "Checkpoint saved for fold 1, session 3, epoch 15\n",
      "Session 3 Epoch 16 - Train Loss: 0.189266\n",
      "Checkpoint saved for fold 1, session 3, epoch 16\n",
      "Session 3 Epoch 17 - Train Loss: 0.188372\n",
      "Checkpoint saved for fold 1, session 3, epoch 17\n",
      "Session 3 Epoch 18 - Train Loss: 0.187438\n",
      "Checkpoint saved for fold 1, session 3, epoch 18\n",
      "Session 3 Epoch 19 - Train Loss: 0.187308\n",
      "Checkpoint saved for fold 1, session 3, epoch 19\n",
      "Session 3 Epoch 20 - Train Loss: 0.186599\n",
      "Checkpoint saved for fold 1, session 3, epoch 20\n",
      "Session 3 Epoch 21 - Train Loss: 0.185863\n",
      "Checkpoint saved for fold 1, session 3, epoch 21\n",
      "Session 3 Epoch 22 - Train Loss: 0.186352\n",
      "Session 3 Epoch 23 - Train Loss: 0.186027\n",
      "Session 3 Epoch 24 - Train Loss: 0.185333\n",
      "Checkpoint saved for fold 1, session 3, epoch 24\n",
      "Session 3 Epoch 25 - Train Loss: 0.184028\n",
      "Checkpoint saved for fold 1, session 3, epoch 25\n",
      "Session 3 Epoch 26 - Train Loss: 0.183734\n",
      "Checkpoint saved for fold 1, session 3, epoch 26\n",
      "Session 3 Epoch 27 - Train Loss: 0.183468\n",
      "Checkpoint saved for fold 1, session 3, epoch 27\n",
      "Session 3 Epoch 28 - Train Loss: 0.183047\n",
      "Checkpoint saved for fold 1, session 3, epoch 28\n",
      "Session 3 Epoch 29 - Train Loss: 0.181301\n",
      "Checkpoint saved for fold 1, session 3, epoch 29\n",
      "Session 3 Epoch 30 - Train Loss: 0.182373\n",
      "Session 3 Epoch 31 - Train Loss: 0.181647\n",
      "Session 3 Epoch 32 - Train Loss: 0.181002\n",
      "Checkpoint saved for fold 1, session 3, epoch 32\n",
      "Session 3 Epoch 33 - Train Loss: 0.180526\n",
      "Checkpoint saved for fold 1, session 3, epoch 33\n",
      "Session 3 Epoch 34 - Train Loss: 0.180015\n",
      "Checkpoint saved for fold 1, session 3, epoch 34\n",
      "Session 3 Epoch 35 - Train Loss: 0.179347\n",
      "Checkpoint saved for fold 1, session 3, epoch 35\n",
      "Session 3 Epoch 36 - Train Loss: 0.179571\n",
      "Session 3 Epoch 37 - Train Loss: 0.178326\n",
      "Checkpoint saved for fold 1, session 3, epoch 37\n",
      "Session 3 Epoch 38 - Train Loss: 0.178094\n",
      "Checkpoint saved for fold 1, session 3, epoch 38\n",
      "Session 3 Epoch 39 - Train Loss: 0.177565\n",
      "Checkpoint saved for fold 1, session 3, epoch 39\n",
      "Session 3 Epoch 40 - Train Loss: 0.176893\n",
      "Checkpoint saved for fold 1, session 3, epoch 40\n",
      "Session 3 Epoch 41 - Train Loss: 0.176472\n",
      "Checkpoint saved for fold 1, session 3, epoch 41\n",
      "Session 3 Epoch 42 - Train Loss: 0.177195\n",
      "Session 3 Epoch 43 - Train Loss: 0.176063\n",
      "Checkpoint saved for fold 1, session 3, epoch 43\n",
      "Session 3 Epoch 44 - Train Loss: 0.175075\n",
      "Checkpoint saved for fold 1, session 3, epoch 44\n",
      "Session 3 Epoch 45 - Train Loss: 0.175575\n",
      "Session 3 Epoch 46 - Train Loss: 0.174285\n",
      "Checkpoint saved for fold 1, session 3, epoch 46\n",
      "Session 3 Epoch 47 - Train Loss: 0.173546\n",
      "Checkpoint saved for fold 1, session 3, epoch 47\n",
      "Session 3 Epoch 48 - Train Loss: 0.174193\n",
      "Session 3 Epoch 49 - Train Loss: 0.173249\n",
      "Checkpoint saved for fold 1, session 3, epoch 49\n",
      "Session 3 Epoch 50 - Train Loss: 0.173096\n",
      "Checkpoint saved for fold 1, session 3, epoch 50\n",
      "Session 3 Epoch 51 - Train Loss: 0.172522\n",
      "Checkpoint saved for fold 1, session 3, epoch 51\n",
      "Session 3 Epoch 52 - Train Loss: 0.171826\n",
      "Checkpoint saved for fold 1, session 3, epoch 52\n",
      "Session 3 Epoch 53 - Train Loss: 0.171514\n",
      "Checkpoint saved for fold 1, session 3, epoch 53\n",
      "Session 3 Epoch 54 - Train Loss: 0.170481\n",
      "Checkpoint saved for fold 1, session 3, epoch 54\n",
      "Session 3 Epoch 55 - Train Loss: 0.170092\n",
      "Checkpoint saved for fold 1, session 3, epoch 55\n",
      "Session 3 Epoch 56 - Train Loss: 0.170401\n",
      "Session 3 Epoch 57 - Train Loss: 0.169574\n",
      "Checkpoint saved for fold 1, session 3, epoch 57\n",
      "Session 3 Epoch 58 - Train Loss: 0.169171\n",
      "Checkpoint saved for fold 1, session 3, epoch 58\n",
      "Session 3 Epoch 59 - Train Loss: 0.168657\n",
      "Checkpoint saved for fold 1, session 3, epoch 59\n",
      "Session 3 Epoch 60 - Train Loss: 0.167521\n",
      "Checkpoint saved for fold 1, session 3, epoch 60\n",
      "Session 3 Epoch 61 - Train Loss: 0.167716\n",
      "Session 3 Epoch 62 - Train Loss: 0.167864\n",
      "Session 3 Epoch 63 - Train Loss: 0.167266\n",
      "Checkpoint saved for fold 1, session 3, epoch 63\n",
      "Session 3 Epoch 64 - Train Loss: 0.167245\n",
      "Session 3 Epoch 65 - Train Loss: 0.166429\n",
      "Checkpoint saved for fold 1, session 3, epoch 65\n",
      "Session 3 Epoch 66 - Train Loss: 0.165371\n",
      "Checkpoint saved for fold 1, session 3, epoch 66\n",
      "Session 3 Epoch 67 - Train Loss: 0.165243\n",
      "Checkpoint saved for fold 1, session 3, epoch 67\n",
      "Session 3 Epoch 68 - Train Loss: 0.165545\n",
      "Session 3 Epoch 69 - Train Loss: 0.164777\n",
      "Checkpoint saved for fold 1, session 3, epoch 69\n",
      "Session 3 Epoch 70 - Train Loss: 0.164672\n",
      "Checkpoint saved for fold 1, session 3, epoch 70\n",
      "Session 3 Epoch 71 - Train Loss: 0.164214\n",
      "Checkpoint saved for fold 1, session 3, epoch 71\n",
      "Session 3 Epoch 72 - Train Loss: 0.163891\n",
      "Checkpoint saved for fold 1, session 3, epoch 72\n",
      "Session 3 Epoch 73 - Train Loss: 0.163788\n",
      "Checkpoint saved for fold 1, session 3, epoch 73\n",
      "Session 3 Epoch 74 - Train Loss: 0.163267\n",
      "Checkpoint saved for fold 1, session 3, epoch 74\n",
      "Session 3 Epoch 75 - Train Loss: 0.162796\n",
      "Checkpoint saved for fold 1, session 3, epoch 75\n",
      "Session 3 Epoch 76 - Train Loss: 0.161526\n",
      "Checkpoint saved for fold 1, session 3, epoch 76\n",
      "Session 3 Epoch 77 - Train Loss: 0.162025\n",
      "Session 3 Epoch 78 - Train Loss: 0.161511\n",
      "Session 3 Epoch 79 - Train Loss: 0.160880\n",
      "Checkpoint saved for fold 1, session 3, epoch 79\n",
      "Session 3 Epoch 80 - Train Loss: 0.161015\n",
      "Training on Session 5/9\n",
      "Session 4 Epoch 1 - Train Loss: 0.311763\n",
      "Checkpoint saved for fold 1, session 4, epoch 1\n",
      "Session 4 Epoch 2 - Train Loss: 0.311921\n",
      "Session 4 Epoch 3 - Train Loss: 0.311350\n",
      "Checkpoint saved for fold 1, session 4, epoch 3\n",
      "Session 4 Epoch 4 - Train Loss: 0.310391\n",
      "Checkpoint saved for fold 1, session 4, epoch 4\n",
      "Session 4 Epoch 5 - Train Loss: 0.309353\n",
      "Checkpoint saved for fold 1, session 4, epoch 5\n",
      "Session 4 Epoch 6 - Train Loss: 0.309942\n",
      "Session 4 Epoch 7 - Train Loss: 0.308880\n",
      "Checkpoint saved for fold 1, session 4, epoch 7\n",
      "Session 4 Epoch 8 - Train Loss: 0.309118\n",
      "Session 4 Epoch 9 - Train Loss: 0.308744\n",
      "Checkpoint saved for fold 1, session 4, epoch 9\n",
      "Session 4 Epoch 10 - Train Loss: 0.307426\n",
      "Checkpoint saved for fold 1, session 4, epoch 10\n",
      "Session 4 Epoch 11 - Train Loss: 0.307728\n",
      "Session 4 Epoch 12 - Train Loss: 0.306600\n",
      "Checkpoint saved for fold 1, session 4, epoch 12\n",
      "Session 4 Epoch 13 - Train Loss: 0.305589\n",
      "Checkpoint saved for fold 1, session 4, epoch 13\n",
      "Session 4 Epoch 14 - Train Loss: 0.305837\n",
      "Session 4 Epoch 15 - Train Loss: 0.305222\n",
      "Checkpoint saved for fold 1, session 4, epoch 15\n",
      "Session 4 Epoch 16 - Train Loss: 0.305014\n",
      "Checkpoint saved for fold 1, session 4, epoch 16\n",
      "Session 4 Epoch 17 - Train Loss: 0.304723\n",
      "Checkpoint saved for fold 1, session 4, epoch 17\n",
      "Session 4 Epoch 18 - Train Loss: 0.303675\n",
      "Checkpoint saved for fold 1, session 4, epoch 18\n",
      "Session 4 Epoch 19 - Train Loss: 0.303128\n",
      "Checkpoint saved for fold 1, session 4, epoch 19\n",
      "Session 4 Epoch 20 - Train Loss: 0.303337\n",
      "Session 4 Epoch 21 - Train Loss: 0.302179\n",
      "Checkpoint saved for fold 1, session 4, epoch 21\n",
      "Session 4 Epoch 22 - Train Loss: 0.302051\n",
      "Checkpoint saved for fold 1, session 4, epoch 22\n",
      "Session 4 Epoch 23 - Train Loss: 0.301574\n",
      "Checkpoint saved for fold 1, session 4, epoch 23\n",
      "Session 4 Epoch 24 - Train Loss: 0.300898\n",
      "Checkpoint saved for fold 1, session 4, epoch 24\n",
      "Session 4 Epoch 25 - Train Loss: 0.300962\n",
      "Session 4 Epoch 26 - Train Loss: 0.299923\n",
      "Checkpoint saved for fold 1, session 4, epoch 26\n",
      "Session 4 Epoch 27 - Train Loss: 0.299425\n",
      "Checkpoint saved for fold 1, session 4, epoch 27\n",
      "Session 4 Epoch 28 - Train Loss: 0.299303\n",
      "Checkpoint saved for fold 1, session 4, epoch 28\n",
      "Session 4 Epoch 29 - Train Loss: 0.298338\n",
      "Checkpoint saved for fold 1, session 4, epoch 29\n",
      "Session 4 Epoch 30 - Train Loss: 0.297665\n",
      "Checkpoint saved for fold 1, session 4, epoch 30\n",
      "Session 4 Epoch 31 - Train Loss: 0.296999\n",
      "Checkpoint saved for fold 1, session 4, epoch 31\n",
      "Session 4 Epoch 32 - Train Loss: 0.296392\n",
      "Checkpoint saved for fold 1, session 4, epoch 32\n",
      "Session 4 Epoch 33 - Train Loss: 0.296552\n",
      "Session 4 Epoch 34 - Train Loss: 0.295938\n",
      "Checkpoint saved for fold 1, session 4, epoch 34\n",
      "Session 4 Epoch 35 - Train Loss: 0.295727\n",
      "Checkpoint saved for fold 1, session 4, epoch 35\n",
      "Session 4 Epoch 36 - Train Loss: 0.295145\n",
      "Checkpoint saved for fold 1, session 4, epoch 36\n",
      "Session 4 Epoch 37 - Train Loss: 0.294966\n",
      "Checkpoint saved for fold 1, session 4, epoch 37\n",
      "Session 4 Epoch 38 - Train Loss: 0.294526\n",
      "Checkpoint saved for fold 1, session 4, epoch 38\n",
      "Session 4 Epoch 39 - Train Loss: 0.293223\n",
      "Checkpoint saved for fold 1, session 4, epoch 39\n",
      "Session 4 Epoch 40 - Train Loss: 0.293168\n",
      "Session 4 Epoch 41 - Train Loss: 0.292924\n",
      "Checkpoint saved for fold 1, session 4, epoch 41\n",
      "Session 4 Epoch 42 - Train Loss: 0.291783\n",
      "Checkpoint saved for fold 1, session 4, epoch 42\n",
      "Session 4 Epoch 43 - Train Loss: 0.291653\n",
      "Checkpoint saved for fold 1, session 4, epoch 43\n",
      "Session 4 Epoch 44 - Train Loss: 0.290866\n",
      "Checkpoint saved for fold 1, session 4, epoch 44\n",
      "Session 4 Epoch 45 - Train Loss: 0.290850\n",
      "Session 4 Epoch 46 - Train Loss: 0.290647\n",
      "Checkpoint saved for fold 1, session 4, epoch 46\n",
      "Session 4 Epoch 47 - Train Loss: 0.290475\n",
      "Checkpoint saved for fold 1, session 4, epoch 47\n",
      "Session 4 Epoch 48 - Train Loss: 0.289113\n",
      "Checkpoint saved for fold 1, session 4, epoch 48\n",
      "Session 4 Epoch 49 - Train Loss: 0.288407\n",
      "Checkpoint saved for fold 1, session 4, epoch 49\n",
      "Session 4 Epoch 50 - Train Loss: 0.288404\n",
      "Session 4 Epoch 51 - Train Loss: 0.287956\n",
      "Checkpoint saved for fold 1, session 4, epoch 51\n",
      "Session 4 Epoch 52 - Train Loss: 0.287563\n",
      "Checkpoint saved for fold 1, session 4, epoch 52\n",
      "Session 4 Epoch 53 - Train Loss: 0.287390\n",
      "Checkpoint saved for fold 1, session 4, epoch 53\n",
      "Session 4 Epoch 54 - Train Loss: 0.286518\n",
      "Checkpoint saved for fold 1, session 4, epoch 54\n",
      "Session 4 Epoch 55 - Train Loss: 0.286364\n",
      "Checkpoint saved for fold 1, session 4, epoch 55\n",
      "Session 4 Epoch 56 - Train Loss: 0.285491\n",
      "Checkpoint saved for fold 1, session 4, epoch 56\n",
      "Session 4 Epoch 57 - Train Loss: 0.285601\n",
      "Session 4 Epoch 58 - Train Loss: 0.284924\n",
      "Checkpoint saved for fold 1, session 4, epoch 58\n",
      "Session 4 Epoch 59 - Train Loss: 0.283890\n",
      "Checkpoint saved for fold 1, session 4, epoch 59\n",
      "Session 4 Epoch 60 - Train Loss: 0.284059\n",
      "Session 4 Epoch 61 - Train Loss: 0.282878\n",
      "Checkpoint saved for fold 1, session 4, epoch 61\n",
      "Session 4 Epoch 62 - Train Loss: 0.283328\n",
      "Session 4 Epoch 63 - Train Loss: 0.282679\n",
      "Checkpoint saved for fold 1, session 4, epoch 63\n",
      "Session 4 Epoch 64 - Train Loss: 0.281545\n",
      "Checkpoint saved for fold 1, session 4, epoch 64\n",
      "Session 4 Epoch 65 - Train Loss: 0.280978\n",
      "Checkpoint saved for fold 1, session 4, epoch 65\n",
      "Session 4 Epoch 66 - Train Loss: 0.281091\n",
      "Session 4 Epoch 67 - Train Loss: 0.280810\n",
      "Checkpoint saved for fold 1, session 4, epoch 67\n",
      "Session 4 Epoch 68 - Train Loss: 0.280726\n",
      "Session 4 Epoch 69 - Train Loss: 0.280285\n",
      "Checkpoint saved for fold 1, session 4, epoch 69\n",
      "Session 4 Epoch 70 - Train Loss: 0.279986\n",
      "Checkpoint saved for fold 1, session 4, epoch 70\n",
      "Session 4 Epoch 71 - Train Loss: 0.278899\n",
      "Checkpoint saved for fold 1, session 4, epoch 71\n",
      "Session 4 Epoch 72 - Train Loss: 0.278265\n",
      "Checkpoint saved for fold 1, session 4, epoch 72\n",
      "Session 4 Epoch 73 - Train Loss: 0.278408\n",
      "Session 4 Epoch 74 - Train Loss: 0.277550\n",
      "Checkpoint saved for fold 1, session 4, epoch 74\n",
      "Session 4 Epoch 75 - Train Loss: 0.277360\n",
      "Checkpoint saved for fold 1, session 4, epoch 75\n",
      "Session 4 Epoch 76 - Train Loss: 0.277101\n",
      "Checkpoint saved for fold 1, session 4, epoch 76\n",
      "Session 4 Epoch 77 - Train Loss: 0.276531\n",
      "Checkpoint saved for fold 1, session 4, epoch 77\n",
      "Session 4 Epoch 78 - Train Loss: 0.276306\n",
      "Checkpoint saved for fold 1, session 4, epoch 78\n",
      "Session 4 Epoch 79 - Train Loss: 0.275286\n",
      "Checkpoint saved for fold 1, session 4, epoch 79\n",
      "Session 4 Epoch 80 - Train Loss: 0.275277\n",
      "Training on Session 6/9\n",
      "Session 5 Epoch 1 - Train Loss: 0.166769\n",
      "Checkpoint saved for fold 1, session 5, epoch 1\n",
      "Session 5 Epoch 2 - Train Loss: 0.167137\n",
      "Session 5 Epoch 3 - Train Loss: 0.166750\n",
      "Session 5 Epoch 4 - Train Loss: 0.166249\n",
      "Checkpoint saved for fold 1, session 5, epoch 4\n",
      "Session 5 Epoch 5 - Train Loss: 0.166214\n",
      "Session 5 Epoch 6 - Train Loss: 0.166572\n",
      "Session 5 Epoch 7 - Train Loss: 0.166034\n",
      "Checkpoint saved for fold 1, session 5, epoch 7\n",
      "Session 5 Epoch 8 - Train Loss: 0.165300\n",
      "Checkpoint saved for fold 1, session 5, epoch 8\n",
      "Session 5 Epoch 9 - Train Loss: 0.165406\n",
      "Session 5 Epoch 10 - Train Loss: 0.164682\n",
      "Checkpoint saved for fold 1, session 5, epoch 10\n",
      "Session 5 Epoch 11 - Train Loss: 0.164753\n",
      "Session 5 Epoch 12 - Train Loss: 0.164620\n",
      "Session 5 Epoch 13 - Train Loss: 0.164856\n",
      "Session 5 Epoch 14 - Train Loss: 0.164492\n",
      "Checkpoint saved for fold 1, session 5, epoch 14\n",
      "Session 5 Epoch 15 - Train Loss: 0.164220\n",
      "Checkpoint saved for fold 1, session 5, epoch 15\n",
      "Session 5 Epoch 16 - Train Loss: 0.163720\n",
      "Checkpoint saved for fold 1, session 5, epoch 16\n",
      "Session 5 Epoch 17 - Train Loss: 0.163683\n",
      "Session 5 Epoch 18 - Train Loss: 0.163626\n",
      "Session 5 Epoch 19 - Train Loss: 0.163581\n",
      "Checkpoint saved for fold 1, session 5, epoch 19\n",
      "Session 5 Epoch 20 - Train Loss: 0.162725\n",
      "Checkpoint saved for fold 1, session 5, epoch 20\n",
      "Session 5 Epoch 21 - Train Loss: 0.162757\n",
      "Session 5 Epoch 22 - Train Loss: 0.162109\n",
      "Checkpoint saved for fold 1, session 5, epoch 22\n",
      "Session 5 Epoch 23 - Train Loss: 0.162401\n",
      "Session 5 Epoch 24 - Train Loss: 0.162602\n",
      "Session 5 Epoch 25 - Train Loss: 0.162022\n",
      "Session 5 Epoch 26 - Train Loss: 0.161976\n",
      "Checkpoint saved for fold 1, session 5, epoch 26\n",
      "Session 5 Epoch 27 - Train Loss: 0.161721\n",
      "Checkpoint saved for fold 1, session 5, epoch 27\n",
      "Session 5 Epoch 28 - Train Loss: 0.161460\n",
      "Checkpoint saved for fold 1, session 5, epoch 28\n",
      "Session 5 Epoch 29 - Train Loss: 0.161575\n",
      "Session 5 Epoch 30 - Train Loss: 0.161827\n",
      "Session 5 Epoch 31 - Train Loss: 0.161236\n",
      "Checkpoint saved for fold 1, session 5, epoch 31\n",
      "Session 5 Epoch 32 - Train Loss: 0.160981\n",
      "Checkpoint saved for fold 1, session 5, epoch 32\n",
      "Session 5 Epoch 33 - Train Loss: 0.160723\n",
      "Checkpoint saved for fold 1, session 5, epoch 33\n",
      "Session 5 Epoch 34 - Train Loss: 0.160616\n",
      "Checkpoint saved for fold 1, session 5, epoch 34\n",
      "Session 5 Epoch 35 - Train Loss: 0.160416\n",
      "Checkpoint saved for fold 1, session 5, epoch 35\n",
      "Session 5 Epoch 36 - Train Loss: 0.160662\n",
      "Session 5 Epoch 37 - Train Loss: 0.160704\n",
      "Session 5 Epoch 38 - Train Loss: 0.159356\n",
      "Checkpoint saved for fold 1, session 5, epoch 38\n",
      "Session 5 Epoch 39 - Train Loss: 0.159728\n",
      "Session 5 Epoch 40 - Train Loss: 0.158736\n",
      "Checkpoint saved for fold 1, session 5, epoch 40\n",
      "Session 5 Epoch 41 - Train Loss: 0.159473\n",
      "Session 5 Epoch 42 - Train Loss: 0.159341\n",
      "Session 5 Epoch 43 - Train Loss: 0.159150\n",
      "Session 5 Epoch 44 - Train Loss: 0.158402\n",
      "Checkpoint saved for fold 1, session 5, epoch 44\n",
      "Session 5 Epoch 45 - Train Loss: 0.157885\n",
      "Checkpoint saved for fold 1, session 5, epoch 45\n",
      "Session 5 Epoch 46 - Train Loss: 0.158372\n",
      "Session 5 Epoch 47 - Train Loss: 0.158280\n",
      "Session 5 Epoch 48 - Train Loss: 0.158066\n",
      "Session 5 Epoch 49 - Train Loss: 0.158382\n",
      "Session 5 Epoch 50 - Train Loss: 0.157225\n",
      "Checkpoint saved for fold 1, session 5, epoch 50\n",
      "Session 5 Epoch 51 - Train Loss: 0.157674\n",
      "Session 5 Epoch 52 - Train Loss: 0.157261\n",
      "Session 5 Epoch 53 - Train Loss: 0.157959\n",
      "Session 5 Epoch 54 - Train Loss: 0.156627\n",
      "Checkpoint saved for fold 1, session 5, epoch 54\n",
      "Session 5 Epoch 55 - Train Loss: 0.156987\n",
      "Session 5 Epoch 56 - Train Loss: 0.156406\n",
      "Checkpoint saved for fold 1, session 5, epoch 56\n",
      "Session 5 Epoch 57 - Train Loss: 0.157219\n",
      "Session 5 Epoch 58 - Train Loss: 0.156569\n",
      "Session 5 Epoch 59 - Train Loss: 0.156184\n",
      "Checkpoint saved for fold 1, session 5, epoch 59\n",
      "Session 5 Epoch 60 - Train Loss: 0.156068\n",
      "Checkpoint saved for fold 1, session 5, epoch 60\n",
      "Session 5 Epoch 61 - Train Loss: 0.155649\n",
      "Checkpoint saved for fold 1, session 5, epoch 61\n",
      "Session 5 Epoch 62 - Train Loss: 0.155528\n",
      "Checkpoint saved for fold 1, session 5, epoch 62\n",
      "Session 5 Epoch 63 - Train Loss: 0.155678\n",
      "Session 5 Epoch 64 - Train Loss: 0.155683\n",
      "Session 5 Epoch 65 - Train Loss: 0.155067\n",
      "Checkpoint saved for fold 1, session 5, epoch 65\n",
      "Session 5 Epoch 66 - Train Loss: 0.155560\n",
      "Session 5 Epoch 67 - Train Loss: 0.154601\n",
      "Checkpoint saved for fold 1, session 5, epoch 67\n",
      "Session 5 Epoch 68 - Train Loss: 0.154962\n",
      "Session 5 Epoch 69 - Train Loss: 0.154803\n",
      "Session 5 Epoch 70 - Train Loss: 0.154242\n",
      "Checkpoint saved for fold 1, session 5, epoch 70\n",
      "Session 5 Epoch 71 - Train Loss: 0.154425\n",
      "Session 5 Epoch 72 - Train Loss: 0.154059\n",
      "Checkpoint saved for fold 1, session 5, epoch 72\n",
      "Session 5 Epoch 73 - Train Loss: 0.153769\n",
      "Checkpoint saved for fold 1, session 5, epoch 73\n",
      "Session 5 Epoch 74 - Train Loss: 0.153246\n",
      "Checkpoint saved for fold 1, session 5, epoch 74\n",
      "Session 5 Epoch 75 - Train Loss: 0.154078\n",
      "Session 5 Epoch 76 - Train Loss: 0.153055\n",
      "Checkpoint saved for fold 1, session 5, epoch 76\n",
      "Session 5 Epoch 77 - Train Loss: 0.152782\n",
      "Checkpoint saved for fold 1, session 5, epoch 77\n",
      "Session 5 Epoch 78 - Train Loss: 0.153411\n",
      "Session 5 Epoch 79 - Train Loss: 0.152970\n",
      "Session 5 Epoch 80 - Train Loss: 0.153000\n",
      "Training on Session 7/9\n",
      "Session 6 Epoch 1 - Train Loss: 0.178366\n",
      "Checkpoint saved for fold 1, session 6, epoch 1\n",
      "Session 6 Epoch 2 - Train Loss: 0.178049\n",
      "Checkpoint saved for fold 1, session 6, epoch 2\n",
      "Session 6 Epoch 3 - Train Loss: 0.177810\n",
      "Checkpoint saved for fold 1, session 6, epoch 3\n",
      "Session 6 Epoch 4 - Train Loss: 0.176619\n",
      "Checkpoint saved for fold 1, session 6, epoch 4\n",
      "Session 6 Epoch 5 - Train Loss: 0.175612\n",
      "Checkpoint saved for fold 1, session 6, epoch 5\n",
      "Session 6 Epoch 6 - Train Loss: 0.175887\n",
      "Session 6 Epoch 7 - Train Loss: 0.175447\n",
      "Checkpoint saved for fold 1, session 6, epoch 7\n",
      "Session 6 Epoch 8 - Train Loss: 0.175095\n",
      "Checkpoint saved for fold 1, session 6, epoch 8\n",
      "Session 6 Epoch 9 - Train Loss: 0.174217\n",
      "Checkpoint saved for fold 1, session 6, epoch 9\n",
      "Session 6 Epoch 10 - Train Loss: 0.174117\n",
      "Session 6 Epoch 11 - Train Loss: 0.173451\n",
      "Checkpoint saved for fold 1, session 6, epoch 11\n",
      "Session 6 Epoch 12 - Train Loss: 0.173325\n",
      "Checkpoint saved for fold 1, session 6, epoch 12\n",
      "Session 6 Epoch 13 - Train Loss: 0.172465\n",
      "Checkpoint saved for fold 1, session 6, epoch 13\n",
      "Session 6 Epoch 14 - Train Loss: 0.171893\n",
      "Checkpoint saved for fold 1, session 6, epoch 14\n",
      "Session 6 Epoch 15 - Train Loss: 0.171848\n",
      "Session 6 Epoch 16 - Train Loss: 0.171728\n",
      "Checkpoint saved for fold 1, session 6, epoch 16\n",
      "Session 6 Epoch 17 - Train Loss: 0.170773\n",
      "Checkpoint saved for fold 1, session 6, epoch 17\n",
      "Session 6 Epoch 18 - Train Loss: 0.170476\n",
      "Checkpoint saved for fold 1, session 6, epoch 18\n",
      "Session 6 Epoch 19 - Train Loss: 0.169887\n",
      "Checkpoint saved for fold 1, session 6, epoch 19\n",
      "Session 6 Epoch 20 - Train Loss: 0.168948\n",
      "Checkpoint saved for fold 1, session 6, epoch 20\n",
      "Session 6 Epoch 21 - Train Loss: 0.169337\n",
      "Session 6 Epoch 22 - Train Loss: 0.168678\n",
      "Checkpoint saved for fold 1, session 6, epoch 22\n",
      "Session 6 Epoch 23 - Train Loss: 0.168565\n",
      "Checkpoint saved for fold 1, session 6, epoch 23\n",
      "Session 6 Epoch 24 - Train Loss: 0.168217\n",
      "Checkpoint saved for fold 1, session 6, epoch 24\n",
      "Session 6 Epoch 25 - Train Loss: 0.167221\n",
      "Checkpoint saved for fold 1, session 6, epoch 25\n",
      "Session 6 Epoch 26 - Train Loss: 0.167056\n",
      "Checkpoint saved for fold 1, session 6, epoch 26\n",
      "Session 6 Epoch 27 - Train Loss: 0.167038\n",
      "Session 6 Epoch 28 - Train Loss: 0.166298\n",
      "Checkpoint saved for fold 1, session 6, epoch 28\n",
      "Session 6 Epoch 29 - Train Loss: 0.165866\n",
      "Checkpoint saved for fold 1, session 6, epoch 29\n",
      "Session 6 Epoch 30 - Train Loss: 0.165230\n",
      "Checkpoint saved for fold 1, session 6, epoch 30\n",
      "Session 6 Epoch 31 - Train Loss: 0.165045\n",
      "Checkpoint saved for fold 1, session 6, epoch 31\n",
      "Session 6 Epoch 32 - Train Loss: 0.165223\n",
      "Session 6 Epoch 33 - Train Loss: 0.164067\n",
      "Checkpoint saved for fold 1, session 6, epoch 33\n",
      "Session 6 Epoch 34 - Train Loss: 0.163467\n",
      "Checkpoint saved for fold 1, session 6, epoch 34\n",
      "Session 6 Epoch 35 - Train Loss: 0.164223\n",
      "Session 6 Epoch 36 - Train Loss: 0.163463\n",
      "Session 6 Epoch 37 - Train Loss: 0.162519\n",
      "Checkpoint saved for fold 1, session 6, epoch 37\n",
      "Session 6 Epoch 38 - Train Loss: 0.162077\n",
      "Checkpoint saved for fold 1, session 6, epoch 38\n",
      "Session 6 Epoch 39 - Train Loss: 0.162045\n",
      "Session 6 Epoch 40 - Train Loss: 0.161632\n",
      "Checkpoint saved for fold 1, session 6, epoch 40\n",
      "Session 6 Epoch 41 - Train Loss: 0.161327\n",
      "Checkpoint saved for fold 1, session 6, epoch 41\n",
      "Session 6 Epoch 42 - Train Loss: 0.161247\n",
      "Session 6 Epoch 43 - Train Loss: 0.160377\n",
      "Checkpoint saved for fold 1, session 6, epoch 43\n",
      "Session 6 Epoch 44 - Train Loss: 0.160350\n",
      "Session 6 Epoch 45 - Train Loss: 0.159136\n",
      "Checkpoint saved for fold 1, session 6, epoch 45\n",
      "Session 6 Epoch 46 - Train Loss: 0.159061\n",
      "Session 6 Epoch 47 - Train Loss: 0.158730\n",
      "Checkpoint saved for fold 1, session 6, epoch 47\n",
      "Session 6 Epoch 48 - Train Loss: 0.159652\n",
      "Session 6 Epoch 49 - Train Loss: 0.157643\n",
      "Checkpoint saved for fold 1, session 6, epoch 49\n",
      "Session 6 Epoch 50 - Train Loss: 0.157618\n",
      "Session 6 Epoch 51 - Train Loss: 0.158133\n",
      "Session 6 Epoch 52 - Train Loss: 0.157069\n",
      "Checkpoint saved for fold 1, session 6, epoch 52\n",
      "Session 6 Epoch 53 - Train Loss: 0.157117\n",
      "Session 6 Epoch 54 - Train Loss: 0.156269\n",
      "Checkpoint saved for fold 1, session 6, epoch 54\n",
      "Session 6 Epoch 55 - Train Loss: 0.156396\n",
      "Session 6 Epoch 56 - Train Loss: 0.156086\n",
      "Checkpoint saved for fold 1, session 6, epoch 56\n",
      "Session 6 Epoch 57 - Train Loss: 0.155445\n",
      "Checkpoint saved for fold 1, session 6, epoch 57\n",
      "Session 6 Epoch 58 - Train Loss: 0.155023\n",
      "Checkpoint saved for fold 1, session 6, epoch 58\n",
      "Session 6 Epoch 59 - Train Loss: 0.154708\n",
      "Checkpoint saved for fold 1, session 6, epoch 59\n",
      "Session 6 Epoch 60 - Train Loss: 0.154848\n",
      "Session 6 Epoch 61 - Train Loss: 0.153866\n",
      "Checkpoint saved for fold 1, session 6, epoch 61\n",
      "Session 6 Epoch 62 - Train Loss: 0.153935\n",
      "Session 6 Epoch 63 - Train Loss: 0.153277\n",
      "Checkpoint saved for fold 1, session 6, epoch 63\n",
      "Session 6 Epoch 64 - Train Loss: 0.152699\n",
      "Checkpoint saved for fold 1, session 6, epoch 64\n",
      "Session 6 Epoch 65 - Train Loss: 0.152672\n",
      "Session 6 Epoch 66 - Train Loss: 0.152516\n",
      "Checkpoint saved for fold 1, session 6, epoch 66\n",
      "Session 6 Epoch 67 - Train Loss: 0.151907\n",
      "Checkpoint saved for fold 1, session 6, epoch 67\n",
      "Session 6 Epoch 68 - Train Loss: 0.152283\n",
      "Session 6 Epoch 69 - Train Loss: 0.151233\n",
      "Checkpoint saved for fold 1, session 6, epoch 69\n",
      "Session 6 Epoch 70 - Train Loss: 0.152134\n",
      "Session 6 Epoch 71 - Train Loss: 0.151047\n",
      "Checkpoint saved for fold 1, session 6, epoch 71\n",
      "Session 6 Epoch 72 - Train Loss: 0.150708\n",
      "Checkpoint saved for fold 1, session 6, epoch 72\n",
      "Session 6 Epoch 73 - Train Loss: 0.150255\n",
      "Checkpoint saved for fold 1, session 6, epoch 73\n",
      "Session 6 Epoch 74 - Train Loss: 0.150297\n",
      "Session 6 Epoch 75 - Train Loss: 0.149882\n",
      "Checkpoint saved for fold 1, session 6, epoch 75\n",
      "Session 6 Epoch 76 - Train Loss: 0.149830\n",
      "Session 6 Epoch 77 - Train Loss: 0.149520\n",
      "Checkpoint saved for fold 1, session 6, epoch 77\n",
      "Session 6 Epoch 78 - Train Loss: 0.149203\n",
      "Checkpoint saved for fold 1, session 6, epoch 78\n",
      "Session 6 Epoch 79 - Train Loss: 0.148780\n",
      "Checkpoint saved for fold 1, session 6, epoch 79\n",
      "Session 6 Epoch 80 - Train Loss: 0.148727\n",
      "Training on Session 8/9\n",
      "Session 7 Epoch 1 - Train Loss: 0.225825\n",
      "Checkpoint saved for fold 1, session 7, epoch 1\n",
      "Session 7 Epoch 2 - Train Loss: 0.225994\n",
      "Session 7 Epoch 3 - Train Loss: 0.226092\n",
      "Session 7 Epoch 4 - Train Loss: 0.225252\n",
      "Checkpoint saved for fold 1, session 7, epoch 4\n",
      "Session 7 Epoch 5 - Train Loss: 0.225663\n",
      "Session 7 Epoch 6 - Train Loss: 0.225305\n",
      "Session 7 Epoch 7 - Train Loss: 0.224400\n",
      "Checkpoint saved for fold 1, session 7, epoch 7\n",
      "Session 7 Epoch 8 - Train Loss: 0.225190\n",
      "Session 7 Epoch 9 - Train Loss: 0.224335\n",
      "Session 7 Epoch 10 - Train Loss: 0.223911\n",
      "Checkpoint saved for fold 1, session 7, epoch 10\n",
      "Session 7 Epoch 11 - Train Loss: 0.223828\n",
      "Session 7 Epoch 12 - Train Loss: 0.223611\n",
      "Checkpoint saved for fold 1, session 7, epoch 12\n",
      "Session 7 Epoch 13 - Train Loss: 0.223380\n",
      "Checkpoint saved for fold 1, session 7, epoch 13\n",
      "Session 7 Epoch 14 - Train Loss: 0.222891\n",
      "Checkpoint saved for fold 1, session 7, epoch 14\n",
      "Session 7 Epoch 15 - Train Loss: 0.222840\n",
      "Session 7 Epoch 16 - Train Loss: 0.222335\n",
      "Checkpoint saved for fold 1, session 7, epoch 16\n",
      "Session 7 Epoch 17 - Train Loss: 0.221920\n",
      "Checkpoint saved for fold 1, session 7, epoch 17\n",
      "Session 7 Epoch 18 - Train Loss: 0.222485\n",
      "Session 7 Epoch 19 - Train Loss: 0.221796\n",
      "Checkpoint saved for fold 1, session 7, epoch 19\n",
      "Session 7 Epoch 20 - Train Loss: 0.221621\n",
      "Checkpoint saved for fold 1, session 7, epoch 20\n",
      "Session 7 Epoch 21 - Train Loss: 0.221264\n",
      "Checkpoint saved for fold 1, session 7, epoch 21\n",
      "Session 7 Epoch 22 - Train Loss: 0.220951\n",
      "Checkpoint saved for fold 1, session 7, epoch 22\n",
      "Session 7 Epoch 23 - Train Loss: 0.221357\n",
      "Session 7 Epoch 24 - Train Loss: 0.221007\n",
      "Session 7 Epoch 25 - Train Loss: 0.220863\n",
      "Session 7 Epoch 26 - Train Loss: 0.220460\n",
      "Checkpoint saved for fold 1, session 7, epoch 26\n",
      "Session 7 Epoch 27 - Train Loss: 0.220313\n",
      "Checkpoint saved for fold 1, session 7, epoch 27\n",
      "Session 7 Epoch 28 - Train Loss: 0.220049\n",
      "Checkpoint saved for fold 1, session 7, epoch 28\n",
      "Session 7 Epoch 29 - Train Loss: 0.219349\n",
      "Checkpoint saved for fold 1, session 7, epoch 29\n",
      "Session 7 Epoch 30 - Train Loss: 0.219829\n",
      "Session 7 Epoch 31 - Train Loss: 0.218515\n",
      "Checkpoint saved for fold 1, session 7, epoch 31\n",
      "Session 7 Epoch 32 - Train Loss: 0.219321\n",
      "Session 7 Epoch 33 - Train Loss: 0.218204\n",
      "Checkpoint saved for fold 1, session 7, epoch 33\n",
      "Session 7 Epoch 34 - Train Loss: 0.218380\n",
      "Session 7 Epoch 35 - Train Loss: 0.217843\n",
      "Checkpoint saved for fold 1, session 7, epoch 35\n",
      "Session 7 Epoch 36 - Train Loss: 0.218219\n",
      "Session 7 Epoch 37 - Train Loss: 0.217743\n",
      "Checkpoint saved for fold 1, session 7, epoch 37\n",
      "Session 7 Epoch 38 - Train Loss: 0.217378\n",
      "Checkpoint saved for fold 1, session 7, epoch 38\n",
      "Session 7 Epoch 39 - Train Loss: 0.217161\n",
      "Checkpoint saved for fold 1, session 7, epoch 39\n",
      "Session 7 Epoch 40 - Train Loss: 0.216982\n",
      "Checkpoint saved for fold 1, session 7, epoch 40\n",
      "Session 7 Epoch 41 - Train Loss: 0.216910\n",
      "Session 7 Epoch 42 - Train Loss: 0.216519\n",
      "Checkpoint saved for fold 1, session 7, epoch 42\n",
      "Session 7 Epoch 43 - Train Loss: 0.216290\n",
      "Checkpoint saved for fold 1, session 7, epoch 43\n",
      "Session 7 Epoch 44 - Train Loss: 0.216202\n",
      "Session 7 Epoch 45 - Train Loss: 0.215355\n",
      "Checkpoint saved for fold 1, session 7, epoch 45\n",
      "Session 7 Epoch 46 - Train Loss: 0.216029\n",
      "Session 7 Epoch 47 - Train Loss: 0.215598\n",
      "Session 7 Epoch 48 - Train Loss: 0.215117\n",
      "Checkpoint saved for fold 1, session 7, epoch 48\n",
      "Session 7 Epoch 49 - Train Loss: 0.215345\n",
      "Session 7 Epoch 50 - Train Loss: 0.214697\n",
      "Checkpoint saved for fold 1, session 7, epoch 50\n",
      "Session 7 Epoch 51 - Train Loss: 0.214387\n",
      "Checkpoint saved for fold 1, session 7, epoch 51\n",
      "Session 7 Epoch 52 - Train Loss: 0.214648\n",
      "Session 7 Epoch 53 - Train Loss: 0.213781\n",
      "Checkpoint saved for fold 1, session 7, epoch 53\n",
      "Session 7 Epoch 54 - Train Loss: 0.213377\n",
      "Checkpoint saved for fold 1, session 7, epoch 54\n",
      "Session 7 Epoch 55 - Train Loss: 0.213252\n",
      "Checkpoint saved for fold 1, session 7, epoch 55\n",
      "Session 7 Epoch 56 - Train Loss: 0.213388\n",
      "Session 7 Epoch 57 - Train Loss: 0.213194\n",
      "Session 7 Epoch 58 - Train Loss: 0.212617\n",
      "Checkpoint saved for fold 1, session 7, epoch 58\n",
      "Session 7 Epoch 59 - Train Loss: 0.212235\n",
      "Checkpoint saved for fold 1, session 7, epoch 59\n",
      "Session 7 Epoch 60 - Train Loss: 0.212139\n",
      "Session 7 Epoch 61 - Train Loss: 0.212390\n",
      "Session 7 Epoch 62 - Train Loss: 0.211961\n",
      "Checkpoint saved for fold 1, session 7, epoch 62\n",
      "Session 7 Epoch 63 - Train Loss: 0.211538\n",
      "Checkpoint saved for fold 1, session 7, epoch 63\n",
      "Session 7 Epoch 64 - Train Loss: 0.211809\n",
      "Session 7 Epoch 65 - Train Loss: 0.211749\n",
      "Session 7 Epoch 66 - Train Loss: 0.210698\n",
      "Checkpoint saved for fold 1, session 7, epoch 66\n",
      "Session 7 Epoch 67 - Train Loss: 0.210915\n",
      "Session 7 Epoch 68 - Train Loss: 0.210482\n",
      "Checkpoint saved for fold 1, session 7, epoch 68\n",
      "Session 7 Epoch 69 - Train Loss: 0.210350\n",
      "Checkpoint saved for fold 1, session 7, epoch 69\n",
      "Session 7 Epoch 70 - Train Loss: 0.210280\n",
      "Session 7 Epoch 71 - Train Loss: 0.209980\n",
      "Checkpoint saved for fold 1, session 7, epoch 71\n",
      "Session 7 Epoch 72 - Train Loss: 0.209621\n",
      "Checkpoint saved for fold 1, session 7, epoch 72\n",
      "Session 7 Epoch 73 - Train Loss: 0.209459\n",
      "Checkpoint saved for fold 1, session 7, epoch 73\n",
      "Session 7 Epoch 74 - Train Loss: 0.209211\n",
      "Checkpoint saved for fold 1, session 7, epoch 74\n",
      "Session 7 Epoch 75 - Train Loss: 0.209363\n",
      "Session 7 Epoch 76 - Train Loss: 0.208783\n",
      "Checkpoint saved for fold 1, session 7, epoch 76\n",
      "Session 7 Epoch 77 - Train Loss: 0.208835\n",
      "Session 7 Epoch 78 - Train Loss: 0.208904\n",
      "Session 7 Epoch 79 - Train Loss: 0.208898\n",
      "Session 7 Epoch 80 - Train Loss: 0.207568\n",
      "Checkpoint saved for fold 1, session 7, epoch 80\n",
      "Training on Session 9/9\n",
      "Session 8 Epoch 1 - Train Loss: 0.127554\n",
      "Checkpoint saved for fold 1, session 8, epoch 1\n",
      "Session 8 Epoch 2 - Train Loss: 0.127766\n",
      "Session 8 Epoch 3 - Train Loss: 0.128082\n",
      "Session 8 Epoch 4 - Train Loss: 0.127597\n",
      "Session 8 Epoch 5 - Train Loss: 0.127157\n",
      "Checkpoint saved for fold 1, session 8, epoch 5\n",
      "Session 8 Epoch 6 - Train Loss: 0.127126\n",
      "Session 8 Epoch 7 - Train Loss: 0.127082\n",
      "Session 8 Epoch 8 - Train Loss: 0.127021\n",
      "Checkpoint saved for fold 1, session 8, epoch 8\n",
      "Session 8 Epoch 9 - Train Loss: 0.126851\n",
      "Checkpoint saved for fold 1, session 8, epoch 9\n",
      "Session 8 Epoch 10 - Train Loss: 0.126805\n",
      "Session 8 Epoch 11 - Train Loss: 0.126388\n",
      "Checkpoint saved for fold 1, session 8, epoch 11\n",
      "Session 8 Epoch 12 - Train Loss: 0.126550\n",
      "Session 8 Epoch 13 - Train Loss: 0.126567\n",
      "Session 8 Epoch 14 - Train Loss: 0.126657\n",
      "Session 8 Epoch 15 - Train Loss: 0.125880\n",
      "Checkpoint saved for fold 1, session 8, epoch 15\n",
      "Session 8 Epoch 16 - Train Loss: 0.126225\n",
      "Session 8 Epoch 17 - Train Loss: 0.125091\n",
      "Checkpoint saved for fold 1, session 8, epoch 17\n",
      "Session 8 Epoch 18 - Train Loss: 0.125909\n",
      "Session 8 Epoch 19 - Train Loss: 0.125640\n",
      "Session 8 Epoch 20 - Train Loss: 0.125691\n",
      "Session 8 Epoch 21 - Train Loss: 0.125472\n",
      "Session 8 Epoch 22 - Train Loss: 0.125643\n",
      "Session 8 Epoch 23 - Train Loss: 0.125896\n",
      "Session 8 Epoch 24 - Train Loss: 0.125537\n",
      "Session 8 Epoch 25 - Train Loss: 0.125786\n",
      "Session 8 Epoch 26 - Train Loss: 0.125388\n",
      "Session 8 Epoch 27 - Train Loss: 0.125086\n",
      "Early stopping at epoch 27 for session 8\n",
      "Fold 1 - Test Loss: 0.1132, R^2: -44688662408493.7109\n",
      "\n",
      "=== Fold 2 ===\n",
      "Training on Session 1/9\n",
      "Session 0 Epoch 1 - Train Loss: 0.054424\n",
      "Checkpoint saved for fold 2, session 0, epoch 1\n",
      "Session 0 Epoch 2 - Train Loss: 0.053160\n",
      "Checkpoint saved for fold 2, session 0, epoch 2\n",
      "Session 0 Epoch 3 - Train Loss: 0.051738\n",
      "Checkpoint saved for fold 2, session 0, epoch 3\n",
      "Session 0 Epoch 4 - Train Loss: 0.046906\n",
      "Checkpoint saved for fold 2, session 0, epoch 4\n",
      "Session 0 Epoch 5 - Train Loss: 0.040402\n",
      "Checkpoint saved for fold 2, session 0, epoch 5\n",
      "Session 0 Epoch 6 - Train Loss: 0.035224\n",
      "Checkpoint saved for fold 2, session 0, epoch 6\n",
      "Session 0 Epoch 7 - Train Loss: 0.034888\n",
      "Checkpoint saved for fold 2, session 0, epoch 7\n",
      "Session 0 Epoch 8 - Train Loss: 0.031930\n",
      "Checkpoint saved for fold 2, session 0, epoch 8\n",
      "Session 0 Epoch 9 - Train Loss: 0.030838\n",
      "Checkpoint saved for fold 2, session 0, epoch 9\n",
      "Session 0 Epoch 10 - Train Loss: 0.030109\n",
      "Checkpoint saved for fold 2, session 0, epoch 10\n",
      "Session 0 Epoch 11 - Train Loss: 0.030103\n",
      "Session 0 Epoch 12 - Train Loss: 0.028578\n",
      "Checkpoint saved for fold 2, session 0, epoch 12\n",
      "Session 0 Epoch 13 - Train Loss: 0.027777\n",
      "Checkpoint saved for fold 2, session 0, epoch 13\n",
      "Session 0 Epoch 14 - Train Loss: 0.027232\n",
      "Checkpoint saved for fold 2, session 0, epoch 14\n",
      "Session 0 Epoch 15 - Train Loss: 0.025861\n",
      "Checkpoint saved for fold 2, session 0, epoch 15\n",
      "Session 0 Epoch 16 - Train Loss: 0.025200\n",
      "Checkpoint saved for fold 2, session 0, epoch 16\n",
      "Session 0 Epoch 17 - Train Loss: 0.024493\n",
      "Checkpoint saved for fold 2, session 0, epoch 17\n",
      "Session 0 Epoch 18 - Train Loss: 0.023402\n",
      "Checkpoint saved for fold 2, session 0, epoch 18\n",
      "Session 0 Epoch 19 - Train Loss: 0.023498\n",
      "Session 0 Epoch 20 - Train Loss: 0.023319\n",
      "Session 0 Epoch 21 - Train Loss: 0.023083\n",
      "Checkpoint saved for fold 2, session 0, epoch 21\n",
      "Session 0 Epoch 22 - Train Loss: 0.021605\n",
      "Checkpoint saved for fold 2, session 0, epoch 22\n",
      "Session 0 Epoch 23 - Train Loss: 0.022087\n",
      "Session 0 Epoch 24 - Train Loss: 0.020322\n",
      "Checkpoint saved for fold 2, session 0, epoch 24\n",
      "Session 0 Epoch 25 - Train Loss: 0.020576\n",
      "Session 0 Epoch 26 - Train Loss: 0.019436\n",
      "Checkpoint saved for fold 2, session 0, epoch 26\n",
      "Session 0 Epoch 27 - Train Loss: 0.018963\n",
      "Checkpoint saved for fold 2, session 0, epoch 27\n",
      "Session 0 Epoch 28 - Train Loss: 0.019210\n",
      "Session 0 Epoch 29 - Train Loss: 0.018529\n",
      "Checkpoint saved for fold 2, session 0, epoch 29\n",
      "Session 0 Epoch 30 - Train Loss: 0.018512\n",
      "Session 0 Epoch 31 - Train Loss: 0.018511\n",
      "Session 0 Epoch 32 - Train Loss: 0.017996\n",
      "Checkpoint saved for fold 2, session 0, epoch 32\n",
      "Session 0 Epoch 33 - Train Loss: 0.017805\n",
      "Checkpoint saved for fold 2, session 0, epoch 33\n",
      "Session 0 Epoch 34 - Train Loss: 0.017392\n",
      "Checkpoint saved for fold 2, session 0, epoch 34\n",
      "Session 0 Epoch 35 - Train Loss: 0.016848\n",
      "Checkpoint saved for fold 2, session 0, epoch 35\n",
      "Session 0 Epoch 36 - Train Loss: 0.016328\n",
      "Checkpoint saved for fold 2, session 0, epoch 36\n",
      "Session 0 Epoch 37 - Train Loss: 0.016564\n",
      "Session 0 Epoch 38 - Train Loss: 0.015695\n",
      "Checkpoint saved for fold 2, session 0, epoch 38\n",
      "Session 0 Epoch 39 - Train Loss: 0.014894\n",
      "Checkpoint saved for fold 2, session 0, epoch 39\n",
      "Session 0 Epoch 40 - Train Loss: 0.015995\n",
      "Session 0 Epoch 41 - Train Loss: 0.014685\n",
      "Checkpoint saved for fold 2, session 0, epoch 41\n",
      "Session 0 Epoch 42 - Train Loss: 0.014585\n",
      "Checkpoint saved for fold 2, session 0, epoch 42\n",
      "Session 0 Epoch 43 - Train Loss: 0.014525\n",
      "Session 0 Epoch 44 - Train Loss: 0.013834\n",
      "Checkpoint saved for fold 2, session 0, epoch 44\n",
      "Session 0 Epoch 45 - Train Loss: 0.014224\n",
      "Session 0 Epoch 46 - Train Loss: 0.013559\n",
      "Checkpoint saved for fold 2, session 0, epoch 46\n",
      "Session 0 Epoch 47 - Train Loss: 0.013514\n",
      "Session 0 Epoch 48 - Train Loss: 0.013388\n",
      "Checkpoint saved for fold 2, session 0, epoch 48\n",
      "Session 0 Epoch 49 - Train Loss: 0.013232\n",
      "Checkpoint saved for fold 2, session 0, epoch 49\n",
      "Session 0 Epoch 50 - Train Loss: 0.013151\n",
      "Session 0 Epoch 51 - Train Loss: 0.012678\n",
      "Checkpoint saved for fold 2, session 0, epoch 51\n",
      "Session 0 Epoch 52 - Train Loss: 0.012329\n",
      "Checkpoint saved for fold 2, session 0, epoch 52\n",
      "Session 0 Epoch 53 - Train Loss: 0.011339\n",
      "Checkpoint saved for fold 2, session 0, epoch 53\n",
      "Session 0 Epoch 54 - Train Loss: 0.011378\n",
      "Session 0 Epoch 55 - Train Loss: 0.010771\n",
      "Checkpoint saved for fold 2, session 0, epoch 55\n",
      "Session 0 Epoch 56 - Train Loss: 0.011017\n",
      "Session 0 Epoch 57 - Train Loss: 0.010767\n",
      "Session 0 Epoch 58 - Train Loss: 0.011096\n",
      "Session 0 Epoch 59 - Train Loss: 0.009962\n",
      "Checkpoint saved for fold 2, session 0, epoch 59\n",
      "Session 0 Epoch 60 - Train Loss: 0.010383\n",
      "Session 0 Epoch 61 - Train Loss: 0.010155\n",
      "Session 0 Epoch 62 - Train Loss: 0.009988\n",
      "Session 0 Epoch 63 - Train Loss: 0.010384\n",
      "Session 0 Epoch 64 - Train Loss: 0.009881\n",
      "Session 0 Epoch 65 - Train Loss: 0.009751\n",
      "Checkpoint saved for fold 2, session 0, epoch 65\n",
      "Session 0 Epoch 66 - Train Loss: 0.009743\n",
      "Session 0 Epoch 67 - Train Loss: 0.009663\n",
      "Session 0 Epoch 68 - Train Loss: 0.009670\n",
      "Session 0 Epoch 69 - Train Loss: 0.008843\n",
      "Checkpoint saved for fold 2, session 0, epoch 69\n",
      "Session 0 Epoch 70 - Train Loss: 0.009264\n",
      "Session 0 Epoch 71 - Train Loss: 0.009540\n",
      "Session 0 Epoch 72 - Train Loss: 0.009223\n",
      "Session 0 Epoch 73 - Train Loss: 0.009069\n",
      "Session 0 Epoch 74 - Train Loss: 0.008641\n",
      "Checkpoint saved for fold 2, session 0, epoch 74\n",
      "Session 0 Epoch 75 - Train Loss: 0.008243\n",
      "Checkpoint saved for fold 2, session 0, epoch 75\n",
      "Session 0 Epoch 76 - Train Loss: 0.008422\n",
      "Session 0 Epoch 77 - Train Loss: 0.008018\n",
      "Checkpoint saved for fold 2, session 0, epoch 77\n",
      "Session 0 Epoch 78 - Train Loss: 0.007636\n",
      "Checkpoint saved for fold 2, session 0, epoch 78\n",
      "Session 0 Epoch 79 - Train Loss: 0.007426\n",
      "Checkpoint saved for fold 2, session 0, epoch 79\n",
      "Session 0 Epoch 80 - Train Loss: 0.007466\n",
      "Training on Session 2/9\n",
      "Session 1 Epoch 1 - Train Loss: 0.051168\n",
      "Checkpoint saved for fold 2, session 1, epoch 1\n",
      "Session 1 Epoch 2 - Train Loss: 0.050836\n",
      "Checkpoint saved for fold 2, session 1, epoch 2\n",
      "Session 1 Epoch 3 - Train Loss: 0.049979\n",
      "Checkpoint saved for fold 2, session 1, epoch 3\n",
      "Session 1 Epoch 4 - Train Loss: 0.048888\n",
      "Checkpoint saved for fold 2, session 1, epoch 4\n",
      "Session 1 Epoch 5 - Train Loss: 0.045286\n",
      "Checkpoint saved for fold 2, session 1, epoch 5\n",
      "Session 1 Epoch 6 - Train Loss: 0.041519\n",
      "Checkpoint saved for fold 2, session 1, epoch 6\n",
      "Session 1 Epoch 7 - Train Loss: 0.040190\n",
      "Checkpoint saved for fold 2, session 1, epoch 7\n",
      "Session 1 Epoch 8 - Train Loss: 0.037994\n",
      "Checkpoint saved for fold 2, session 1, epoch 8\n",
      "Session 1 Epoch 9 - Train Loss: 0.037013\n",
      "Checkpoint saved for fold 2, session 1, epoch 9\n",
      "Session 1 Epoch 10 - Train Loss: 0.035787\n",
      "Checkpoint saved for fold 2, session 1, epoch 10\n",
      "Session 1 Epoch 11 - Train Loss: 0.032986\n",
      "Checkpoint saved for fold 2, session 1, epoch 11\n",
      "Session 1 Epoch 12 - Train Loss: 0.029537\n",
      "Checkpoint saved for fold 2, session 1, epoch 12\n",
      "Session 1 Epoch 13 - Train Loss: 0.028445\n",
      "Checkpoint saved for fold 2, session 1, epoch 13\n",
      "Session 1 Epoch 14 - Train Loss: 0.028311\n",
      "Checkpoint saved for fold 2, session 1, epoch 14\n",
      "Session 1 Epoch 15 - Train Loss: 0.027000\n",
      "Checkpoint saved for fold 2, session 1, epoch 15\n",
      "Session 1 Epoch 16 - Train Loss: 0.026234\n",
      "Checkpoint saved for fold 2, session 1, epoch 16\n",
      "Session 1 Epoch 17 - Train Loss: 0.025148\n",
      "Checkpoint saved for fold 2, session 1, epoch 17\n",
      "Session 1 Epoch 18 - Train Loss: 0.024754\n",
      "Checkpoint saved for fold 2, session 1, epoch 18\n",
      "Session 1 Epoch 19 - Train Loss: 0.024861\n",
      "Session 1 Epoch 20 - Train Loss: 0.023938\n",
      "Checkpoint saved for fold 2, session 1, epoch 20\n",
      "Session 1 Epoch 21 - Train Loss: 0.023425\n",
      "Checkpoint saved for fold 2, session 1, epoch 21\n",
      "Session 1 Epoch 22 - Train Loss: 0.023061\n",
      "Checkpoint saved for fold 2, session 1, epoch 22\n",
      "Session 1 Epoch 23 - Train Loss: 0.022650\n",
      "Checkpoint saved for fold 2, session 1, epoch 23\n",
      "Session 1 Epoch 24 - Train Loss: 0.021823\n",
      "Checkpoint saved for fold 2, session 1, epoch 24\n",
      "Session 1 Epoch 25 - Train Loss: 0.021255\n",
      "Checkpoint saved for fold 2, session 1, epoch 25\n",
      "Session 1 Epoch 26 - Train Loss: 0.021127\n",
      "Checkpoint saved for fold 2, session 1, epoch 26\n",
      "Session 1 Epoch 27 - Train Loss: 0.021036\n",
      "Session 1 Epoch 28 - Train Loss: 0.020858\n",
      "Checkpoint saved for fold 2, session 1, epoch 28\n",
      "Session 1 Epoch 29 - Train Loss: 0.020551\n",
      "Checkpoint saved for fold 2, session 1, epoch 29\n",
      "Session 1 Epoch 30 - Train Loss: 0.020043\n",
      "Checkpoint saved for fold 2, session 1, epoch 30\n",
      "Session 1 Epoch 31 - Train Loss: 0.019967\n",
      "Session 1 Epoch 32 - Train Loss: 0.020256\n",
      "Session 1 Epoch 33 - Train Loss: 0.019793\n",
      "Checkpoint saved for fold 2, session 1, epoch 33\n",
      "Session 1 Epoch 34 - Train Loss: 0.019596\n",
      "Checkpoint saved for fold 2, session 1, epoch 34\n",
      "Session 1 Epoch 35 - Train Loss: 0.019396\n",
      "Checkpoint saved for fold 2, session 1, epoch 35\n",
      "Session 1 Epoch 36 - Train Loss: 0.019036\n",
      "Checkpoint saved for fold 2, session 1, epoch 36\n",
      "Session 1 Epoch 37 - Train Loss: 0.019446\n",
      "Session 1 Epoch 38 - Train Loss: 0.019282\n",
      "Session 1 Epoch 39 - Train Loss: 0.019249\n",
      "Session 1 Epoch 40 - Train Loss: 0.019279\n",
      "Session 1 Epoch 41 - Train Loss: 0.019270\n",
      "Session 1 Epoch 42 - Train Loss: 0.019075\n",
      "Session 1 Epoch 43 - Train Loss: 0.018858\n",
      "Checkpoint saved for fold 2, session 1, epoch 43\n",
      "Session 1 Epoch 44 - Train Loss: 0.019119\n",
      "Session 1 Epoch 45 - Train Loss: 0.018590\n",
      "Checkpoint saved for fold 2, session 1, epoch 45\n",
      "Session 1 Epoch 46 - Train Loss: 0.018855\n",
      "Session 1 Epoch 47 - Train Loss: 0.019290\n",
      "Session 1 Epoch 48 - Train Loss: 0.019135\n",
      "Session 1 Epoch 49 - Train Loss: 0.018926\n",
      "Session 1 Epoch 50 - Train Loss: 0.018966\n",
      "Session 1 Epoch 51 - Train Loss: 0.018841\n",
      "Session 1 Epoch 52 - Train Loss: 0.018515\n",
      "Session 1 Epoch 53 - Train Loss: 0.018967\n",
      "Session 1 Epoch 54 - Train Loss: 0.018605\n",
      "Session 1 Epoch 55 - Train Loss: 0.018811\n",
      "Early stopping at epoch 55 for session 1\n",
      "Training on Session 3/9\n",
      "Session 2 Epoch 1 - Train Loss: 0.136693\n",
      "Checkpoint saved for fold 2, session 2, epoch 1\n",
      "Session 2 Epoch 2 - Train Loss: 0.131354\n",
      "Checkpoint saved for fold 2, session 2, epoch 2\n",
      "Session 2 Epoch 3 - Train Loss: 0.124266\n",
      "Checkpoint saved for fold 2, session 2, epoch 3\n",
      "Session 2 Epoch 4 - Train Loss: 0.113286\n",
      "Checkpoint saved for fold 2, session 2, epoch 4\n",
      "Session 2 Epoch 5 - Train Loss: 0.103569\n",
      "Checkpoint saved for fold 2, session 2, epoch 5\n",
      "Session 2 Epoch 6 - Train Loss: 0.097344\n",
      "Checkpoint saved for fold 2, session 2, epoch 6\n",
      "Session 2 Epoch 7 - Train Loss: 0.092437\n",
      "Checkpoint saved for fold 2, session 2, epoch 7\n",
      "Session 2 Epoch 8 - Train Loss: 0.088629\n",
      "Checkpoint saved for fold 2, session 2, epoch 8\n",
      "Session 2 Epoch 9 - Train Loss: 0.086075\n",
      "Checkpoint saved for fold 2, session 2, epoch 9\n",
      "Session 2 Epoch 10 - Train Loss: 0.084337\n",
      "Checkpoint saved for fold 2, session 2, epoch 10\n",
      "Session 2 Epoch 11 - Train Loss: 0.083152\n",
      "Checkpoint saved for fold 2, session 2, epoch 11\n",
      "Session 2 Epoch 12 - Train Loss: 0.082147\n",
      "Checkpoint saved for fold 2, session 2, epoch 12\n",
      "Session 2 Epoch 13 - Train Loss: 0.081585\n",
      "Checkpoint saved for fold 2, session 2, epoch 13\n",
      "Session 2 Epoch 14 - Train Loss: 0.080406\n",
      "Checkpoint saved for fold 2, session 2, epoch 14\n",
      "Session 2 Epoch 15 - Train Loss: 0.079900\n",
      "Checkpoint saved for fold 2, session 2, epoch 15\n",
      "Session 2 Epoch 16 - Train Loss: 0.079391\n",
      "Checkpoint saved for fold 2, session 2, epoch 16\n",
      "Session 2 Epoch 17 - Train Loss: 0.078815\n",
      "Checkpoint saved for fold 2, session 2, epoch 17\n",
      "Session 2 Epoch 18 - Train Loss: 0.078497\n",
      "Checkpoint saved for fold 2, session 2, epoch 18\n",
      "Session 2 Epoch 19 - Train Loss: 0.078323\n",
      "Checkpoint saved for fold 2, session 2, epoch 19\n",
      "Session 2 Epoch 20 - Train Loss: 0.077657\n",
      "Checkpoint saved for fold 2, session 2, epoch 20\n",
      "Session 2 Epoch 21 - Train Loss: 0.077853\n",
      "Session 2 Epoch 22 - Train Loss: 0.077328\n",
      "Checkpoint saved for fold 2, session 2, epoch 22\n",
      "Session 2 Epoch 23 - Train Loss: 0.076968\n",
      "Checkpoint saved for fold 2, session 2, epoch 23\n",
      "Session 2 Epoch 24 - Train Loss: 0.076839\n",
      "Checkpoint saved for fold 2, session 2, epoch 24\n",
      "Session 2 Epoch 25 - Train Loss: 0.076898\n",
      "Session 2 Epoch 26 - Train Loss: 0.076579\n",
      "Checkpoint saved for fold 2, session 2, epoch 26\n",
      "Session 2 Epoch 27 - Train Loss: 0.076372\n",
      "Checkpoint saved for fold 2, session 2, epoch 27\n",
      "Session 2 Epoch 28 - Train Loss: 0.076341\n",
      "Session 2 Epoch 29 - Train Loss: 0.076235\n",
      "Checkpoint saved for fold 2, session 2, epoch 29\n",
      "Session 2 Epoch 30 - Train Loss: 0.076263\n",
      "Session 2 Epoch 31 - Train Loss: 0.076143\n",
      "Session 2 Epoch 32 - Train Loss: 0.076162\n",
      "Session 2 Epoch 33 - Train Loss: 0.076063\n",
      "Checkpoint saved for fold 2, session 2, epoch 33\n",
      "Session 2 Epoch 34 - Train Loss: 0.075819\n",
      "Checkpoint saved for fold 2, session 2, epoch 34\n",
      "Session 2 Epoch 35 - Train Loss: 0.075877\n",
      "Session 2 Epoch 36 - Train Loss: 0.075734\n",
      "Session 2 Epoch 37 - Train Loss: 0.075486\n",
      "Checkpoint saved for fold 2, session 2, epoch 37\n",
      "Session 2 Epoch 38 - Train Loss: 0.075681\n",
      "Session 2 Epoch 39 - Train Loss: 0.075729\n",
      "Session 2 Epoch 40 - Train Loss: 0.075589\n",
      "Session 2 Epoch 41 - Train Loss: 0.075587\n",
      "Session 2 Epoch 42 - Train Loss: 0.075547\n",
      "Session 2 Epoch 43 - Train Loss: 0.075747\n",
      "Session 2 Epoch 44 - Train Loss: 0.075692\n",
      "Session 2 Epoch 45 - Train Loss: 0.075788\n",
      "Session 2 Epoch 46 - Train Loss: 0.075555\n",
      "Session 2 Epoch 47 - Train Loss: 0.075762\n",
      "Early stopping at epoch 47 for session 2\n",
      "Training on Session 4/9\n",
      "Session 3 Epoch 1 - Train Loss: 0.119501\n",
      "Checkpoint saved for fold 2, session 3, epoch 1\n",
      "Session 3 Epoch 2 - Train Loss: 0.118951\n",
      "Checkpoint saved for fold 2, session 3, epoch 2\n",
      "Session 3 Epoch 3 - Train Loss: 0.118911\n",
      "Session 3 Epoch 4 - Train Loss: 0.118535\n",
      "Checkpoint saved for fold 2, session 3, epoch 4\n",
      "Session 3 Epoch 5 - Train Loss: 0.117778\n",
      "Checkpoint saved for fold 2, session 3, epoch 5\n",
      "Session 3 Epoch 6 - Train Loss: 0.117659\n",
      "Checkpoint saved for fold 2, session 3, epoch 6\n",
      "Session 3 Epoch 7 - Train Loss: 0.116857\n",
      "Checkpoint saved for fold 2, session 3, epoch 7\n",
      "Session 3 Epoch 8 - Train Loss: 0.116937\n",
      "Session 3 Epoch 9 - Train Loss: 0.116488\n",
      "Checkpoint saved for fold 2, session 3, epoch 9\n",
      "Session 3 Epoch 10 - Train Loss: 0.116152\n",
      "Checkpoint saved for fold 2, session 3, epoch 10\n",
      "Session 3 Epoch 11 - Train Loss: 0.115809\n",
      "Checkpoint saved for fold 2, session 3, epoch 11\n",
      "Session 3 Epoch 12 - Train Loss: 0.115396\n",
      "Checkpoint saved for fold 2, session 3, epoch 12\n",
      "Session 3 Epoch 13 - Train Loss: 0.115097\n",
      "Checkpoint saved for fold 2, session 3, epoch 13\n",
      "Session 3 Epoch 14 - Train Loss: 0.114754\n",
      "Checkpoint saved for fold 2, session 3, epoch 14\n",
      "Session 3 Epoch 15 - Train Loss: 0.114167\n",
      "Checkpoint saved for fold 2, session 3, epoch 15\n",
      "Session 3 Epoch 16 - Train Loss: 0.114131\n",
      "Session 3 Epoch 17 - Train Loss: 0.113747\n",
      "Checkpoint saved for fold 2, session 3, epoch 17\n",
      "Session 3 Epoch 18 - Train Loss: 0.113486\n",
      "Checkpoint saved for fold 2, session 3, epoch 18\n",
      "Session 3 Epoch 19 - Train Loss: 0.113242\n",
      "Checkpoint saved for fold 2, session 3, epoch 19\n",
      "Session 3 Epoch 20 - Train Loss: 0.112654\n",
      "Checkpoint saved for fold 2, session 3, epoch 20\n",
      "Session 3 Epoch 21 - Train Loss: 0.112717\n",
      "Session 3 Epoch 22 - Train Loss: 0.112300\n",
      "Checkpoint saved for fold 2, session 3, epoch 22\n",
      "Session 3 Epoch 23 - Train Loss: 0.111750\n",
      "Checkpoint saved for fold 2, session 3, epoch 23\n",
      "Session 3 Epoch 24 - Train Loss: 0.111373\n",
      "Checkpoint saved for fold 2, session 3, epoch 24\n",
      "Session 3 Epoch 25 - Train Loss: 0.111074\n",
      "Checkpoint saved for fold 2, session 3, epoch 25\n",
      "Session 3 Epoch 26 - Train Loss: 0.110882\n",
      "Checkpoint saved for fold 2, session 3, epoch 26\n",
      "Session 3 Epoch 27 - Train Loss: 0.110277\n",
      "Checkpoint saved for fold 2, session 3, epoch 27\n",
      "Session 3 Epoch 28 - Train Loss: 0.110502\n",
      "Session 3 Epoch 29 - Train Loss: 0.109909\n",
      "Checkpoint saved for fold 2, session 3, epoch 29\n",
      "Session 3 Epoch 30 - Train Loss: 0.109460\n",
      "Checkpoint saved for fold 2, session 3, epoch 30\n",
      "Session 3 Epoch 31 - Train Loss: 0.109158\n",
      "Checkpoint saved for fold 2, session 3, epoch 31\n",
      "Session 3 Epoch 32 - Train Loss: 0.108933\n",
      "Checkpoint saved for fold 2, session 3, epoch 32\n",
      "Session 3 Epoch 33 - Train Loss: 0.108391\n",
      "Checkpoint saved for fold 2, session 3, epoch 33\n",
      "Session 3 Epoch 34 - Train Loss: 0.108259\n",
      "Checkpoint saved for fold 2, session 3, epoch 34\n",
      "Session 3 Epoch 35 - Train Loss: 0.107959\n",
      "Checkpoint saved for fold 2, session 3, epoch 35\n",
      "Session 3 Epoch 36 - Train Loss: 0.107789\n",
      "Checkpoint saved for fold 2, session 3, epoch 36\n",
      "Session 3 Epoch 37 - Train Loss: 0.107214\n",
      "Checkpoint saved for fold 2, session 3, epoch 37\n",
      "Session 3 Epoch 38 - Train Loss: 0.107308\n",
      "Session 3 Epoch 39 - Train Loss: 0.106517\n",
      "Checkpoint saved for fold 2, session 3, epoch 39\n",
      "Session 3 Epoch 40 - Train Loss: 0.106548\n",
      "Session 3 Epoch 41 - Train Loss: 0.106497\n",
      "Session 3 Epoch 42 - Train Loss: 0.106009\n",
      "Checkpoint saved for fold 2, session 3, epoch 42\n",
      "Session 3 Epoch 43 - Train Loss: 0.105608\n",
      "Checkpoint saved for fold 2, session 3, epoch 43\n",
      "Session 3 Epoch 44 - Train Loss: 0.105445\n",
      "Checkpoint saved for fold 2, session 3, epoch 44\n",
      "Session 3 Epoch 45 - Train Loss: 0.105037\n",
      "Checkpoint saved for fold 2, session 3, epoch 45\n",
      "Session 3 Epoch 46 - Train Loss: 0.104618\n",
      "Checkpoint saved for fold 2, session 3, epoch 46\n",
      "Session 3 Epoch 47 - Train Loss: 0.104656\n",
      "Session 3 Epoch 48 - Train Loss: 0.104503\n",
      "Checkpoint saved for fold 2, session 3, epoch 48\n",
      "Session 3 Epoch 49 - Train Loss: 0.103963\n",
      "Checkpoint saved for fold 2, session 3, epoch 49\n",
      "Session 3 Epoch 50 - Train Loss: 0.103423\n",
      "Checkpoint saved for fold 2, session 3, epoch 50\n",
      "Session 3 Epoch 51 - Train Loss: 0.103452\n",
      "Session 3 Epoch 52 - Train Loss: 0.103158\n",
      "Checkpoint saved for fold 2, session 3, epoch 52\n",
      "Session 3 Epoch 53 - Train Loss: 0.102873\n",
      "Checkpoint saved for fold 2, session 3, epoch 53\n",
      "Session 3 Epoch 54 - Train Loss: 0.102511\n",
      "Checkpoint saved for fold 2, session 3, epoch 54\n",
      "Session 3 Epoch 55 - Train Loss: 0.102026\n",
      "Checkpoint saved for fold 2, session 3, epoch 55\n",
      "Session 3 Epoch 56 - Train Loss: 0.102025\n",
      "Session 3 Epoch 57 - Train Loss: 0.101533\n",
      "Checkpoint saved for fold 2, session 3, epoch 57\n",
      "Session 3 Epoch 58 - Train Loss: 0.101230\n",
      "Checkpoint saved for fold 2, session 3, epoch 58\n",
      "Session 3 Epoch 59 - Train Loss: 0.101206\n",
      "Session 3 Epoch 60 - Train Loss: 0.101033\n",
      "Checkpoint saved for fold 2, session 3, epoch 60\n",
      "Session 3 Epoch 61 - Train Loss: 0.100754\n",
      "Checkpoint saved for fold 2, session 3, epoch 61\n",
      "Session 3 Epoch 62 - Train Loss: 0.100240\n",
      "Checkpoint saved for fold 2, session 3, epoch 62\n",
      "Session 3 Epoch 63 - Train Loss: 0.100034\n",
      "Checkpoint saved for fold 2, session 3, epoch 63\n",
      "Session 3 Epoch 64 - Train Loss: 0.099983\n",
      "Session 3 Epoch 65 - Train Loss: 0.099416\n",
      "Checkpoint saved for fold 2, session 3, epoch 65\n",
      "Session 3 Epoch 66 - Train Loss: 0.099253\n",
      "Checkpoint saved for fold 2, session 3, epoch 66\n",
      "Session 3 Epoch 67 - Train Loss: 0.099040\n",
      "Checkpoint saved for fold 2, session 3, epoch 67\n",
      "Session 3 Epoch 68 - Train Loss: 0.098814\n",
      "Checkpoint saved for fold 2, session 3, epoch 68\n",
      "Session 3 Epoch 69 - Train Loss: 0.098423\n",
      "Checkpoint saved for fold 2, session 3, epoch 69\n",
      "Session 3 Epoch 70 - Train Loss: 0.098039\n",
      "Checkpoint saved for fold 2, session 3, epoch 70\n",
      "Session 3 Epoch 71 - Train Loss: 0.097971\n",
      "Session 3 Epoch 72 - Train Loss: 0.097681\n",
      "Checkpoint saved for fold 2, session 3, epoch 72\n",
      "Session 3 Epoch 73 - Train Loss: 0.097641\n",
      "Session 3 Epoch 74 - Train Loss: 0.097121\n",
      "Checkpoint saved for fold 2, session 3, epoch 74\n",
      "Session 3 Epoch 75 - Train Loss: 0.096971\n",
      "Checkpoint saved for fold 2, session 3, epoch 75\n",
      "Session 3 Epoch 76 - Train Loss: 0.096663\n",
      "Checkpoint saved for fold 2, session 3, epoch 76\n",
      "Session 3 Epoch 77 - Train Loss: 0.096470\n",
      "Checkpoint saved for fold 2, session 3, epoch 77\n",
      "Session 3 Epoch 78 - Train Loss: 0.096262\n",
      "Checkpoint saved for fold 2, session 3, epoch 78\n",
      "Session 3 Epoch 79 - Train Loss: 0.095773\n",
      "Checkpoint saved for fold 2, session 3, epoch 79\n",
      "Session 3 Epoch 80 - Train Loss: 0.095602\n",
      "Checkpoint saved for fold 2, session 3, epoch 80\n",
      "Training on Session 5/9\n",
      "Session 4 Epoch 1 - Train Loss: 0.140208\n",
      "Checkpoint saved for fold 2, session 4, epoch 1\n",
      "Session 4 Epoch 2 - Train Loss: 0.139726\n",
      "Checkpoint saved for fold 2, session 4, epoch 2\n",
      "Session 4 Epoch 3 - Train Loss: 0.139512\n",
      "Checkpoint saved for fold 2, session 4, epoch 3\n",
      "Session 4 Epoch 4 - Train Loss: 0.139509\n",
      "Session 4 Epoch 5 - Train Loss: 0.139330\n",
      "Checkpoint saved for fold 2, session 4, epoch 5\n",
      "Session 4 Epoch 6 - Train Loss: 0.139147\n",
      "Checkpoint saved for fold 2, session 4, epoch 6\n",
      "Session 4 Epoch 7 - Train Loss: 0.139063\n",
      "Session 4 Epoch 8 - Train Loss: 0.139094\n",
      "Session 4 Epoch 9 - Train Loss: 0.138857\n",
      "Checkpoint saved for fold 2, session 4, epoch 9\n",
      "Session 4 Epoch 10 - Train Loss: 0.138746\n",
      "Checkpoint saved for fold 2, session 4, epoch 10\n",
      "Session 4 Epoch 11 - Train Loss: 0.138650\n",
      "Session 4 Epoch 12 - Train Loss: 0.138430\n",
      "Checkpoint saved for fold 2, session 4, epoch 12\n",
      "Session 4 Epoch 13 - Train Loss: 0.138566\n",
      "Session 4 Epoch 14 - Train Loss: 0.138239\n",
      "Checkpoint saved for fold 2, session 4, epoch 14\n",
      "Session 4 Epoch 15 - Train Loss: 0.138142\n",
      "Session 4 Epoch 16 - Train Loss: 0.138120\n",
      "Checkpoint saved for fold 2, session 4, epoch 16\n",
      "Session 4 Epoch 17 - Train Loss: 0.137735\n",
      "Checkpoint saved for fold 2, session 4, epoch 17\n",
      "Session 4 Epoch 18 - Train Loss: 0.137936\n",
      "Session 4 Epoch 19 - Train Loss: 0.137602\n",
      "Checkpoint saved for fold 2, session 4, epoch 19\n",
      "Session 4 Epoch 20 - Train Loss: 0.137722\n",
      "Session 4 Epoch 21 - Train Loss: 0.137368\n",
      "Checkpoint saved for fold 2, session 4, epoch 21\n",
      "Session 4 Epoch 22 - Train Loss: 0.137306\n",
      "Session 4 Epoch 23 - Train Loss: 0.136988\n",
      "Checkpoint saved for fold 2, session 4, epoch 23\n",
      "Session 4 Epoch 24 - Train Loss: 0.136944\n",
      "Session 4 Epoch 25 - Train Loss: 0.136942\n",
      "Session 4 Epoch 26 - Train Loss: 0.136847\n",
      "Checkpoint saved for fold 2, session 4, epoch 26\n",
      "Session 4 Epoch 27 - Train Loss: 0.136613\n",
      "Checkpoint saved for fold 2, session 4, epoch 27\n",
      "Session 4 Epoch 28 - Train Loss: 0.136177\n",
      "Checkpoint saved for fold 2, session 4, epoch 28\n",
      "Session 4 Epoch 29 - Train Loss: 0.136401\n",
      "Session 4 Epoch 30 - Train Loss: 0.136039\n",
      "Checkpoint saved for fold 2, session 4, epoch 30\n",
      "Session 4 Epoch 31 - Train Loss: 0.136361\n",
      "Session 4 Epoch 32 - Train Loss: 0.135703\n",
      "Checkpoint saved for fold 2, session 4, epoch 32\n",
      "Session 4 Epoch 33 - Train Loss: 0.135909\n",
      "Session 4 Epoch 34 - Train Loss: 0.135608\n",
      "Session 4 Epoch 35 - Train Loss: 0.135774\n",
      "Session 4 Epoch 36 - Train Loss: 0.135484\n",
      "Checkpoint saved for fold 2, session 4, epoch 36\n",
      "Session 4 Epoch 37 - Train Loss: 0.135325\n",
      "Checkpoint saved for fold 2, session 4, epoch 37\n",
      "Session 4 Epoch 38 - Train Loss: 0.135263\n",
      "Session 4 Epoch 39 - Train Loss: 0.135185\n",
      "Checkpoint saved for fold 2, session 4, epoch 39\n",
      "Session 4 Epoch 40 - Train Loss: 0.135078\n",
      "Checkpoint saved for fold 2, session 4, epoch 40\n",
      "Session 4 Epoch 41 - Train Loss: 0.134894\n",
      "Checkpoint saved for fold 2, session 4, epoch 41\n",
      "Session 4 Epoch 42 - Train Loss: 0.134826\n",
      "Session 4 Epoch 43 - Train Loss: 0.134467\n",
      "Checkpoint saved for fold 2, session 4, epoch 43\n",
      "Session 4 Epoch 44 - Train Loss: 0.134401\n",
      "Session 4 Epoch 45 - Train Loss: 0.134198\n",
      "Checkpoint saved for fold 2, session 4, epoch 45\n",
      "Session 4 Epoch 46 - Train Loss: 0.134436\n",
      "Session 4 Epoch 47 - Train Loss: 0.134250\n",
      "Session 4 Epoch 48 - Train Loss: 0.134139\n",
      "Session 4 Epoch 49 - Train Loss: 0.133767\n",
      "Checkpoint saved for fold 2, session 4, epoch 49\n",
      "Session 4 Epoch 50 - Train Loss: 0.133895\n",
      "Session 4 Epoch 51 - Train Loss: 0.133720\n",
      "Session 4 Epoch 52 - Train Loss: 0.133601\n",
      "Checkpoint saved for fold 2, session 4, epoch 52\n",
      "Session 4 Epoch 53 - Train Loss: 0.133724\n",
      "Session 4 Epoch 54 - Train Loss: 0.133311\n",
      "Checkpoint saved for fold 2, session 4, epoch 54\n",
      "Session 4 Epoch 55 - Train Loss: 0.133379\n",
      "Session 4 Epoch 56 - Train Loss: 0.133163\n",
      "Checkpoint saved for fold 2, session 4, epoch 56\n",
      "Session 4 Epoch 57 - Train Loss: 0.132933\n",
      "Checkpoint saved for fold 2, session 4, epoch 57\n",
      "Session 4 Epoch 58 - Train Loss: 0.132910\n",
      "Session 4 Epoch 59 - Train Loss: 0.132852\n",
      "Session 4 Epoch 60 - Train Loss: 0.132804\n",
      "Checkpoint saved for fold 2, session 4, epoch 60\n",
      "Session 4 Epoch 61 - Train Loss: 0.132527\n",
      "Checkpoint saved for fold 2, session 4, epoch 61\n",
      "Session 4 Epoch 62 - Train Loss: 0.132330\n",
      "Checkpoint saved for fold 2, session 4, epoch 62\n",
      "Session 4 Epoch 63 - Train Loss: 0.132452\n",
      "Session 4 Epoch 64 - Train Loss: 0.132327\n",
      "Session 4 Epoch 65 - Train Loss: 0.132160\n",
      "Checkpoint saved for fold 2, session 4, epoch 65\n",
      "Session 4 Epoch 66 - Train Loss: 0.132083\n",
      "Session 4 Epoch 67 - Train Loss: 0.131957\n",
      "Checkpoint saved for fold 2, session 4, epoch 67\n",
      "Session 4 Epoch 68 - Train Loss: 0.131437\n",
      "Checkpoint saved for fold 2, session 4, epoch 68\n",
      "Session 4 Epoch 69 - Train Loss: 0.131847\n",
      "Session 4 Epoch 70 - Train Loss: 0.131772\n",
      "Session 4 Epoch 71 - Train Loss: 0.131457\n",
      "Session 4 Epoch 72 - Train Loss: 0.131243\n",
      "Checkpoint saved for fold 2, session 4, epoch 72\n",
      "Session 4 Epoch 73 - Train Loss: 0.131336\n",
      "Session 4 Epoch 74 - Train Loss: 0.131155\n",
      "Session 4 Epoch 75 - Train Loss: 0.130929\n",
      "Checkpoint saved for fold 2, session 4, epoch 75\n",
      "Session 4 Epoch 76 - Train Loss: 0.131027\n",
      "Session 4 Epoch 77 - Train Loss: 0.130902\n",
      "Session 4 Epoch 78 - Train Loss: 0.130965\n",
      "Session 4 Epoch 79 - Train Loss: 0.130502\n",
      "Checkpoint saved for fold 2, session 4, epoch 79\n",
      "Session 4 Epoch 80 - Train Loss: 0.130493\n",
      "Training on Session 6/9\n",
      "Session 5 Epoch 1 - Train Loss: 0.082473\n",
      "Checkpoint saved for fold 2, session 5, epoch 1\n",
      "Session 5 Epoch 2 - Train Loss: 0.082780\n",
      "Session 5 Epoch 3 - Train Loss: 0.082520\n",
      "Session 5 Epoch 4 - Train Loss: 0.082363\n",
      "Checkpoint saved for fold 2, session 5, epoch 4\n",
      "Session 5 Epoch 5 - Train Loss: 0.082253\n",
      "Checkpoint saved for fold 2, session 5, epoch 5\n",
      "Session 5 Epoch 6 - Train Loss: 0.082302\n",
      "Session 5 Epoch 7 - Train Loss: 0.082424\n",
      "Session 5 Epoch 8 - Train Loss: 0.082159\n",
      "Session 5 Epoch 9 - Train Loss: 0.082151\n",
      "Checkpoint saved for fold 2, session 5, epoch 9\n",
      "Session 5 Epoch 10 - Train Loss: 0.082151\n",
      "Session 5 Epoch 11 - Train Loss: 0.081966\n",
      "Checkpoint saved for fold 2, session 5, epoch 11\n",
      "Session 5 Epoch 12 - Train Loss: 0.082266\n",
      "Session 5 Epoch 13 - Train Loss: 0.081936\n",
      "Session 5 Epoch 14 - Train Loss: 0.081987\n",
      "Session 5 Epoch 15 - Train Loss: 0.082215\n",
      "Session 5 Epoch 16 - Train Loss: 0.082064\n",
      "Session 5 Epoch 17 - Train Loss: 0.082136\n",
      "Session 5 Epoch 18 - Train Loss: 0.081948\n",
      "Session 5 Epoch 19 - Train Loss: 0.082121\n",
      "Session 5 Epoch 20 - Train Loss: 0.081674\n",
      "Checkpoint saved for fold 2, session 5, epoch 20\n",
      "Session 5 Epoch 21 - Train Loss: 0.082059\n",
      "Session 5 Epoch 22 - Train Loss: 0.081637\n",
      "Session 5 Epoch 23 - Train Loss: 0.081749\n",
      "Session 5 Epoch 24 - Train Loss: 0.082112\n",
      "Session 5 Epoch 25 - Train Loss: 0.081753\n",
      "Session 5 Epoch 26 - Train Loss: 0.081720\n",
      "Session 5 Epoch 27 - Train Loss: 0.081824\n",
      "Session 5 Epoch 28 - Train Loss: 0.081680\n",
      "Session 5 Epoch 29 - Train Loss: 0.081600\n",
      "Session 5 Epoch 30 - Train Loss: 0.081547\n",
      "Checkpoint saved for fold 2, session 5, epoch 30\n",
      "Session 5 Epoch 31 - Train Loss: 0.081462\n",
      "Session 5 Epoch 32 - Train Loss: 0.081727\n",
      "Session 5 Epoch 33 - Train Loss: 0.081461\n",
      "Session 5 Epoch 34 - Train Loss: 0.081715\n",
      "Session 5 Epoch 35 - Train Loss: 0.081694\n",
      "Session 5 Epoch 36 - Train Loss: 0.081543\n",
      "Session 5 Epoch 37 - Train Loss: 0.081486\n",
      "Session 5 Epoch 38 - Train Loss: 0.081575\n",
      "Session 5 Epoch 39 - Train Loss: 0.081330\n",
      "Checkpoint saved for fold 2, session 5, epoch 39\n",
      "Session 5 Epoch 40 - Train Loss: 0.081530\n",
      "Session 5 Epoch 41 - Train Loss: 0.081326\n",
      "Session 5 Epoch 42 - Train Loss: 0.081503\n",
      "Session 5 Epoch 43 - Train Loss: 0.081506\n",
      "Session 5 Epoch 44 - Train Loss: 0.081495\n",
      "Session 5 Epoch 45 - Train Loss: 0.081377\n",
      "Session 5 Epoch 46 - Train Loss: 0.081469\n",
      "Session 5 Epoch 47 - Train Loss: 0.081384\n",
      "Session 5 Epoch 48 - Train Loss: 0.081365\n",
      "Session 5 Epoch 49 - Train Loss: 0.080916\n",
      "Checkpoint saved for fold 2, session 5, epoch 49\n",
      "Session 5 Epoch 50 - Train Loss: 0.081226\n",
      "Session 5 Epoch 51 - Train Loss: 0.081243\n",
      "Session 5 Epoch 52 - Train Loss: 0.081348\n",
      "Session 5 Epoch 53 - Train Loss: 0.081118\n",
      "Session 5 Epoch 54 - Train Loss: 0.080762\n",
      "Checkpoint saved for fold 2, session 5, epoch 54\n",
      "Session 5 Epoch 55 - Train Loss: 0.081063\n",
      "Session 5 Epoch 56 - Train Loss: 0.080796\n",
      "Session 5 Epoch 57 - Train Loss: 0.081229\n",
      "Session 5 Epoch 58 - Train Loss: 0.081099\n",
      "Session 5 Epoch 59 - Train Loss: 0.081178\n",
      "Session 5 Epoch 60 - Train Loss: 0.080984\n",
      "Session 5 Epoch 61 - Train Loss: 0.080808\n",
      "Session 5 Epoch 62 - Train Loss: 0.080734\n",
      "Session 5 Epoch 63 - Train Loss: 0.080836\n",
      "Session 5 Epoch 64 - Train Loss: 0.080908\n",
      "Early stopping at epoch 64 for session 5\n",
      "Training on Session 7/9\n",
      "Session 6 Epoch 1 - Train Loss: 0.100191\n",
      "Checkpoint saved for fold 2, session 6, epoch 1\n",
      "Session 6 Epoch 2 - Train Loss: 0.100263\n",
      "Session 6 Epoch 3 - Train Loss: 0.099714\n",
      "Checkpoint saved for fold 2, session 6, epoch 3\n",
      "Session 6 Epoch 4 - Train Loss: 0.099505\n",
      "Checkpoint saved for fold 2, session 6, epoch 4\n",
      "Session 6 Epoch 5 - Train Loss: 0.099258\n",
      "Checkpoint saved for fold 2, session 6, epoch 5\n",
      "Session 6 Epoch 6 - Train Loss: 0.098908\n",
      "Checkpoint saved for fold 2, session 6, epoch 6\n",
      "Session 6 Epoch 7 - Train Loss: 0.098772\n",
      "Checkpoint saved for fold 2, session 6, epoch 7\n",
      "Session 6 Epoch 8 - Train Loss: 0.098240\n",
      "Checkpoint saved for fold 2, session 6, epoch 8\n",
      "Session 6 Epoch 9 - Train Loss: 0.098295\n",
      "Session 6 Epoch 10 - Train Loss: 0.098038\n",
      "Checkpoint saved for fold 2, session 6, epoch 10\n",
      "Session 6 Epoch 11 - Train Loss: 0.097753\n",
      "Checkpoint saved for fold 2, session 6, epoch 11\n",
      "Session 6 Epoch 12 - Train Loss: 0.097588\n",
      "Checkpoint saved for fold 2, session 6, epoch 12\n",
      "Session 6 Epoch 13 - Train Loss: 0.097283\n",
      "Checkpoint saved for fold 2, session 6, epoch 13\n",
      "Session 6 Epoch 14 - Train Loss: 0.096969\n",
      "Checkpoint saved for fold 2, session 6, epoch 14\n",
      "Session 6 Epoch 15 - Train Loss: 0.096827\n",
      "Checkpoint saved for fold 2, session 6, epoch 15\n",
      "Session 6 Epoch 16 - Train Loss: 0.096591\n",
      "Checkpoint saved for fold 2, session 6, epoch 16\n",
      "Session 6 Epoch 17 - Train Loss: 0.096215\n",
      "Checkpoint saved for fold 2, session 6, epoch 17\n",
      "Session 6 Epoch 18 - Train Loss: 0.095987\n",
      "Checkpoint saved for fold 2, session 6, epoch 18\n",
      "Session 6 Epoch 19 - Train Loss: 0.095879\n",
      "Checkpoint saved for fold 2, session 6, epoch 19\n",
      "Session 6 Epoch 20 - Train Loss: 0.095378\n",
      "Checkpoint saved for fold 2, session 6, epoch 20\n",
      "Session 6 Epoch 21 - Train Loss: 0.095516\n",
      "Session 6 Epoch 22 - Train Loss: 0.095116\n",
      "Checkpoint saved for fold 2, session 6, epoch 22\n",
      "Session 6 Epoch 23 - Train Loss: 0.094808\n",
      "Checkpoint saved for fold 2, session 6, epoch 23\n",
      "Session 6 Epoch 24 - Train Loss: 0.095007\n",
      "Session 6 Epoch 25 - Train Loss: 0.094506\n",
      "Checkpoint saved for fold 2, session 6, epoch 25\n",
      "Session 6 Epoch 26 - Train Loss: 0.094250\n",
      "Checkpoint saved for fold 2, session 6, epoch 26\n",
      "Session 6 Epoch 27 - Train Loss: 0.094341\n",
      "Session 6 Epoch 28 - Train Loss: 0.093961\n",
      "Checkpoint saved for fold 2, session 6, epoch 28\n",
      "Session 6 Epoch 29 - Train Loss: 0.093498\n",
      "Checkpoint saved for fold 2, session 6, epoch 29\n",
      "Session 6 Epoch 30 - Train Loss: 0.093305\n",
      "Checkpoint saved for fold 2, session 6, epoch 30\n",
      "Session 6 Epoch 31 - Train Loss: 0.093104\n",
      "Checkpoint saved for fold 2, session 6, epoch 31\n",
      "Session 6 Epoch 32 - Train Loss: 0.093158\n",
      "Session 6 Epoch 33 - Train Loss: 0.092692\n",
      "Checkpoint saved for fold 2, session 6, epoch 33\n",
      "Session 6 Epoch 34 - Train Loss: 0.092711\n",
      "Session 6 Epoch 35 - Train Loss: 0.092222\n",
      "Checkpoint saved for fold 2, session 6, epoch 35\n",
      "Session 6 Epoch 36 - Train Loss: 0.091912\n",
      "Checkpoint saved for fold 2, session 6, epoch 36\n",
      "Session 6 Epoch 37 - Train Loss: 0.091625\n",
      "Checkpoint saved for fold 2, session 6, epoch 37\n",
      "Session 6 Epoch 38 - Train Loss: 0.091553\n",
      "Session 6 Epoch 39 - Train Loss: 0.091345\n",
      "Checkpoint saved for fold 2, session 6, epoch 39\n",
      "Session 6 Epoch 40 - Train Loss: 0.091436\n",
      "Session 6 Epoch 41 - Train Loss: 0.091018\n",
      "Checkpoint saved for fold 2, session 6, epoch 41\n",
      "Session 6 Epoch 42 - Train Loss: 0.090946\n",
      "Session 6 Epoch 43 - Train Loss: 0.090696\n",
      "Checkpoint saved for fold 2, session 6, epoch 43\n",
      "Session 6 Epoch 44 - Train Loss: 0.090582\n",
      "Checkpoint saved for fold 2, session 6, epoch 44\n",
      "Session 6 Epoch 45 - Train Loss: 0.090396\n",
      "Checkpoint saved for fold 2, session 6, epoch 45\n",
      "Session 6 Epoch 46 - Train Loss: 0.089904\n",
      "Checkpoint saved for fold 2, session 6, epoch 46\n",
      "Session 6 Epoch 47 - Train Loss: 0.089685\n",
      "Checkpoint saved for fold 2, session 6, epoch 47\n",
      "Session 6 Epoch 48 - Train Loss: 0.089716\n",
      "Session 6 Epoch 49 - Train Loss: 0.089351\n",
      "Checkpoint saved for fold 2, session 6, epoch 49\n",
      "Session 6 Epoch 50 - Train Loss: 0.089216\n",
      "Checkpoint saved for fold 2, session 6, epoch 50\n",
      "Session 6 Epoch 51 - Train Loss: 0.088897\n",
      "Checkpoint saved for fold 2, session 6, epoch 51\n",
      "Session 6 Epoch 52 - Train Loss: 0.088958\n",
      "Session 6 Epoch 53 - Train Loss: 0.088707\n",
      "Checkpoint saved for fold 2, session 6, epoch 53\n",
      "Session 6 Epoch 54 - Train Loss: 0.088558\n",
      "Checkpoint saved for fold 2, session 6, epoch 54\n",
      "Session 6 Epoch 55 - Train Loss: 0.088192\n",
      "Checkpoint saved for fold 2, session 6, epoch 55\n",
      "Session 6 Epoch 56 - Train Loss: 0.088095\n",
      "Session 6 Epoch 57 - Train Loss: 0.087914\n",
      "Checkpoint saved for fold 2, session 6, epoch 57\n",
      "Session 6 Epoch 58 - Train Loss: 0.087753\n",
      "Checkpoint saved for fold 2, session 6, epoch 58\n",
      "Session 6 Epoch 59 - Train Loss: 0.087708\n",
      "Session 6 Epoch 60 - Train Loss: 0.087144\n",
      "Checkpoint saved for fold 2, session 6, epoch 60\n",
      "Session 6 Epoch 61 - Train Loss: 0.087162\n",
      "Session 6 Epoch 62 - Train Loss: 0.087167\n",
      "Session 6 Epoch 63 - Train Loss: 0.086943\n",
      "Checkpoint saved for fold 2, session 6, epoch 63\n",
      "Session 6 Epoch 64 - Train Loss: 0.086698\n",
      "Checkpoint saved for fold 2, session 6, epoch 64\n",
      "Session 6 Epoch 65 - Train Loss: 0.086339\n",
      "Checkpoint saved for fold 2, session 6, epoch 65\n",
      "Session 6 Epoch 66 - Train Loss: 0.086312\n",
      "Session 6 Epoch 67 - Train Loss: 0.086032\n",
      "Checkpoint saved for fold 2, session 6, epoch 67\n",
      "Session 6 Epoch 68 - Train Loss: 0.085854\n",
      "Checkpoint saved for fold 2, session 6, epoch 68\n",
      "Session 6 Epoch 69 - Train Loss: 0.085740\n",
      "Checkpoint saved for fold 2, session 6, epoch 69\n",
      "Session 6 Epoch 70 - Train Loss: 0.085434\n",
      "Checkpoint saved for fold 2, session 6, epoch 70\n",
      "Session 6 Epoch 71 - Train Loss: 0.085473\n",
      "Session 6 Epoch 72 - Train Loss: 0.085225\n",
      "Checkpoint saved for fold 2, session 6, epoch 72\n",
      "Session 6 Epoch 73 - Train Loss: 0.085070\n",
      "Checkpoint saved for fold 2, session 6, epoch 73\n",
      "Session 6 Epoch 74 - Train Loss: 0.084881\n",
      "Checkpoint saved for fold 2, session 6, epoch 74\n",
      "Session 6 Epoch 75 - Train Loss: 0.084928\n",
      "Session 6 Epoch 76 - Train Loss: 0.084595\n",
      "Checkpoint saved for fold 2, session 6, epoch 76\n",
      "Session 6 Epoch 77 - Train Loss: 0.084448\n",
      "Checkpoint saved for fold 2, session 6, epoch 77\n",
      "Session 6 Epoch 78 - Train Loss: 0.084291\n",
      "Checkpoint saved for fold 2, session 6, epoch 78\n",
      "Session 6 Epoch 79 - Train Loss: 0.084146\n",
      "Checkpoint saved for fold 2, session 6, epoch 79\n",
      "Session 6 Epoch 80 - Train Loss: 0.084050\n",
      "Training on Session 8/9\n",
      "Session 7 Epoch 1 - Train Loss: 0.114727\n",
      "Checkpoint saved for fold 2, session 7, epoch 1\n",
      "Session 7 Epoch 2 - Train Loss: 0.114822\n",
      "Session 7 Epoch 3 - Train Loss: 0.114521\n",
      "Checkpoint saved for fold 2, session 7, epoch 3\n",
      "Session 7 Epoch 4 - Train Loss: 0.114304\n",
      "Checkpoint saved for fold 2, session 7, epoch 4\n",
      "Session 7 Epoch 5 - Train Loss: 0.114410\n",
      "Session 7 Epoch 6 - Train Loss: 0.114436\n",
      "Session 7 Epoch 7 - Train Loss: 0.114136\n",
      "Checkpoint saved for fold 2, session 7, epoch 7\n",
      "Session 7 Epoch 8 - Train Loss: 0.114219\n",
      "Session 7 Epoch 9 - Train Loss: 0.114201\n",
      "Session 7 Epoch 10 - Train Loss: 0.113873\n",
      "Checkpoint saved for fold 2, session 7, epoch 10\n",
      "Session 7 Epoch 11 - Train Loss: 0.113845\n",
      "Session 7 Epoch 12 - Train Loss: 0.113927\n",
      "Session 7 Epoch 13 - Train Loss: 0.113816\n",
      "Session 7 Epoch 14 - Train Loss: 0.113759\n",
      "Checkpoint saved for fold 2, session 7, epoch 14\n",
      "Session 7 Epoch 15 - Train Loss: 0.113708\n",
      "Session 7 Epoch 16 - Train Loss: 0.113546\n",
      "Checkpoint saved for fold 2, session 7, epoch 16\n",
      "Session 7 Epoch 17 - Train Loss: 0.113681\n",
      "Session 7 Epoch 18 - Train Loss: 0.113502\n",
      "Session 7 Epoch 19 - Train Loss: 0.113643\n",
      "Session 7 Epoch 20 - Train Loss: 0.113302\n",
      "Checkpoint saved for fold 2, session 7, epoch 20\n",
      "Session 7 Epoch 21 - Train Loss: 0.113324\n",
      "Session 7 Epoch 22 - Train Loss: 0.113175\n",
      "Checkpoint saved for fold 2, session 7, epoch 22\n",
      "Session 7 Epoch 23 - Train Loss: 0.113239\n",
      "Session 7 Epoch 24 - Train Loss: 0.112995\n",
      "Checkpoint saved for fold 2, session 7, epoch 24\n",
      "Session 7 Epoch 25 - Train Loss: 0.112759\n",
      "Checkpoint saved for fold 2, session 7, epoch 25\n",
      "Session 7 Epoch 26 - Train Loss: 0.112944\n",
      "Session 7 Epoch 27 - Train Loss: 0.112815\n",
      "Session 7 Epoch 28 - Train Loss: 0.112711\n",
      "Session 7 Epoch 29 - Train Loss: 0.112892\n",
      "Session 7 Epoch 30 - Train Loss: 0.112440\n",
      "Checkpoint saved for fold 2, session 7, epoch 30\n",
      "Session 7 Epoch 31 - Train Loss: 0.112727\n",
      "Session 7 Epoch 32 - Train Loss: 0.112622\n",
      "Session 7 Epoch 33 - Train Loss: 0.112632\n",
      "Session 7 Epoch 34 - Train Loss: 0.112447\n",
      "Session 7 Epoch 35 - Train Loss: 0.112251\n",
      "Checkpoint saved for fold 2, session 7, epoch 35\n",
      "Session 7 Epoch 36 - Train Loss: 0.112547\n",
      "Session 7 Epoch 37 - Train Loss: 0.112136\n",
      "Checkpoint saved for fold 2, session 7, epoch 37\n",
      "Session 7 Epoch 38 - Train Loss: 0.112265\n",
      "Session 7 Epoch 39 - Train Loss: 0.112251\n",
      "Session 7 Epoch 40 - Train Loss: 0.112239\n",
      "Session 7 Epoch 41 - Train Loss: 0.112207\n",
      "Session 7 Epoch 42 - Train Loss: 0.111801\n",
      "Checkpoint saved for fold 2, session 7, epoch 42\n",
      "Session 7 Epoch 43 - Train Loss: 0.111884\n",
      "Session 7 Epoch 44 - Train Loss: 0.111900\n",
      "Session 7 Epoch 45 - Train Loss: 0.111715\n",
      "Session 7 Epoch 46 - Train Loss: 0.111721\n",
      "Session 7 Epoch 47 - Train Loss: 0.111630\n",
      "Checkpoint saved for fold 2, session 7, epoch 47\n",
      "Session 7 Epoch 48 - Train Loss: 0.111510\n",
      "Checkpoint saved for fold 2, session 7, epoch 48\n",
      "Session 7 Epoch 49 - Train Loss: 0.111398\n",
      "Checkpoint saved for fold 2, session 7, epoch 49\n",
      "Session 7 Epoch 50 - Train Loss: 0.111447\n",
      "Session 7 Epoch 51 - Train Loss: 0.111357\n",
      "Session 7 Epoch 52 - Train Loss: 0.111454\n",
      "Session 7 Epoch 53 - Train Loss: 0.111316\n",
      "Session 7 Epoch 54 - Train Loss: 0.111258\n",
      "Checkpoint saved for fold 2, session 7, epoch 54\n",
      "Session 7 Epoch 55 - Train Loss: 0.111338\n",
      "Session 7 Epoch 56 - Train Loss: 0.111075\n",
      "Checkpoint saved for fold 2, session 7, epoch 56\n",
      "Session 7 Epoch 57 - Train Loss: 0.111231\n",
      "Session 7 Epoch 58 - Train Loss: 0.110940\n",
      "Checkpoint saved for fold 2, session 7, epoch 58\n",
      "Session 7 Epoch 59 - Train Loss: 0.110925\n",
      "Session 7 Epoch 60 - Train Loss: 0.110884\n",
      "Session 7 Epoch 61 - Train Loss: 0.110737\n",
      "Checkpoint saved for fold 2, session 7, epoch 61\n",
      "Session 7 Epoch 62 - Train Loss: 0.110651\n",
      "Session 7 Epoch 63 - Train Loss: 0.110722\n",
      "Session 7 Epoch 64 - Train Loss: 0.110786\n",
      "Session 7 Epoch 65 - Train Loss: 0.110673\n",
      "Session 7 Epoch 66 - Train Loss: 0.110488\n",
      "Checkpoint saved for fold 2, session 7, epoch 66\n",
      "Session 7 Epoch 67 - Train Loss: 0.110625\n",
      "Session 7 Epoch 68 - Train Loss: 0.110375\n",
      "Checkpoint saved for fold 2, session 7, epoch 68\n",
      "Session 7 Epoch 69 - Train Loss: 0.110526\n",
      "Session 7 Epoch 70 - Train Loss: 0.110275\n",
      "Checkpoint saved for fold 2, session 7, epoch 70\n",
      "Session 7 Epoch 71 - Train Loss: 0.110377\n",
      "Session 7 Epoch 72 - Train Loss: 0.110279\n",
      "Session 7 Epoch 73 - Train Loss: 0.110172\n",
      "Checkpoint saved for fold 2, session 7, epoch 73\n",
      "Session 7 Epoch 74 - Train Loss: 0.110078\n",
      "Session 7 Epoch 75 - Train Loss: 0.110206\n",
      "Session 7 Epoch 76 - Train Loss: 0.110113\n",
      "Session 7 Epoch 77 - Train Loss: 0.109872\n",
      "Checkpoint saved for fold 2, session 7, epoch 77\n",
      "Session 7 Epoch 78 - Train Loss: 0.110047\n",
      "Session 7 Epoch 79 - Train Loss: 0.109795\n",
      "Session 7 Epoch 80 - Train Loss: 0.109780\n",
      "Training on Session 9/9\n",
      "Session 8 Epoch 1 - Train Loss: 0.062113\n",
      "Checkpoint saved for fold 2, session 8, epoch 1\n",
      "Session 8 Epoch 2 - Train Loss: 0.062197\n",
      "Session 8 Epoch 3 - Train Loss: 0.062040\n",
      "Session 8 Epoch 4 - Train Loss: 0.062246\n",
      "Session 8 Epoch 5 - Train Loss: 0.061989\n",
      "Checkpoint saved for fold 2, session 8, epoch 5\n",
      "Session 8 Epoch 6 - Train Loss: 0.061870\n",
      "Checkpoint saved for fold 2, session 8, epoch 6\n",
      "Session 8 Epoch 7 - Train Loss: 0.062011\n",
      "Session 8 Epoch 8 - Train Loss: 0.061849\n",
      "Session 8 Epoch 9 - Train Loss: 0.061913\n",
      "Session 8 Epoch 10 - Train Loss: 0.061689\n",
      "Checkpoint saved for fold 2, session 8, epoch 10\n",
      "Session 8 Epoch 11 - Train Loss: 0.062097\n",
      "Session 8 Epoch 12 - Train Loss: 0.061727\n",
      "Session 8 Epoch 13 - Train Loss: 0.061682\n",
      "Session 8 Epoch 14 - Train Loss: 0.061851\n",
      "Session 8 Epoch 15 - Train Loss: 0.061780\n",
      "Session 8 Epoch 16 - Train Loss: 0.061865\n",
      "Session 8 Epoch 17 - Train Loss: 0.061662\n",
      "Session 8 Epoch 18 - Train Loss: 0.061581\n",
      "Checkpoint saved for fold 2, session 8, epoch 18\n",
      "Session 8 Epoch 19 - Train Loss: 0.061764\n",
      "Session 8 Epoch 20 - Train Loss: 0.061687\n",
      "Session 8 Epoch 21 - Train Loss: 0.061638\n",
      "Session 8 Epoch 22 - Train Loss: 0.061538\n",
      "Session 8 Epoch 23 - Train Loss: 0.061637\n",
      "Session 8 Epoch 24 - Train Loss: 0.061659\n",
      "Session 8 Epoch 25 - Train Loss: 0.061659\n",
      "Session 8 Epoch 26 - Train Loss: 0.061418\n",
      "Checkpoint saved for fold 2, session 8, epoch 26\n",
      "Session 8 Epoch 27 - Train Loss: 0.061541\n",
      "Session 8 Epoch 28 - Train Loss: 0.061637\n",
      "Session 8 Epoch 29 - Train Loss: 0.061606\n",
      "Session 8 Epoch 30 - Train Loss: 0.061478\n",
      "Session 8 Epoch 31 - Train Loss: 0.061560\n",
      "Session 8 Epoch 32 - Train Loss: 0.061467\n",
      "Session 8 Epoch 33 - Train Loss: 0.061667\n",
      "Session 8 Epoch 34 - Train Loss: 0.061592\n",
      "Session 8 Epoch 35 - Train Loss: 0.061419\n",
      "Session 8 Epoch 36 - Train Loss: 0.061452\n",
      "Early stopping at epoch 36 for session 8\n",
      "Fold 2 - Test Loss: 0.4074, R^2: -2575502151738.9575\n",
      "\n",
      "=== Fold 3 ===\n",
      "Training on Session 1/9\n",
      "Session 0 Epoch 1 - Train Loss: 0.054450\n",
      "Checkpoint saved for fold 3, session 0, epoch 1\n",
      "Session 0 Epoch 2 - Train Loss: 0.053016\n",
      "Checkpoint saved for fold 3, session 0, epoch 2\n",
      "Session 0 Epoch 3 - Train Loss: 0.050272\n",
      "Checkpoint saved for fold 3, session 0, epoch 3\n",
      "Session 0 Epoch 4 - Train Loss: 0.043553\n",
      "Checkpoint saved for fold 3, session 0, epoch 4\n",
      "Session 0 Epoch 5 - Train Loss: 0.036449\n",
      "Checkpoint saved for fold 3, session 0, epoch 5\n",
      "Session 0 Epoch 6 - Train Loss: 0.033042\n",
      "Checkpoint saved for fold 3, session 0, epoch 6\n",
      "Session 0 Epoch 7 - Train Loss: 0.030111\n",
      "Checkpoint saved for fold 3, session 0, epoch 7\n",
      "Session 0 Epoch 8 - Train Loss: 0.027753\n",
      "Checkpoint saved for fold 3, session 0, epoch 8\n",
      "Session 0 Epoch 9 - Train Loss: 0.026211\n",
      "Checkpoint saved for fold 3, session 0, epoch 9\n",
      "Session 0 Epoch 10 - Train Loss: 0.023770\n",
      "Checkpoint saved for fold 3, session 0, epoch 10\n",
      "Session 0 Epoch 11 - Train Loss: 0.023920\n",
      "Session 0 Epoch 12 - Train Loss: 0.021601\n",
      "Checkpoint saved for fold 3, session 0, epoch 12\n",
      "Session 0 Epoch 13 - Train Loss: 0.020364\n",
      "Checkpoint saved for fold 3, session 0, epoch 13\n",
      "Session 0 Epoch 14 - Train Loss: 0.020856\n",
      "Session 0 Epoch 15 - Train Loss: 0.019095\n",
      "Checkpoint saved for fold 3, session 0, epoch 15\n",
      "Session 0 Epoch 16 - Train Loss: 0.019879\n",
      "Session 0 Epoch 17 - Train Loss: 0.018674\n",
      "Checkpoint saved for fold 3, session 0, epoch 17\n",
      "Session 0 Epoch 18 - Train Loss: 0.018382\n",
      "Checkpoint saved for fold 3, session 0, epoch 18\n",
      "Session 0 Epoch 19 - Train Loss: 0.017519\n",
      "Checkpoint saved for fold 3, session 0, epoch 19\n",
      "Session 0 Epoch 20 - Train Loss: 0.017339\n",
      "Checkpoint saved for fold 3, session 0, epoch 20\n",
      "Session 0 Epoch 21 - Train Loss: 0.016158\n",
      "Checkpoint saved for fold 3, session 0, epoch 21\n",
      "Session 0 Epoch 22 - Train Loss: 0.016921\n",
      "Session 0 Epoch 23 - Train Loss: 0.017406\n",
      "Session 0 Epoch 24 - Train Loss: 0.016647\n",
      "Session 0 Epoch 25 - Train Loss: 0.016038\n",
      "Checkpoint saved for fold 3, session 0, epoch 25\n",
      "Session 0 Epoch 26 - Train Loss: 0.016271\n",
      "Session 0 Epoch 27 - Train Loss: 0.016066\n",
      "Session 0 Epoch 28 - Train Loss: 0.015732\n",
      "Checkpoint saved for fold 3, session 0, epoch 28\n",
      "Session 0 Epoch 29 - Train Loss: 0.015594\n",
      "Checkpoint saved for fold 3, session 0, epoch 29\n",
      "Session 0 Epoch 30 - Train Loss: 0.015576\n",
      "Session 0 Epoch 31 - Train Loss: 0.016412\n",
      "Session 0 Epoch 32 - Train Loss: 0.015100\n",
      "Checkpoint saved for fold 3, session 0, epoch 32\n",
      "Session 0 Epoch 33 - Train Loss: 0.014645\n",
      "Checkpoint saved for fold 3, session 0, epoch 33\n",
      "Session 0 Epoch 34 - Train Loss: 0.015156\n",
      "Session 0 Epoch 35 - Train Loss: 0.014392\n",
      "Checkpoint saved for fold 3, session 0, epoch 35\n",
      "Session 0 Epoch 36 - Train Loss: 0.014285\n",
      "Checkpoint saved for fold 3, session 0, epoch 36\n",
      "Session 0 Epoch 37 - Train Loss: 0.014079\n",
      "Checkpoint saved for fold 3, session 0, epoch 37\n",
      "Session 0 Epoch 38 - Train Loss: 0.015075\n",
      "Session 0 Epoch 39 - Train Loss: 0.014283\n",
      "Session 0 Epoch 40 - Train Loss: 0.013579\n",
      "Checkpoint saved for fold 3, session 0, epoch 40\n",
      "Session 0 Epoch 41 - Train Loss: 0.012934\n",
      "Checkpoint saved for fold 3, session 0, epoch 41\n",
      "Session 0 Epoch 42 - Train Loss: 0.013258\n",
      "Session 0 Epoch 43 - Train Loss: 0.012981\n",
      "Session 0 Epoch 44 - Train Loss: 0.013607\n",
      "Session 0 Epoch 45 - Train Loss: 0.013468\n",
      "Session 0 Epoch 46 - Train Loss: 0.012573\n",
      "Checkpoint saved for fold 3, session 0, epoch 46\n",
      "Session 0 Epoch 47 - Train Loss: 0.012750\n",
      "Session 0 Epoch 48 - Train Loss: 0.012524\n",
      "Session 0 Epoch 49 - Train Loss: 0.011998\n",
      "Checkpoint saved for fold 3, session 0, epoch 49\n",
      "Session 0 Epoch 50 - Train Loss: 0.012927\n",
      "Session 0 Epoch 51 - Train Loss: 0.012021\n",
      "Session 0 Epoch 52 - Train Loss: 0.012190\n",
      "Session 0 Epoch 53 - Train Loss: 0.011081\n",
      "Checkpoint saved for fold 3, session 0, epoch 53\n",
      "Session 0 Epoch 54 - Train Loss: 0.010838\n",
      "Checkpoint saved for fold 3, session 0, epoch 54\n",
      "Session 0 Epoch 55 - Train Loss: 0.010385\n",
      "Checkpoint saved for fold 3, session 0, epoch 55\n",
      "Session 0 Epoch 56 - Train Loss: 0.010165\n",
      "Checkpoint saved for fold 3, session 0, epoch 56\n",
      "Session 0 Epoch 57 - Train Loss: 0.010200\n",
      "Session 0 Epoch 58 - Train Loss: 0.010238\n",
      "Session 0 Epoch 59 - Train Loss: 0.010183\n",
      "Session 0 Epoch 60 - Train Loss: 0.009931\n",
      "Checkpoint saved for fold 3, session 0, epoch 60\n",
      "Session 0 Epoch 61 - Train Loss: 0.009224\n",
      "Checkpoint saved for fold 3, session 0, epoch 61\n",
      "Session 0 Epoch 62 - Train Loss: 0.009975\n",
      "Session 0 Epoch 63 - Train Loss: 0.009533\n",
      "Session 0 Epoch 64 - Train Loss: 0.009016\n",
      "Checkpoint saved for fold 3, session 0, epoch 64\n",
      "Session 0 Epoch 65 - Train Loss: 0.009111\n",
      "Session 0 Epoch 66 - Train Loss: 0.008977\n",
      "Session 0 Epoch 67 - Train Loss: 0.009666\n",
      "Session 0 Epoch 68 - Train Loss: 0.008313\n",
      "Checkpoint saved for fold 3, session 0, epoch 68\n",
      "Session 0 Epoch 69 - Train Loss: 0.008795\n",
      "Session 0 Epoch 70 - Train Loss: 0.008242\n",
      "Session 0 Epoch 71 - Train Loss: 0.008689\n",
      "Session 0 Epoch 72 - Train Loss: 0.008814\n",
      "Session 0 Epoch 73 - Train Loss: 0.008090\n",
      "Checkpoint saved for fold 3, session 0, epoch 73\n",
      "Session 0 Epoch 74 - Train Loss: 0.008301\n",
      "Session 0 Epoch 75 - Train Loss: 0.008101\n",
      "Session 0 Epoch 76 - Train Loss: 0.007753\n",
      "Checkpoint saved for fold 3, session 0, epoch 76\n",
      "Session 0 Epoch 77 - Train Loss: 0.007568\n",
      "Checkpoint saved for fold 3, session 0, epoch 77\n",
      "Session 0 Epoch 78 - Train Loss: 0.007900\n",
      "Session 0 Epoch 79 - Train Loss: 0.007324\n",
      "Checkpoint saved for fold 3, session 0, epoch 79\n",
      "Session 0 Epoch 80 - Train Loss: 0.007022\n",
      "Checkpoint saved for fold 3, session 0, epoch 80\n",
      "Training on Session 2/9\n",
      "Session 1 Epoch 1 - Train Loss: 0.050716\n",
      "Checkpoint saved for fold 3, session 1, epoch 1\n",
      "Session 1 Epoch 2 - Train Loss: 0.049576\n",
      "Checkpoint saved for fold 3, session 1, epoch 2\n",
      "Session 1 Epoch 3 - Train Loss: 0.048584\n",
      "Checkpoint saved for fold 3, session 1, epoch 3\n",
      "Session 1 Epoch 4 - Train Loss: 0.047079\n",
      "Checkpoint saved for fold 3, session 1, epoch 4\n",
      "Session 1 Epoch 5 - Train Loss: 0.043359\n",
      "Checkpoint saved for fold 3, session 1, epoch 5\n",
      "Session 1 Epoch 6 - Train Loss: 0.041093\n",
      "Checkpoint saved for fold 3, session 1, epoch 6\n",
      "Session 1 Epoch 7 - Train Loss: 0.039231\n",
      "Checkpoint saved for fold 3, session 1, epoch 7\n",
      "Session 1 Epoch 8 - Train Loss: 0.038038\n",
      "Checkpoint saved for fold 3, session 1, epoch 8\n",
      "Session 1 Epoch 9 - Train Loss: 0.037708\n",
      "Checkpoint saved for fold 3, session 1, epoch 9\n",
      "Session 1 Epoch 10 - Train Loss: 0.037833\n",
      "Session 1 Epoch 11 - Train Loss: 0.037342\n",
      "Checkpoint saved for fold 3, session 1, epoch 11\n",
      "Session 1 Epoch 12 - Train Loss: 0.036451\n",
      "Checkpoint saved for fold 3, session 1, epoch 12\n",
      "Session 1 Epoch 13 - Train Loss: 0.036047\n",
      "Checkpoint saved for fold 3, session 1, epoch 13\n",
      "Session 1 Epoch 14 - Train Loss: 0.035321\n",
      "Checkpoint saved for fold 3, session 1, epoch 14\n",
      "Session 1 Epoch 15 - Train Loss: 0.033902\n",
      "Checkpoint saved for fold 3, session 1, epoch 15\n",
      "Session 1 Epoch 16 - Train Loss: 0.032706\n",
      "Checkpoint saved for fold 3, session 1, epoch 16\n",
      "Session 1 Epoch 17 - Train Loss: 0.031895\n",
      "Checkpoint saved for fold 3, session 1, epoch 17\n",
      "Session 1 Epoch 18 - Train Loss: 0.031388\n",
      "Checkpoint saved for fold 3, session 1, epoch 18\n",
      "Session 1 Epoch 19 - Train Loss: 0.030884\n",
      "Checkpoint saved for fold 3, session 1, epoch 19\n",
      "Session 1 Epoch 20 - Train Loss: 0.030319\n",
      "Checkpoint saved for fold 3, session 1, epoch 20\n",
      "Session 1 Epoch 21 - Train Loss: 0.029715\n",
      "Checkpoint saved for fold 3, session 1, epoch 21\n",
      "Session 1 Epoch 22 - Train Loss: 0.029362\n",
      "Checkpoint saved for fold 3, session 1, epoch 22\n",
      "Session 1 Epoch 23 - Train Loss: 0.028893\n",
      "Checkpoint saved for fold 3, session 1, epoch 23\n",
      "Session 1 Epoch 24 - Train Loss: 0.028836\n",
      "Session 1 Epoch 25 - Train Loss: 0.028378\n",
      "Checkpoint saved for fold 3, session 1, epoch 25\n",
      "Session 1 Epoch 26 - Train Loss: 0.028083\n",
      "Checkpoint saved for fold 3, session 1, epoch 26\n",
      "Session 1 Epoch 27 - Train Loss: 0.027662\n",
      "Checkpoint saved for fold 3, session 1, epoch 27\n",
      "Session 1 Epoch 28 - Train Loss: 0.027321\n",
      "Checkpoint saved for fold 3, session 1, epoch 28\n",
      "Session 1 Epoch 29 - Train Loss: 0.027253\n",
      "Session 1 Epoch 30 - Train Loss: 0.027057\n",
      "Checkpoint saved for fold 3, session 1, epoch 30\n",
      "Session 1 Epoch 31 - Train Loss: 0.026291\n",
      "Checkpoint saved for fold 3, session 1, epoch 31\n",
      "Session 1 Epoch 32 - Train Loss: 0.026300\n",
      "Session 1 Epoch 33 - Train Loss: 0.025997\n",
      "Checkpoint saved for fold 3, session 1, epoch 33\n",
      "Session 1 Epoch 34 - Train Loss: 0.025273\n",
      "Checkpoint saved for fold 3, session 1, epoch 34\n",
      "Session 1 Epoch 35 - Train Loss: 0.025393\n",
      "Session 1 Epoch 36 - Train Loss: 0.024814\n",
      "Checkpoint saved for fold 3, session 1, epoch 36\n",
      "Session 1 Epoch 37 - Train Loss: 0.024807\n",
      "Session 1 Epoch 38 - Train Loss: 0.024609\n",
      "Checkpoint saved for fold 3, session 1, epoch 38\n",
      "Session 1 Epoch 39 - Train Loss: 0.024314\n",
      "Checkpoint saved for fold 3, session 1, epoch 39\n",
      "Session 1 Epoch 40 - Train Loss: 0.024471\n",
      "Session 1 Epoch 41 - Train Loss: 0.023927\n",
      "Checkpoint saved for fold 3, session 1, epoch 41\n",
      "Session 1 Epoch 42 - Train Loss: 0.023766\n",
      "Checkpoint saved for fold 3, session 1, epoch 42\n",
      "Session 1 Epoch 43 - Train Loss: 0.024196\n",
      "Session 1 Epoch 44 - Train Loss: 0.023620\n",
      "Checkpoint saved for fold 3, session 1, epoch 44\n",
      "Session 1 Epoch 45 - Train Loss: 0.023815\n",
      "Session 1 Epoch 46 - Train Loss: 0.023792\n",
      "Session 1 Epoch 47 - Train Loss: 0.023548\n",
      "Session 1 Epoch 48 - Train Loss: 0.023607\n",
      "Session 1 Epoch 49 - Train Loss: 0.023014\n",
      "Checkpoint saved for fold 3, session 1, epoch 49\n",
      "Session 1 Epoch 50 - Train Loss: 0.023563\n",
      "Session 1 Epoch 51 - Train Loss: 0.023557\n",
      "Session 1 Epoch 52 - Train Loss: 0.023225\n",
      "Session 1 Epoch 53 - Train Loss: 0.023377\n",
      "Session 1 Epoch 54 - Train Loss: 0.022870\n",
      "Checkpoint saved for fold 3, session 1, epoch 54\n",
      "Session 1 Epoch 55 - Train Loss: 0.022928\n",
      "Session 1 Epoch 56 - Train Loss: 0.022964\n",
      "Session 1 Epoch 57 - Train Loss: 0.022890\n",
      "Session 1 Epoch 58 - Train Loss: 0.023002\n",
      "Session 1 Epoch 59 - Train Loss: 0.022689\n",
      "Checkpoint saved for fold 3, session 1, epoch 59\n",
      "Session 1 Epoch 60 - Train Loss: 0.023159\n",
      "Session 1 Epoch 61 - Train Loss: 0.022616\n",
      "Session 1 Epoch 62 - Train Loss: 0.023148\n",
      "Session 1 Epoch 63 - Train Loss: 0.022919\n",
      "Session 1 Epoch 64 - Train Loss: 0.022475\n",
      "Checkpoint saved for fold 3, session 1, epoch 64\n",
      "Session 1 Epoch 65 - Train Loss: 0.022643\n",
      "Session 1 Epoch 66 - Train Loss: 0.022733\n",
      "Session 1 Epoch 67 - Train Loss: 0.022687\n",
      "Session 1 Epoch 68 - Train Loss: 0.022825\n",
      "Session 1 Epoch 69 - Train Loss: 0.022898\n",
      "Session 1 Epoch 70 - Train Loss: 0.023019\n",
      "Session 1 Epoch 71 - Train Loss: 0.023231\n",
      "Session 1 Epoch 72 - Train Loss: 0.022911\n",
      "Session 1 Epoch 73 - Train Loss: 0.022819\n",
      "Session 1 Epoch 74 - Train Loss: 0.022456\n",
      "Early stopping at epoch 74 for session 1\n",
      "Training on Session 3/9\n",
      "Session 2 Epoch 1 - Train Loss: 0.277803\n",
      "Checkpoint saved for fold 3, session 2, epoch 1\n",
      "Session 2 Epoch 2 - Train Loss: 0.276718\n",
      "Checkpoint saved for fold 3, session 2, epoch 2\n",
      "Session 2 Epoch 3 - Train Loss: 0.276350\n",
      "Checkpoint saved for fold 3, session 2, epoch 3\n",
      "Session 2 Epoch 4 - Train Loss: 0.275496\n",
      "Checkpoint saved for fold 3, session 2, epoch 4\n",
      "Session 2 Epoch 5 - Train Loss: 0.275271\n",
      "Checkpoint saved for fold 3, session 2, epoch 5\n",
      "Session 2 Epoch 6 - Train Loss: 0.274955\n",
      "Checkpoint saved for fold 3, session 2, epoch 6\n",
      "Session 2 Epoch 7 - Train Loss: 0.274213\n",
      "Checkpoint saved for fold 3, session 2, epoch 7\n",
      "Session 2 Epoch 8 - Train Loss: 0.274192\n",
      "Session 2 Epoch 9 - Train Loss: 0.273553\n",
      "Checkpoint saved for fold 3, session 2, epoch 9\n",
      "Session 2 Epoch 10 - Train Loss: 0.273431\n",
      "Checkpoint saved for fold 3, session 2, epoch 10\n",
      "Session 2 Epoch 11 - Train Loss: 0.273115\n",
      "Checkpoint saved for fold 3, session 2, epoch 11\n",
      "Session 2 Epoch 12 - Train Loss: 0.273200\n",
      "Session 2 Epoch 13 - Train Loss: 0.272851\n",
      "Checkpoint saved for fold 3, session 2, epoch 13\n",
      "Session 2 Epoch 14 - Train Loss: 0.272707\n",
      "Checkpoint saved for fold 3, session 2, epoch 14\n",
      "Session 2 Epoch 15 - Train Loss: 0.272397\n",
      "Checkpoint saved for fold 3, session 2, epoch 15\n",
      "Session 2 Epoch 16 - Train Loss: 0.272257\n",
      "Checkpoint saved for fold 3, session 2, epoch 16\n",
      "Session 2 Epoch 17 - Train Loss: 0.272494\n",
      "Session 2 Epoch 18 - Train Loss: 0.271929\n",
      "Checkpoint saved for fold 3, session 2, epoch 18\n",
      "Session 2 Epoch 19 - Train Loss: 0.271693\n",
      "Checkpoint saved for fold 3, session 2, epoch 19\n",
      "Session 2 Epoch 20 - Train Loss: 0.271717\n",
      "Session 2 Epoch 21 - Train Loss: 0.272380\n",
      "Session 2 Epoch 22 - Train Loss: 0.272013\n",
      "Session 2 Epoch 23 - Train Loss: 0.271635\n",
      "Session 2 Epoch 24 - Train Loss: 0.271791\n",
      "Session 2 Epoch 25 - Train Loss: 0.271782\n",
      "Session 2 Epoch 26 - Train Loss: 0.271731\n",
      "Session 2 Epoch 27 - Train Loss: 0.271729\n",
      "Session 2 Epoch 28 - Train Loss: 0.271611\n",
      "Session 2 Epoch 29 - Train Loss: 0.271230\n",
      "Checkpoint saved for fold 3, session 2, epoch 29\n",
      "Session 2 Epoch 30 - Train Loss: 0.271767\n",
      "Session 2 Epoch 31 - Train Loss: 0.271725\n",
      "Session 2 Epoch 32 - Train Loss: 0.271374\n",
      "Session 2 Epoch 33 - Train Loss: 0.271302\n",
      "Session 2 Epoch 34 - Train Loss: 0.271558\n",
      "Session 2 Epoch 35 - Train Loss: 0.271379\n",
      "Session 2 Epoch 36 - Train Loss: 0.271866\n",
      "Session 2 Epoch 37 - Train Loss: 0.271352\n",
      "Session 2 Epoch 38 - Train Loss: 0.271428\n",
      "Session 2 Epoch 39 - Train Loss: 0.271535\n",
      "Early stopping at epoch 39 for session 2\n",
      "Training on Session 4/9\n",
      "Session 3 Epoch 1 - Train Loss: 0.055044\n",
      "Checkpoint saved for fold 3, session 3, epoch 1\n",
      "Session 3 Epoch 2 - Train Loss: 0.054777\n",
      "Checkpoint saved for fold 3, session 3, epoch 2\n",
      "Session 3 Epoch 3 - Train Loss: 0.054751\n",
      "Session 3 Epoch 4 - Train Loss: 0.054992\n",
      "Session 3 Epoch 5 - Train Loss: 0.054937\n",
      "Session 3 Epoch 6 - Train Loss: 0.054742\n",
      "Session 3 Epoch 7 - Train Loss: 0.055166\n",
      "Session 3 Epoch 8 - Train Loss: 0.055024\n",
      "Session 3 Epoch 9 - Train Loss: 0.054979\n",
      "Session 3 Epoch 10 - Train Loss: 0.055315\n",
      "Session 3 Epoch 11 - Train Loss: 0.055032\n",
      "Session 3 Epoch 12 - Train Loss: 0.054883\n",
      "Early stopping at epoch 12 for session 3\n",
      "Training on Session 5/9\n",
      "Session 4 Epoch 1 - Train Loss: 0.222900\n",
      "Checkpoint saved for fold 3, session 4, epoch 1\n",
      "Session 4 Epoch 2 - Train Loss: 0.222919\n",
      "Session 4 Epoch 3 - Train Loss: 0.222911\n",
      "Session 4 Epoch 4 - Train Loss: 0.222827\n",
      "Session 4 Epoch 5 - Train Loss: 0.222310\n",
      "Checkpoint saved for fold 3, session 4, epoch 5\n",
      "Session 4 Epoch 6 - Train Loss: 0.222341\n",
      "Session 4 Epoch 7 - Train Loss: 0.223250\n",
      "Session 4 Epoch 8 - Train Loss: 0.222871\n",
      "Session 4 Epoch 9 - Train Loss: 0.222189\n",
      "Checkpoint saved for fold 3, session 4, epoch 9\n",
      "Session 4 Epoch 10 - Train Loss: 0.222338\n",
      "Session 4 Epoch 11 - Train Loss: 0.222424\n",
      "Session 4 Epoch 12 - Train Loss: 0.222286\n",
      "Session 4 Epoch 13 - Train Loss: 0.222470\n",
      "Session 4 Epoch 14 - Train Loss: 0.222521\n",
      "Session 4 Epoch 15 - Train Loss: 0.222042\n",
      "Checkpoint saved for fold 3, session 4, epoch 15\n",
      "Session 4 Epoch 16 - Train Loss: 0.222432\n",
      "Session 4 Epoch 17 - Train Loss: 0.222310\n",
      "Session 4 Epoch 18 - Train Loss: 0.222073\n",
      "Session 4 Epoch 19 - Train Loss: 0.222162\n",
      "Session 4 Epoch 20 - Train Loss: 0.222106\n",
      "Session 4 Epoch 21 - Train Loss: 0.222118\n",
      "Session 4 Epoch 22 - Train Loss: 0.222312\n",
      "Session 4 Epoch 23 - Train Loss: 0.222014\n",
      "Session 4 Epoch 24 - Train Loss: 0.221672\n",
      "Checkpoint saved for fold 3, session 4, epoch 24\n",
      "Session 4 Epoch 25 - Train Loss: 0.222038\n",
      "Session 4 Epoch 26 - Train Loss: 0.222075\n",
      "Session 4 Epoch 27 - Train Loss: 0.222118\n",
      "Session 4 Epoch 28 - Train Loss: 0.221750\n",
      "Session 4 Epoch 29 - Train Loss: 0.221530\n",
      "Checkpoint saved for fold 3, session 4, epoch 29\n",
      "Session 4 Epoch 30 - Train Loss: 0.221310\n",
      "Checkpoint saved for fold 3, session 4, epoch 30\n",
      "Session 4 Epoch 31 - Train Loss: 0.222071\n",
      "Session 4 Epoch 32 - Train Loss: 0.222006\n",
      "Session 4 Epoch 33 - Train Loss: 0.221882\n",
      "Session 4 Epoch 34 - Train Loss: 0.221643\n",
      "Session 4 Epoch 35 - Train Loss: 0.221358\n",
      "Session 4 Epoch 36 - Train Loss: 0.221216\n",
      "Session 4 Epoch 37 - Train Loss: 0.221438\n",
      "Session 4 Epoch 38 - Train Loss: 0.221235\n",
      "Session 4 Epoch 39 - Train Loss: 0.221451\n",
      "Session 4 Epoch 40 - Train Loss: 0.220915\n",
      "Checkpoint saved for fold 3, session 4, epoch 40\n",
      "Session 4 Epoch 41 - Train Loss: 0.221322\n",
      "Session 4 Epoch 42 - Train Loss: 0.221316\n",
      "Session 4 Epoch 43 - Train Loss: 0.221389\n",
      "Session 4 Epoch 44 - Train Loss: 0.220690\n",
      "Checkpoint saved for fold 3, session 4, epoch 44\n",
      "Session 4 Epoch 45 - Train Loss: 0.220923\n",
      "Session 4 Epoch 46 - Train Loss: 0.221249\n",
      "Session 4 Epoch 47 - Train Loss: 0.221119\n",
      "Session 4 Epoch 48 - Train Loss: 0.220223\n",
      "Checkpoint saved for fold 3, session 4, epoch 48\n",
      "Session 4 Epoch 49 - Train Loss: 0.220998\n",
      "Session 4 Epoch 50 - Train Loss: 0.220854\n",
      "Session 4 Epoch 51 - Train Loss: 0.220879\n",
      "Session 4 Epoch 52 - Train Loss: 0.220831\n",
      "Session 4 Epoch 53 - Train Loss: 0.220665\n",
      "Session 4 Epoch 54 - Train Loss: 0.220180\n",
      "Session 4 Epoch 55 - Train Loss: 0.220456\n",
      "Session 4 Epoch 56 - Train Loss: 0.220463\n",
      "Session 4 Epoch 57 - Train Loss: 0.220505\n",
      "Session 4 Epoch 58 - Train Loss: 0.220130\n",
      "Early stopping at epoch 58 for session 4\n",
      "Training on Session 6/9\n",
      "Session 5 Epoch 1 - Train Loss: 0.158123\n",
      "Checkpoint saved for fold 3, session 5, epoch 1\n",
      "Session 5 Epoch 2 - Train Loss: 0.157900\n",
      "Checkpoint saved for fold 3, session 5, epoch 2\n",
      "Session 5 Epoch 3 - Train Loss: 0.158224\n",
      "Session 5 Epoch 4 - Train Loss: 0.157662\n",
      "Checkpoint saved for fold 3, session 5, epoch 4\n",
      "Session 5 Epoch 5 - Train Loss: 0.157860\n",
      "Session 5 Epoch 6 - Train Loss: 0.157787\n",
      "Session 5 Epoch 7 - Train Loss: 0.158041\n",
      "Session 5 Epoch 8 - Train Loss: 0.157415\n",
      "Checkpoint saved for fold 3, session 5, epoch 8\n",
      "Session 5 Epoch 9 - Train Loss: 0.157656\n",
      "Session 5 Epoch 10 - Train Loss: 0.157735\n",
      "Session 5 Epoch 11 - Train Loss: 0.157514\n",
      "Session 5 Epoch 12 - Train Loss: 0.157739\n",
      "Session 5 Epoch 13 - Train Loss: 0.157684\n",
      "Session 5 Epoch 14 - Train Loss: 0.157248\n",
      "Checkpoint saved for fold 3, session 5, epoch 14\n",
      "Session 5 Epoch 15 - Train Loss: 0.157417\n",
      "Session 5 Epoch 16 - Train Loss: 0.157541\n",
      "Session 5 Epoch 17 - Train Loss: 0.157239\n",
      "Session 5 Epoch 18 - Train Loss: 0.156913\n",
      "Checkpoint saved for fold 3, session 5, epoch 18\n",
      "Session 5 Epoch 19 - Train Loss: 0.157294\n",
      "Session 5 Epoch 20 - Train Loss: 0.156777\n",
      "Checkpoint saved for fold 3, session 5, epoch 20\n",
      "Session 5 Epoch 21 - Train Loss: 0.157508\n",
      "Session 5 Epoch 22 - Train Loss: 0.156857\n",
      "Session 5 Epoch 23 - Train Loss: 0.157177\n",
      "Session 5 Epoch 24 - Train Loss: 0.157128\n",
      "Session 5 Epoch 25 - Train Loss: 0.156618\n",
      "Checkpoint saved for fold 3, session 5, epoch 25\n",
      "Session 5 Epoch 26 - Train Loss: 0.157304\n",
      "Session 5 Epoch 27 - Train Loss: 0.156790\n",
      "Session 5 Epoch 28 - Train Loss: 0.157207\n",
      "Session 5 Epoch 29 - Train Loss: 0.157204\n",
      "Session 5 Epoch 30 - Train Loss: 0.156851\n",
      "Session 5 Epoch 31 - Train Loss: 0.156619\n",
      "Session 5 Epoch 32 - Train Loss: 0.156791\n",
      "Session 5 Epoch 33 - Train Loss: 0.156439\n",
      "Checkpoint saved for fold 3, session 5, epoch 33\n",
      "Session 5 Epoch 34 - Train Loss: 0.156532\n",
      "Session 5 Epoch 35 - Train Loss: 0.156555\n",
      "Session 5 Epoch 36 - Train Loss: 0.156560\n",
      "Session 5 Epoch 37 - Train Loss: 0.156398\n",
      "Session 5 Epoch 38 - Train Loss: 0.156506\n",
      "Session 5 Epoch 39 - Train Loss: 0.156165\n",
      "Checkpoint saved for fold 3, session 5, epoch 39\n",
      "Session 5 Epoch 40 - Train Loss: 0.156658\n",
      "Session 5 Epoch 41 - Train Loss: 0.156243\n",
      "Session 5 Epoch 42 - Train Loss: 0.156025\n",
      "Checkpoint saved for fold 3, session 5, epoch 42\n",
      "Session 5 Epoch 43 - Train Loss: 0.156084\n",
      "Session 5 Epoch 44 - Train Loss: 0.156571\n",
      "Session 5 Epoch 45 - Train Loss: 0.155939\n",
      "Session 5 Epoch 46 - Train Loss: 0.156081\n",
      "Session 5 Epoch 47 - Train Loss: 0.156029\n",
      "Session 5 Epoch 48 - Train Loss: 0.155720\n",
      "Checkpoint saved for fold 3, session 5, epoch 48\n",
      "Session 5 Epoch 49 - Train Loss: 0.155972\n",
      "Session 5 Epoch 50 - Train Loss: 0.155855\n",
      "Session 5 Epoch 51 - Train Loss: 0.155644\n",
      "Session 5 Epoch 52 - Train Loss: 0.155728\n",
      "Session 5 Epoch 53 - Train Loss: 0.155757\n",
      "Session 5 Epoch 54 - Train Loss: 0.155573\n",
      "Checkpoint saved for fold 3, session 5, epoch 54\n",
      "Session 5 Epoch 55 - Train Loss: 0.155869\n",
      "Session 5 Epoch 56 - Train Loss: 0.155652\n",
      "Session 5 Epoch 57 - Train Loss: 0.155555\n",
      "Session 5 Epoch 58 - Train Loss: 0.155565\n",
      "Session 5 Epoch 59 - Train Loss: 0.155312\n",
      "Checkpoint saved for fold 3, session 5, epoch 59\n",
      "Session 5 Epoch 60 - Train Loss: 0.155461\n",
      "Session 5 Epoch 61 - Train Loss: 0.155002\n",
      "Checkpoint saved for fold 3, session 5, epoch 61\n",
      "Session 5 Epoch 62 - Train Loss: 0.155384\n",
      "Session 5 Epoch 63 - Train Loss: 0.155285\n",
      "Session 5 Epoch 64 - Train Loss: 0.154921\n",
      "Session 5 Epoch 65 - Train Loss: 0.154956\n",
      "Session 5 Epoch 66 - Train Loss: 0.155390\n",
      "Session 5 Epoch 67 - Train Loss: 0.155164\n",
      "Session 5 Epoch 68 - Train Loss: 0.155280\n",
      "Session 5 Epoch 69 - Train Loss: 0.155016\n",
      "Session 5 Epoch 70 - Train Loss: 0.155598\n",
      "Session 5 Epoch 71 - Train Loss: 0.155097\n",
      "Early stopping at epoch 71 for session 5\n",
      "Training on Session 7/9\n",
      "Session 6 Epoch 1 - Train Loss: 0.062805\n",
      "Checkpoint saved for fold 3, session 6, epoch 1\n",
      "Session 6 Epoch 2 - Train Loss: 0.062742\n",
      "Session 6 Epoch 3 - Train Loss: 0.062917\n",
      "Session 6 Epoch 4 - Train Loss: 0.062547\n",
      "Checkpoint saved for fold 3, session 6, epoch 4\n",
      "Session 6 Epoch 5 - Train Loss: 0.062804\n",
      "Session 6 Epoch 6 - Train Loss: 0.062918\n",
      "Session 6 Epoch 7 - Train Loss: 0.062681\n",
      "Session 6 Epoch 8 - Train Loss: 0.062884\n",
      "Session 6 Epoch 9 - Train Loss: 0.062859\n",
      "Session 6 Epoch 10 - Train Loss: 0.062840\n",
      "Session 6 Epoch 11 - Train Loss: 0.062888\n",
      "Session 6 Epoch 12 - Train Loss: 0.062916\n",
      "Session 6 Epoch 13 - Train Loss: 0.062719\n",
      "Session 6 Epoch 14 - Train Loss: 0.062753\n",
      "Early stopping at epoch 14 for session 6\n",
      "Training on Session 8/9\n",
      "Session 7 Epoch 1 - Train Loss: 0.120717\n",
      "Checkpoint saved for fold 3, session 7, epoch 1\n",
      "Session 7 Epoch 2 - Train Loss: 0.120749\n",
      "Session 7 Epoch 3 - Train Loss: 0.120767\n",
      "Session 7 Epoch 4 - Train Loss: 0.120825\n",
      "Session 7 Epoch 5 - Train Loss: 0.120324\n",
      "Checkpoint saved for fold 3, session 7, epoch 5\n",
      "Session 7 Epoch 6 - Train Loss: 0.120762\n",
      "Session 7 Epoch 7 - Train Loss: 0.120410\n",
      "Session 7 Epoch 8 - Train Loss: 0.120888\n",
      "Session 7 Epoch 9 - Train Loss: 0.120587\n",
      "Session 7 Epoch 10 - Train Loss: 0.120533\n",
      "Session 7 Epoch 11 - Train Loss: 0.120815\n",
      "Session 7 Epoch 12 - Train Loss: 0.120586\n",
      "Session 7 Epoch 13 - Train Loss: 0.120558\n",
      "Session 7 Epoch 14 - Train Loss: 0.120313\n",
      "Session 7 Epoch 15 - Train Loss: 0.120460\n",
      "Early stopping at epoch 15 for session 7\n",
      "Training on Session 9/9\n",
      "Session 8 Epoch 1 - Train Loss: 0.079433\n",
      "Checkpoint saved for fold 3, session 8, epoch 1\n",
      "Session 8 Epoch 2 - Train Loss: 0.079414\n",
      "Session 8 Epoch 3 - Train Loss: 0.079221\n",
      "Checkpoint saved for fold 3, session 8, epoch 3\n",
      "Session 8 Epoch 4 - Train Loss: 0.079690\n",
      "Session 8 Epoch 5 - Train Loss: 0.079106\n",
      "Checkpoint saved for fold 3, session 8, epoch 5\n",
      "Session 8 Epoch 6 - Train Loss: 0.079394\n",
      "Session 8 Epoch 7 - Train Loss: 0.078975\n",
      "Checkpoint saved for fold 3, session 8, epoch 7\n",
      "Session 8 Epoch 8 - Train Loss: 0.079372\n",
      "Session 8 Epoch 9 - Train Loss: 0.079699\n",
      "Session 8 Epoch 10 - Train Loss: 0.079302\n",
      "Session 8 Epoch 11 - Train Loss: 0.079270\n",
      "Session 8 Epoch 12 - Train Loss: 0.079306\n",
      "Session 8 Epoch 13 - Train Loss: 0.079388\n",
      "Session 8 Epoch 14 - Train Loss: 0.079330\n",
      "Session 8 Epoch 15 - Train Loss: 0.079177\n",
      "Session 8 Epoch 16 - Train Loss: 0.079213\n",
      "Session 8 Epoch 17 - Train Loss: 0.079314\n",
      "Early stopping at epoch 17 for session 8\n",
      "Fold 3 - Test Loss: 0.1421, R^2: -3469154865284.7056\n",
      "\n",
      "=== Fold 4 ===\n",
      "Training on Session 1/9\n",
      "Session 0 Epoch 1 - Train Loss: 0.054452\n",
      "Checkpoint saved for fold 4, session 0, epoch 1\n",
      "Session 0 Epoch 2 - Train Loss: 0.053309\n",
      "Checkpoint saved for fold 4, session 0, epoch 2\n",
      "Session 0 Epoch 3 - Train Loss: 0.051034\n",
      "Checkpoint saved for fold 4, session 0, epoch 3\n",
      "Session 0 Epoch 4 - Train Loss: 0.044491\n",
      "Checkpoint saved for fold 4, session 0, epoch 4\n",
      "Session 0 Epoch 5 - Train Loss: 0.036729\n",
      "Checkpoint saved for fold 4, session 0, epoch 5\n",
      "Session 0 Epoch 6 - Train Loss: 0.032906\n",
      "Checkpoint saved for fold 4, session 0, epoch 6\n",
      "Session 0 Epoch 7 - Train Loss: 0.032256\n",
      "Checkpoint saved for fold 4, session 0, epoch 7\n",
      "Session 0 Epoch 8 - Train Loss: 0.031618\n",
      "Checkpoint saved for fold 4, session 0, epoch 8\n",
      "Session 0 Epoch 9 - Train Loss: 0.028196\n",
      "Checkpoint saved for fold 4, session 0, epoch 9\n",
      "Session 0 Epoch 10 - Train Loss: 0.028119\n",
      "Session 0 Epoch 11 - Train Loss: 0.026514\n",
      "Checkpoint saved for fold 4, session 0, epoch 11\n",
      "Session 0 Epoch 12 - Train Loss: 0.025811\n",
      "Checkpoint saved for fold 4, session 0, epoch 12\n",
      "Session 0 Epoch 13 - Train Loss: 0.025538\n",
      "Checkpoint saved for fold 4, session 0, epoch 13\n",
      "Session 0 Epoch 14 - Train Loss: 0.024527\n",
      "Checkpoint saved for fold 4, session 0, epoch 14\n",
      "Session 0 Epoch 15 - Train Loss: 0.023349\n",
      "Checkpoint saved for fold 4, session 0, epoch 15\n",
      "Session 0 Epoch 16 - Train Loss: 0.021979\n",
      "Checkpoint saved for fold 4, session 0, epoch 16\n",
      "Session 0 Epoch 17 - Train Loss: 0.022004\n",
      "Session 0 Epoch 18 - Train Loss: 0.019776\n",
      "Checkpoint saved for fold 4, session 0, epoch 18\n",
      "Session 0 Epoch 19 - Train Loss: 0.019690\n",
      "Session 0 Epoch 20 - Train Loss: 0.019326\n",
      "Checkpoint saved for fold 4, session 0, epoch 20\n",
      "Session 0 Epoch 21 - Train Loss: 0.018232\n",
      "Checkpoint saved for fold 4, session 0, epoch 21\n",
      "Session 0 Epoch 22 - Train Loss: 0.018265\n",
      "Session 0 Epoch 23 - Train Loss: 0.017912\n",
      "Checkpoint saved for fold 4, session 0, epoch 23\n",
      "Session 0 Epoch 24 - Train Loss: 0.017562\n",
      "Checkpoint saved for fold 4, session 0, epoch 24\n",
      "Session 0 Epoch 25 - Train Loss: 0.017351\n",
      "Checkpoint saved for fold 4, session 0, epoch 25\n",
      "Session 0 Epoch 26 - Train Loss: 0.017352\n",
      "Session 0 Epoch 27 - Train Loss: 0.016703\n",
      "Checkpoint saved for fold 4, session 0, epoch 27\n",
      "Session 0 Epoch 28 - Train Loss: 0.016149\n",
      "Checkpoint saved for fold 4, session 0, epoch 28\n",
      "Session 0 Epoch 29 - Train Loss: 0.016119\n",
      "Session 0 Epoch 30 - Train Loss: 0.015902\n",
      "Checkpoint saved for fold 4, session 0, epoch 30\n",
      "Session 0 Epoch 31 - Train Loss: 0.015449\n",
      "Checkpoint saved for fold 4, session 0, epoch 31\n",
      "Session 0 Epoch 32 - Train Loss: 0.015182\n",
      "Checkpoint saved for fold 4, session 0, epoch 32\n",
      "Session 0 Epoch 33 - Train Loss: 0.014658\n",
      "Checkpoint saved for fold 4, session 0, epoch 33\n",
      "Session 0 Epoch 34 - Train Loss: 0.014963\n",
      "Session 0 Epoch 35 - Train Loss: 0.013981\n",
      "Checkpoint saved for fold 4, session 0, epoch 35\n",
      "Session 0 Epoch 36 - Train Loss: 0.013569\n",
      "Checkpoint saved for fold 4, session 0, epoch 36\n",
      "Session 0 Epoch 37 - Train Loss: 0.014243\n",
      "Session 0 Epoch 38 - Train Loss: 0.013474\n",
      "Session 0 Epoch 39 - Train Loss: 0.013344\n",
      "Checkpoint saved for fold 4, session 0, epoch 39\n",
      "Session 0 Epoch 40 - Train Loss: 0.013129\n",
      "Checkpoint saved for fold 4, session 0, epoch 40\n",
      "Session 0 Epoch 41 - Train Loss: 0.013074\n",
      "Session 0 Epoch 42 - Train Loss: 0.012752\n",
      "Checkpoint saved for fold 4, session 0, epoch 42\n",
      "Session 0 Epoch 43 - Train Loss: 0.011962\n",
      "Checkpoint saved for fold 4, session 0, epoch 43\n",
      "Session 0 Epoch 44 - Train Loss: 0.012207\n",
      "Session 0 Epoch 45 - Train Loss: 0.011317\n",
      "Checkpoint saved for fold 4, session 0, epoch 45\n",
      "Session 0 Epoch 46 - Train Loss: 0.011617\n",
      "Session 0 Epoch 47 - Train Loss: 0.010989\n",
      "Checkpoint saved for fold 4, session 0, epoch 47\n",
      "Session 0 Epoch 48 - Train Loss: 0.011089\n",
      "Session 0 Epoch 49 - Train Loss: 0.010161\n",
      "Checkpoint saved for fold 4, session 0, epoch 49\n",
      "Session 0 Epoch 50 - Train Loss: 0.010240\n",
      "Session 0 Epoch 51 - Train Loss: 0.009522\n",
      "Checkpoint saved for fold 4, session 0, epoch 51\n",
      "Session 0 Epoch 52 - Train Loss: 0.010202\n",
      "Session 0 Epoch 53 - Train Loss: 0.010818\n",
      "Session 0 Epoch 54 - Train Loss: 0.009398\n",
      "Checkpoint saved for fold 4, session 0, epoch 54\n",
      "Session 0 Epoch 55 - Train Loss: 0.009063\n",
      "Checkpoint saved for fold 4, session 0, epoch 55\n",
      "Session 0 Epoch 56 - Train Loss: 0.009235\n",
      "Session 0 Epoch 57 - Train Loss: 0.009225\n",
      "Session 0 Epoch 58 - Train Loss: 0.008870\n",
      "Checkpoint saved for fold 4, session 0, epoch 58\n",
      "Session 0 Epoch 59 - Train Loss: 0.008546\n",
      "Checkpoint saved for fold 4, session 0, epoch 59\n",
      "Session 0 Epoch 60 - Train Loss: 0.008685\n",
      "Session 0 Epoch 61 - Train Loss: 0.008447\n",
      "Session 0 Epoch 62 - Train Loss: 0.008761\n",
      "Session 0 Epoch 63 - Train Loss: 0.007861\n",
      "Checkpoint saved for fold 4, session 0, epoch 63\n",
      "Session 0 Epoch 64 - Train Loss: 0.008495\n",
      "Session 0 Epoch 65 - Train Loss: 0.007486\n",
      "Checkpoint saved for fold 4, session 0, epoch 65\n",
      "Session 0 Epoch 66 - Train Loss: 0.007690\n",
      "Session 0 Epoch 67 - Train Loss: 0.007037\n",
      "Checkpoint saved for fold 4, session 0, epoch 67\n",
      "Session 0 Epoch 68 - Train Loss: 0.006906\n",
      "Checkpoint saved for fold 4, session 0, epoch 68\n",
      "Session 0 Epoch 69 - Train Loss: 0.007318\n",
      "Session 0 Epoch 70 - Train Loss: 0.006940\n",
      "Session 0 Epoch 71 - Train Loss: 0.006695\n",
      "Checkpoint saved for fold 4, session 0, epoch 71\n",
      "Session 0 Epoch 72 - Train Loss: 0.006626\n",
      "Session 0 Epoch 73 - Train Loss: 0.006870\n",
      "Session 0 Epoch 74 - Train Loss: 0.005971\n",
      "Checkpoint saved for fold 4, session 0, epoch 74\n",
      "Session 0 Epoch 75 - Train Loss: 0.006466\n",
      "Session 0 Epoch 76 - Train Loss: 0.006634\n",
      "Session 0 Epoch 77 - Train Loss: 0.006219\n",
      "Session 0 Epoch 78 - Train Loss: 0.005807\n",
      "Checkpoint saved for fold 4, session 0, epoch 78\n",
      "Session 0 Epoch 79 - Train Loss: 0.005776\n",
      "Session 0 Epoch 80 - Train Loss: 0.005912\n",
      "Training on Session 2/9\n",
      "Session 1 Epoch 1 - Train Loss: 0.051219\n",
      "Checkpoint saved for fold 4, session 1, epoch 1\n",
      "Session 1 Epoch 2 - Train Loss: 0.050857\n",
      "Checkpoint saved for fold 4, session 1, epoch 2\n",
      "Session 1 Epoch 3 - Train Loss: 0.050820\n",
      "Session 1 Epoch 4 - Train Loss: 0.050492\n",
      "Checkpoint saved for fold 4, session 1, epoch 4\n",
      "Session 1 Epoch 5 - Train Loss: 0.046599\n",
      "Checkpoint saved for fold 4, session 1, epoch 5\n",
      "Session 1 Epoch 6 - Train Loss: 0.037377\n",
      "Checkpoint saved for fold 4, session 1, epoch 6\n",
      "Session 1 Epoch 7 - Train Loss: 0.031290\n",
      "Checkpoint saved for fold 4, session 1, epoch 7\n",
      "Session 1 Epoch 8 - Train Loss: 0.028462\n",
      "Checkpoint saved for fold 4, session 1, epoch 8\n",
      "Session 1 Epoch 9 - Train Loss: 0.027241\n",
      "Checkpoint saved for fold 4, session 1, epoch 9\n",
      "Session 1 Epoch 10 - Train Loss: 0.023708\n",
      "Checkpoint saved for fold 4, session 1, epoch 10\n",
      "Session 1 Epoch 11 - Train Loss: 0.020804\n",
      "Checkpoint saved for fold 4, session 1, epoch 11\n",
      "Session 1 Epoch 12 - Train Loss: 0.019095\n",
      "Checkpoint saved for fold 4, session 1, epoch 12\n",
      "Session 1 Epoch 13 - Train Loss: 0.017892\n",
      "Checkpoint saved for fold 4, session 1, epoch 13\n",
      "Session 1 Epoch 14 - Train Loss: 0.017607\n",
      "Checkpoint saved for fold 4, session 1, epoch 14\n",
      "Session 1 Epoch 15 - Train Loss: 0.017123\n",
      "Checkpoint saved for fold 4, session 1, epoch 15\n",
      "Session 1 Epoch 16 - Train Loss: 0.016339\n",
      "Checkpoint saved for fold 4, session 1, epoch 16\n",
      "Session 1 Epoch 17 - Train Loss: 0.015970\n",
      "Checkpoint saved for fold 4, session 1, epoch 17\n",
      "Session 1 Epoch 18 - Train Loss: 0.014691\n",
      "Checkpoint saved for fold 4, session 1, epoch 18\n",
      "Session 1 Epoch 19 - Train Loss: 0.013963\n",
      "Checkpoint saved for fold 4, session 1, epoch 19\n",
      "Session 1 Epoch 20 - Train Loss: 0.013972\n",
      "Session 1 Epoch 21 - Train Loss: 0.013738\n",
      "Checkpoint saved for fold 4, session 1, epoch 21\n",
      "Session 1 Epoch 22 - Train Loss: 0.013304\n",
      "Checkpoint saved for fold 4, session 1, epoch 22\n",
      "Session 1 Epoch 23 - Train Loss: 0.013278\n",
      "Session 1 Epoch 24 - Train Loss: 0.012684\n",
      "Checkpoint saved for fold 4, session 1, epoch 24\n",
      "Session 1 Epoch 25 - Train Loss: 0.012494\n",
      "Checkpoint saved for fold 4, session 1, epoch 25\n",
      "Session 1 Epoch 26 - Train Loss: 0.012324\n",
      "Checkpoint saved for fold 4, session 1, epoch 26\n",
      "Session 1 Epoch 27 - Train Loss: 0.012567\n",
      "Session 1 Epoch 28 - Train Loss: 0.012088\n",
      "Checkpoint saved for fold 4, session 1, epoch 28\n",
      "Session 1 Epoch 29 - Train Loss: 0.012298\n",
      "Session 1 Epoch 30 - Train Loss: 0.011834\n",
      "Checkpoint saved for fold 4, session 1, epoch 30\n",
      "Session 1 Epoch 31 - Train Loss: 0.012117\n",
      "Session 1 Epoch 32 - Train Loss: 0.012104\n",
      "Session 1 Epoch 33 - Train Loss: 0.012044\n",
      "Session 1 Epoch 34 - Train Loss: 0.011987\n",
      "Session 1 Epoch 35 - Train Loss: 0.012220\n",
      "Session 1 Epoch 36 - Train Loss: 0.011716\n",
      "Checkpoint saved for fold 4, session 1, epoch 36\n",
      "Session 1 Epoch 37 - Train Loss: 0.011940\n",
      "Session 1 Epoch 38 - Train Loss: 0.011877\n",
      "Session 1 Epoch 39 - Train Loss: 0.011938\n",
      "Session 1 Epoch 40 - Train Loss: 0.011773\n",
      "Session 1 Epoch 41 - Train Loss: 0.011975\n",
      "Session 1 Epoch 42 - Train Loss: 0.011582\n",
      "Checkpoint saved for fold 4, session 1, epoch 42\n",
      "Session 1 Epoch 43 - Train Loss: 0.011526\n",
      "Session 1 Epoch 44 - Train Loss: 0.011609\n",
      "Session 1 Epoch 45 - Train Loss: 0.011738\n",
      "Session 1 Epoch 46 - Train Loss: 0.011884\n",
      "Session 1 Epoch 47 - Train Loss: 0.011875\n",
      "Session 1 Epoch 48 - Train Loss: 0.011639\n",
      "Session 1 Epoch 49 - Train Loss: 0.011770\n",
      "Session 1 Epoch 50 - Train Loss: 0.011586\n",
      "Session 1 Epoch 51 - Train Loss: 0.011829\n",
      "Session 1 Epoch 52 - Train Loss: 0.011410\n",
      "Checkpoint saved for fold 4, session 1, epoch 52\n",
      "Session 1 Epoch 53 - Train Loss: 0.011776\n",
      "Session 1 Epoch 54 - Train Loss: 0.011745\n",
      "Session 1 Epoch 55 - Train Loss: 0.011555\n",
      "Session 1 Epoch 56 - Train Loss: 0.011662\n",
      "Session 1 Epoch 57 - Train Loss: 0.011852\n",
      "Session 1 Epoch 58 - Train Loss: 0.011838\n",
      "Session 1 Epoch 59 - Train Loss: 0.011640\n",
      "Session 1 Epoch 60 - Train Loss: 0.011759\n",
      "Session 1 Epoch 61 - Train Loss: 0.011540\n",
      "Session 1 Epoch 62 - Train Loss: 0.011880\n",
      "Early stopping at epoch 62 for session 1\n",
      "Training on Session 3/9\n",
      "Session 2 Epoch 1 - Train Loss: 0.277891\n",
      "Checkpoint saved for fold 4, session 2, epoch 1\n",
      "Session 2 Epoch 2 - Train Loss: 0.273680\n",
      "Checkpoint saved for fold 4, session 2, epoch 2\n",
      "Session 2 Epoch 3 - Train Loss: 0.271655\n",
      "Checkpoint saved for fold 4, session 2, epoch 3\n",
      "Session 2 Epoch 4 - Train Loss: 0.269665\n",
      "Checkpoint saved for fold 4, session 2, epoch 4\n",
      "Session 2 Epoch 5 - Train Loss: 0.268733\n",
      "Checkpoint saved for fold 4, session 2, epoch 5\n",
      "Session 2 Epoch 6 - Train Loss: 0.267814\n",
      "Checkpoint saved for fold 4, session 2, epoch 6\n",
      "Session 2 Epoch 7 - Train Loss: 0.266830\n",
      "Checkpoint saved for fold 4, session 2, epoch 7\n",
      "Session 2 Epoch 8 - Train Loss: 0.266046\n",
      "Checkpoint saved for fold 4, session 2, epoch 8\n",
      "Session 2 Epoch 9 - Train Loss: 0.265080\n",
      "Checkpoint saved for fold 4, session 2, epoch 9\n",
      "Session 2 Epoch 10 - Train Loss: 0.264039\n",
      "Checkpoint saved for fold 4, session 2, epoch 10\n",
      "Session 2 Epoch 11 - Train Loss: 0.263343\n",
      "Checkpoint saved for fold 4, session 2, epoch 11\n",
      "Session 2 Epoch 12 - Train Loss: 0.263049\n",
      "Checkpoint saved for fold 4, session 2, epoch 12\n",
      "Session 2 Epoch 13 - Train Loss: 0.262393\n",
      "Checkpoint saved for fold 4, session 2, epoch 13\n",
      "Session 2 Epoch 14 - Train Loss: 0.262054\n",
      "Checkpoint saved for fold 4, session 2, epoch 14\n",
      "Session 2 Epoch 15 - Train Loss: 0.261679\n",
      "Checkpoint saved for fold 4, session 2, epoch 15\n",
      "Session 2 Epoch 16 - Train Loss: 0.261040\n",
      "Checkpoint saved for fold 4, session 2, epoch 16\n",
      "Session 2 Epoch 17 - Train Loss: 0.260691\n",
      "Checkpoint saved for fold 4, session 2, epoch 17\n",
      "Session 2 Epoch 18 - Train Loss: 0.260558\n",
      "Checkpoint saved for fold 4, session 2, epoch 18\n",
      "Session 2 Epoch 19 - Train Loss: 0.260444\n",
      "Checkpoint saved for fold 4, session 2, epoch 19\n",
      "Session 2 Epoch 20 - Train Loss: 0.259987\n",
      "Checkpoint saved for fold 4, session 2, epoch 20\n",
      "Session 2 Epoch 21 - Train Loss: 0.259931\n",
      "Session 2 Epoch 22 - Train Loss: 0.259775\n",
      "Checkpoint saved for fold 4, session 2, epoch 22\n",
      "Session 2 Epoch 23 - Train Loss: 0.259450\n",
      "Checkpoint saved for fold 4, session 2, epoch 23\n",
      "Session 2 Epoch 24 - Train Loss: 0.259773\n",
      "Session 2 Epoch 25 - Train Loss: 0.259586\n",
      "Session 2 Epoch 26 - Train Loss: 0.259687\n",
      "Session 2 Epoch 27 - Train Loss: 0.259353\n",
      "Session 2 Epoch 28 - Train Loss: 0.259035\n",
      "Checkpoint saved for fold 4, session 2, epoch 28\n",
      "Session 2 Epoch 29 - Train Loss: 0.259122\n",
      "Session 2 Epoch 30 - Train Loss: 0.259029\n",
      "Session 2 Epoch 31 - Train Loss: 0.258973\n",
      "Session 2 Epoch 32 - Train Loss: 0.258889\n",
      "Checkpoint saved for fold 4, session 2, epoch 32\n",
      "Session 2 Epoch 33 - Train Loss: 0.259142\n",
      "Session 2 Epoch 34 - Train Loss: 0.258665\n",
      "Checkpoint saved for fold 4, session 2, epoch 34\n",
      "Session 2 Epoch 35 - Train Loss: 0.258656\n",
      "Session 2 Epoch 36 - Train Loss: 0.258624\n",
      "Session 2 Epoch 37 - Train Loss: 0.258560\n",
      "Checkpoint saved for fold 4, session 2, epoch 37\n",
      "Session 2 Epoch 38 - Train Loss: 0.258780\n",
      "Session 2 Epoch 39 - Train Loss: 0.258776\n",
      "Session 2 Epoch 40 - Train Loss: 0.258618\n",
      "Session 2 Epoch 41 - Train Loss: 0.258733\n",
      "Session 2 Epoch 42 - Train Loss: 0.258713\n",
      "Session 2 Epoch 43 - Train Loss: 0.258458\n",
      "Checkpoint saved for fold 4, session 2, epoch 43\n",
      "Session 2 Epoch 44 - Train Loss: 0.258429\n",
      "Session 2 Epoch 45 - Train Loss: 0.258249\n",
      "Checkpoint saved for fold 4, session 2, epoch 45\n",
      "Session 2 Epoch 46 - Train Loss: 0.258272\n",
      "Session 2 Epoch 47 - Train Loss: 0.258424\n",
      "Session 2 Epoch 48 - Train Loss: 0.258156\n",
      "Session 2 Epoch 49 - Train Loss: 0.258020\n",
      "Checkpoint saved for fold 4, session 2, epoch 49\n",
      "Session 2 Epoch 50 - Train Loss: 0.258305\n",
      "Session 2 Epoch 51 - Train Loss: 0.258162\n",
      "Session 2 Epoch 52 - Train Loss: 0.258360\n",
      "Session 2 Epoch 53 - Train Loss: 0.258119\n",
      "Session 2 Epoch 54 - Train Loss: 0.258209\n",
      "Session 2 Epoch 55 - Train Loss: 0.258266\n",
      "Session 2 Epoch 56 - Train Loss: 0.258227\n",
      "Session 2 Epoch 57 - Train Loss: 0.258306\n",
      "Session 2 Epoch 58 - Train Loss: 0.258025\n",
      "Session 2 Epoch 59 - Train Loss: 0.258105\n",
      "Early stopping at epoch 59 for session 2\n",
      "Training on Session 4/9\n",
      "Session 3 Epoch 1 - Train Loss: 0.152202\n",
      "Checkpoint saved for fold 4, session 3, epoch 1\n",
      "Session 3 Epoch 2 - Train Loss: 0.152509\n",
      "Session 3 Epoch 3 - Train Loss: 0.152435\n",
      "Session 3 Epoch 4 - Train Loss: 0.152158\n",
      "Session 3 Epoch 5 - Train Loss: 0.152159\n",
      "Session 3 Epoch 6 - Train Loss: 0.152245\n",
      "Session 3 Epoch 7 - Train Loss: 0.152650\n",
      "Session 3 Epoch 8 - Train Loss: 0.152044\n",
      "Checkpoint saved for fold 4, session 3, epoch 8\n",
      "Session 3 Epoch 9 - Train Loss: 0.151917\n",
      "Checkpoint saved for fold 4, session 3, epoch 9\n",
      "Session 3 Epoch 10 - Train Loss: 0.152457\n",
      "Session 3 Epoch 11 - Train Loss: 0.152692\n",
      "Session 3 Epoch 12 - Train Loss: 0.152050\n",
      "Session 3 Epoch 13 - Train Loss: 0.151947\n",
      "Session 3 Epoch 14 - Train Loss: 0.152201\n",
      "Session 3 Epoch 15 - Train Loss: 0.152047\n",
      "Session 3 Epoch 16 - Train Loss: 0.152166\n",
      "Session 3 Epoch 17 - Train Loss: 0.152263\n",
      "Session 3 Epoch 18 - Train Loss: 0.151878\n",
      "Session 3 Epoch 19 - Train Loss: 0.152194\n",
      "Early stopping at epoch 19 for session 3\n",
      "Training on Session 5/9\n",
      "Session 4 Epoch 1 - Train Loss: 0.229003\n",
      "Checkpoint saved for fold 4, session 4, epoch 1\n",
      "Session 4 Epoch 2 - Train Loss: 0.229088\n",
      "Session 4 Epoch 3 - Train Loss: 0.229360\n",
      "Session 4 Epoch 4 - Train Loss: 0.228839\n",
      "Checkpoint saved for fold 4, session 4, epoch 4\n",
      "Session 4 Epoch 5 - Train Loss: 0.229348\n",
      "Session 4 Epoch 6 - Train Loss: 0.229020\n",
      "Session 4 Epoch 7 - Train Loss: 0.229065\n",
      "Session 4 Epoch 8 - Train Loss: 0.228987\n",
      "Session 4 Epoch 9 - Train Loss: 0.228779\n",
      "Session 4 Epoch 10 - Train Loss: 0.228748\n",
      "Session 4 Epoch 11 - Train Loss: 0.228900\n",
      "Session 4 Epoch 12 - Train Loss: 0.228643\n",
      "Checkpoint saved for fold 4, session 4, epoch 12\n",
      "Session 4 Epoch 13 - Train Loss: 0.228666\n",
      "Session 4 Epoch 14 - Train Loss: 0.228721\n",
      "Session 4 Epoch 15 - Train Loss: 0.228729\n",
      "Session 4 Epoch 16 - Train Loss: 0.228593\n",
      "Session 4 Epoch 17 - Train Loss: 0.228902\n",
      "Session 4 Epoch 18 - Train Loss: 0.228934\n",
      "Session 4 Epoch 19 - Train Loss: 0.228549\n",
      "Session 4 Epoch 20 - Train Loss: 0.228754\n",
      "Session 4 Epoch 21 - Train Loss: 0.228399\n",
      "Checkpoint saved for fold 4, session 4, epoch 21\n",
      "Session 4 Epoch 22 - Train Loss: 0.228496\n",
      "Session 4 Epoch 23 - Train Loss: 0.228550\n",
      "Session 4 Epoch 24 - Train Loss: 0.228434\n",
      "Session 4 Epoch 25 - Train Loss: 0.228724\n",
      "Session 4 Epoch 26 - Train Loss: 0.228459\n",
      "Session 4 Epoch 27 - Train Loss: 0.228521\n",
      "Session 4 Epoch 28 - Train Loss: 0.228662\n",
      "Session 4 Epoch 29 - Train Loss: 0.228317\n",
      "Session 4 Epoch 30 - Train Loss: 0.228242\n",
      "Checkpoint saved for fold 4, session 4, epoch 30\n",
      "Session 4 Epoch 31 - Train Loss: 0.228523\n",
      "Session 4 Epoch 32 - Train Loss: 0.228461\n",
      "Session 4 Epoch 33 - Train Loss: 0.228409\n",
      "Session 4 Epoch 34 - Train Loss: 0.228244\n",
      "Session 4 Epoch 35 - Train Loss: 0.228284\n",
      "Session 4 Epoch 36 - Train Loss: 0.228516\n",
      "Session 4 Epoch 37 - Train Loss: 0.228410\n",
      "Session 4 Epoch 38 - Train Loss: 0.228530\n",
      "Session 4 Epoch 39 - Train Loss: 0.228146\n",
      "Session 4 Epoch 40 - Train Loss: 0.228297\n",
      "Early stopping at epoch 40 for session 4\n",
      "Training on Session 6/9\n",
      "Session 5 Epoch 1 - Train Loss: 0.164977\n",
      "Checkpoint saved for fold 4, session 5, epoch 1\n",
      "Session 5 Epoch 2 - Train Loss: 0.165307\n",
      "Session 5 Epoch 3 - Train Loss: 0.164846\n",
      "Checkpoint saved for fold 4, session 5, epoch 3\n",
      "Session 5 Epoch 4 - Train Loss: 0.165090\n",
      "Session 5 Epoch 5 - Train Loss: 0.164830\n",
      "Session 5 Epoch 6 - Train Loss: 0.165180\n",
      "Session 5 Epoch 7 - Train Loss: 0.164967\n",
      "Session 5 Epoch 8 - Train Loss: 0.164882\n",
      "Session 5 Epoch 9 - Train Loss: 0.164683\n",
      "Checkpoint saved for fold 4, session 5, epoch 9\n",
      "Session 5 Epoch 10 - Train Loss: 0.164538\n",
      "Checkpoint saved for fold 4, session 5, epoch 10\n",
      "Session 5 Epoch 11 - Train Loss: 0.164858\n",
      "Session 5 Epoch 12 - Train Loss: 0.164714\n",
      "Session 5 Epoch 13 - Train Loss: 0.164447\n",
      "Session 5 Epoch 14 - Train Loss: 0.164899\n",
      "Session 5 Epoch 15 - Train Loss: 0.164694\n",
      "Session 5 Epoch 16 - Train Loss: 0.164360\n",
      "Checkpoint saved for fold 4, session 5, epoch 16\n",
      "Session 5 Epoch 17 - Train Loss: 0.164462\n",
      "Session 5 Epoch 18 - Train Loss: 0.164576\n",
      "Session 5 Epoch 19 - Train Loss: 0.164589\n",
      "Session 5 Epoch 20 - Train Loss: 0.164632\n",
      "Session 5 Epoch 21 - Train Loss: 0.164665\n",
      "Session 5 Epoch 22 - Train Loss: 0.164380\n",
      "Session 5 Epoch 23 - Train Loss: 0.164947\n",
      "Session 5 Epoch 24 - Train Loss: 0.164559\n",
      "Session 5 Epoch 25 - Train Loss: 0.164767\n",
      "Session 5 Epoch 26 - Train Loss: 0.164360\n",
      "Early stopping at epoch 26 for session 5\n",
      "Training on Session 7/9\n",
      "Session 6 Epoch 1 - Train Loss: 0.063071\n",
      "Checkpoint saved for fold 4, session 6, epoch 1\n",
      "Session 6 Epoch 2 - Train Loss: 0.063057\n",
      "Session 6 Epoch 3 - Train Loss: 0.062929\n",
      "Checkpoint saved for fold 4, session 6, epoch 3\n",
      "Session 6 Epoch 4 - Train Loss: 0.062905\n",
      "Session 6 Epoch 5 - Train Loss: 0.063088\n",
      "Session 6 Epoch 6 - Train Loss: 0.062876\n",
      "Session 6 Epoch 7 - Train Loss: 0.062908\n",
      "Session 6 Epoch 8 - Train Loss: 0.063011\n",
      "Session 6 Epoch 9 - Train Loss: 0.062961\n",
      "Session 6 Epoch 10 - Train Loss: 0.062933\n",
      "Session 6 Epoch 11 - Train Loss: 0.063195\n",
      "Session 6 Epoch 12 - Train Loss: 0.062948\n",
      "Session 6 Epoch 13 - Train Loss: 0.063032\n",
      "Early stopping at epoch 13 for session 6\n",
      "Training on Session 8/9\n",
      "Session 7 Epoch 1 - Train Loss: 0.124652\n",
      "Checkpoint saved for fold 4, session 7, epoch 1\n",
      "Session 7 Epoch 2 - Train Loss: 0.124374\n",
      "Checkpoint saved for fold 4, session 7, epoch 2\n",
      "Session 7 Epoch 3 - Train Loss: 0.124480\n",
      "Session 7 Epoch 4 - Train Loss: 0.124466\n",
      "Session 7 Epoch 5 - Train Loss: 0.124505\n",
      "Session 7 Epoch 6 - Train Loss: 0.124073\n",
      "Checkpoint saved for fold 4, session 7, epoch 6\n",
      "Session 7 Epoch 7 - Train Loss: 0.124733\n",
      "Session 7 Epoch 8 - Train Loss: 0.123801\n",
      "Checkpoint saved for fold 4, session 7, epoch 8\n",
      "Session 7 Epoch 9 - Train Loss: 0.124420\n",
      "Session 7 Epoch 10 - Train Loss: 0.123811\n",
      "Session 7 Epoch 11 - Train Loss: 0.124496\n",
      "Session 7 Epoch 12 - Train Loss: 0.124273\n",
      "Session 7 Epoch 13 - Train Loss: 0.123912\n",
      "Session 7 Epoch 14 - Train Loss: 0.123978\n",
      "Session 7 Epoch 15 - Train Loss: 0.124175\n",
      "Session 7 Epoch 16 - Train Loss: 0.123934\n",
      "Session 7 Epoch 17 - Train Loss: 0.124165\n",
      "Session 7 Epoch 18 - Train Loss: 0.124060\n",
      "Early stopping at epoch 18 for session 7\n",
      "Training on Session 9/9\n",
      "Session 8 Epoch 1 - Train Loss: 0.082940\n",
      "Checkpoint saved for fold 4, session 8, epoch 1\n",
      "Session 8 Epoch 2 - Train Loss: 0.083275\n",
      "Session 8 Epoch 3 - Train Loss: 0.082990\n",
      "Session 8 Epoch 4 - Train Loss: 0.083034\n",
      "Session 8 Epoch 5 - Train Loss: 0.082868\n",
      "Session 8 Epoch 6 - Train Loss: 0.082744\n",
      "Checkpoint saved for fold 4, session 8, epoch 6\n",
      "Session 8 Epoch 7 - Train Loss: 0.083266\n",
      "Session 8 Epoch 8 - Train Loss: 0.082858\n",
      "Session 8 Epoch 9 - Train Loss: 0.082968\n",
      "Session 8 Epoch 10 - Train Loss: 0.082729\n",
      "Session 8 Epoch 11 - Train Loss: 0.082830\n",
      "Session 8 Epoch 12 - Train Loss: 0.082969\n",
      "Session 8 Epoch 13 - Train Loss: 0.082819\n",
      "Session 8 Epoch 14 - Train Loss: 0.082986\n",
      "Session 8 Epoch 15 - Train Loss: 0.082640\n",
      "Checkpoint saved for fold 4, session 8, epoch 15\n",
      "Session 8 Epoch 16 - Train Loss: 0.082755\n",
      "Session 8 Epoch 17 - Train Loss: 0.082689\n",
      "Session 8 Epoch 18 - Train Loss: 0.082766\n",
      "Session 8 Epoch 19 - Train Loss: 0.083072\n",
      "Session 8 Epoch 20 - Train Loss: 0.082347\n",
      "Checkpoint saved for fold 4, session 8, epoch 20\n",
      "Session 8 Epoch 21 - Train Loss: 0.082798\n",
      "Session 8 Epoch 22 - Train Loss: 0.082931\n",
      "Session 8 Epoch 23 - Train Loss: 0.082408\n",
      "Session 8 Epoch 24 - Train Loss: 0.082634\n",
      "Session 8 Epoch 25 - Train Loss: 0.082485\n",
      "Session 8 Epoch 26 - Train Loss: 0.082793\n",
      "Session 8 Epoch 27 - Train Loss: 0.082676\n",
      "Session 8 Epoch 28 - Train Loss: 0.082681\n",
      "Session 8 Epoch 29 - Train Loss: 0.082360\n",
      "Session 8 Epoch 30 - Train Loss: 0.082603\n",
      "Early stopping at epoch 30 for session 8\n",
      "Fold 4 - Test Loss: 0.0547, R^2: -153035927648403.3750\n",
      "\n",
      "=== Fold 5 ===\n",
      "Training on Session 1/9\n",
      "Session 0 Epoch 1 - Train Loss: 0.054594\n",
      "Checkpoint saved for fold 5, session 0, epoch 1\n",
      "Session 0 Epoch 2 - Train Loss: 0.053094\n",
      "Checkpoint saved for fold 5, session 0, epoch 2\n",
      "Session 0 Epoch 3 - Train Loss: 0.050134\n",
      "Checkpoint saved for fold 5, session 0, epoch 3\n",
      "Session 0 Epoch 4 - Train Loss: 0.045323\n",
      "Checkpoint saved for fold 5, session 0, epoch 4\n",
      "Session 0 Epoch 5 - Train Loss: 0.040603\n",
      "Checkpoint saved for fold 5, session 0, epoch 5\n",
      "Session 0 Epoch 6 - Train Loss: 0.035806\n",
      "Checkpoint saved for fold 5, session 0, epoch 6\n",
      "Session 0 Epoch 7 - Train Loss: 0.032656\n",
      "Checkpoint saved for fold 5, session 0, epoch 7\n",
      "Session 0 Epoch 8 - Train Loss: 0.030522\n",
      "Checkpoint saved for fold 5, session 0, epoch 8\n",
      "Session 0 Epoch 9 - Train Loss: 0.029133\n",
      "Checkpoint saved for fold 5, session 0, epoch 9\n",
      "Session 0 Epoch 10 - Train Loss: 0.027160\n",
      "Checkpoint saved for fold 5, session 0, epoch 10\n",
      "Session 0 Epoch 11 - Train Loss: 0.024954\n",
      "Checkpoint saved for fold 5, session 0, epoch 11\n",
      "Session 0 Epoch 12 - Train Loss: 0.024449\n",
      "Checkpoint saved for fold 5, session 0, epoch 12\n",
      "Session 0 Epoch 13 - Train Loss: 0.024051\n",
      "Checkpoint saved for fold 5, session 0, epoch 13\n",
      "Session 0 Epoch 14 - Train Loss: 0.023136\n",
      "Checkpoint saved for fold 5, session 0, epoch 14\n",
      "Session 0 Epoch 15 - Train Loss: 0.021539\n",
      "Checkpoint saved for fold 5, session 0, epoch 15\n",
      "Session 0 Epoch 16 - Train Loss: 0.022132\n",
      "Session 0 Epoch 17 - Train Loss: 0.021730\n",
      "Session 0 Epoch 18 - Train Loss: 0.019591\n",
      "Checkpoint saved for fold 5, session 0, epoch 18\n",
      "Session 0 Epoch 19 - Train Loss: 0.020017\n",
      "Session 0 Epoch 20 - Train Loss: 0.020094\n",
      "Session 0 Epoch 21 - Train Loss: 0.019617\n",
      "Session 0 Epoch 22 - Train Loss: 0.019456\n",
      "Checkpoint saved for fold 5, session 0, epoch 22\n",
      "Session 0 Epoch 23 - Train Loss: 0.018920\n",
      "Checkpoint saved for fold 5, session 0, epoch 23\n",
      "Session 0 Epoch 24 - Train Loss: 0.017504\n",
      "Checkpoint saved for fold 5, session 0, epoch 24\n",
      "Session 0 Epoch 25 - Train Loss: 0.017092\n",
      "Checkpoint saved for fold 5, session 0, epoch 25\n",
      "Session 0 Epoch 26 - Train Loss: 0.016282\n",
      "Checkpoint saved for fold 5, session 0, epoch 26\n",
      "Session 0 Epoch 27 - Train Loss: 0.017258\n",
      "Session 0 Epoch 28 - Train Loss: 0.015118\n",
      "Checkpoint saved for fold 5, session 0, epoch 28\n",
      "Session 0 Epoch 29 - Train Loss: 0.016397\n",
      "Session 0 Epoch 30 - Train Loss: 0.016187\n",
      "Session 0 Epoch 31 - Train Loss: 0.015384\n",
      "Session 0 Epoch 32 - Train Loss: 0.014678\n",
      "Checkpoint saved for fold 5, session 0, epoch 32\n",
      "Session 0 Epoch 33 - Train Loss: 0.014939\n",
      "Session 0 Epoch 34 - Train Loss: 0.014991\n",
      "Session 0 Epoch 35 - Train Loss: 0.013820\n",
      "Checkpoint saved for fold 5, session 0, epoch 35\n",
      "Session 0 Epoch 36 - Train Loss: 0.015046\n",
      "Session 0 Epoch 37 - Train Loss: 0.013912\n",
      "Session 0 Epoch 38 - Train Loss: 0.015162\n",
      "Session 0 Epoch 39 - Train Loss: 0.013013\n",
      "Checkpoint saved for fold 5, session 0, epoch 39\n",
      "Session 0 Epoch 40 - Train Loss: 0.013399\n",
      "Session 0 Epoch 41 - Train Loss: 0.013280\n",
      "Session 0 Epoch 42 - Train Loss: 0.013556\n",
      "Session 0 Epoch 43 - Train Loss: 0.012588\n",
      "Checkpoint saved for fold 5, session 0, epoch 43\n",
      "Session 0 Epoch 44 - Train Loss: 0.011840\n",
      "Checkpoint saved for fold 5, session 0, epoch 44\n",
      "Session 0 Epoch 45 - Train Loss: 0.012952\n",
      "Session 0 Epoch 46 - Train Loss: 0.012653\n",
      "Session 0 Epoch 47 - Train Loss: 0.012175\n",
      "Session 0 Epoch 48 - Train Loss: 0.012092\n",
      "Session 0 Epoch 49 - Train Loss: 0.011761\n",
      "Session 0 Epoch 50 - Train Loss: 0.011952\n",
      "Session 0 Epoch 51 - Train Loss: 0.010783\n",
      "Checkpoint saved for fold 5, session 0, epoch 51\n",
      "Session 0 Epoch 52 - Train Loss: 0.010341\n",
      "Checkpoint saved for fold 5, session 0, epoch 52\n",
      "Session 0 Epoch 53 - Train Loss: 0.010785\n",
      "Session 0 Epoch 54 - Train Loss: 0.010155\n",
      "Checkpoint saved for fold 5, session 0, epoch 54\n",
      "Session 0 Epoch 55 - Train Loss: 0.009506\n",
      "Checkpoint saved for fold 5, session 0, epoch 55\n",
      "Session 0 Epoch 56 - Train Loss: 0.009465\n",
      "Session 0 Epoch 57 - Train Loss: 0.009037\n",
      "Checkpoint saved for fold 5, session 0, epoch 57\n",
      "Session 0 Epoch 58 - Train Loss: 0.009766\n",
      "Session 0 Epoch 59 - Train Loss: 0.009244\n",
      "Session 0 Epoch 60 - Train Loss: 0.009126\n",
      "Session 0 Epoch 61 - Train Loss: 0.009219\n",
      "Session 0 Epoch 62 - Train Loss: 0.008677\n",
      "Checkpoint saved for fold 5, session 0, epoch 62\n",
      "Session 0 Epoch 63 - Train Loss: 0.008647\n",
      "Session 0 Epoch 64 - Train Loss: 0.008330\n",
      "Checkpoint saved for fold 5, session 0, epoch 64\n",
      "Session 0 Epoch 65 - Train Loss: 0.008747\n",
      "Session 0 Epoch 66 - Train Loss: 0.007953\n",
      "Checkpoint saved for fold 5, session 0, epoch 66\n",
      "Session 0 Epoch 67 - Train Loss: 0.008157\n",
      "Session 0 Epoch 68 - Train Loss: 0.009366\n",
      "Session 0 Epoch 69 - Train Loss: 0.008141\n",
      "Session 0 Epoch 70 - Train Loss: 0.007575\n",
      "Checkpoint saved for fold 5, session 0, epoch 70\n",
      "Session 0 Epoch 71 - Train Loss: 0.008369\n",
      "Session 0 Epoch 72 - Train Loss: 0.008035\n",
      "Session 0 Epoch 73 - Train Loss: 0.009081\n",
      "Session 0 Epoch 74 - Train Loss: 0.007969\n",
      "Session 0 Epoch 75 - Train Loss: 0.007508\n",
      "Session 0 Epoch 76 - Train Loss: 0.007315\n",
      "Checkpoint saved for fold 5, session 0, epoch 76\n",
      "Session 0 Epoch 77 - Train Loss: 0.007219\n",
      "Session 0 Epoch 78 - Train Loss: 0.007063\n",
      "Checkpoint saved for fold 5, session 0, epoch 78\n",
      "Session 0 Epoch 79 - Train Loss: 0.006803\n",
      "Checkpoint saved for fold 5, session 0, epoch 79\n",
      "Session 0 Epoch 80 - Train Loss: 0.007000\n",
      "Training on Session 2/9\n",
      "Session 1 Epoch 1 - Train Loss: 0.051131\n",
      "Checkpoint saved for fold 5, session 1, epoch 1\n",
      "Session 1 Epoch 2 - Train Loss: 0.050856\n",
      "Checkpoint saved for fold 5, session 1, epoch 2\n",
      "Session 1 Epoch 3 - Train Loss: 0.050735\n",
      "Checkpoint saved for fold 5, session 1, epoch 3\n",
      "Session 1 Epoch 4 - Train Loss: 0.049040\n",
      "Checkpoint saved for fold 5, session 1, epoch 4\n",
      "Session 1 Epoch 5 - Train Loss: 0.040719\n",
      "Checkpoint saved for fold 5, session 1, epoch 5\n",
      "Session 1 Epoch 6 - Train Loss: 0.034484\n",
      "Checkpoint saved for fold 5, session 1, epoch 6\n",
      "Session 1 Epoch 7 - Train Loss: 0.032728\n",
      "Checkpoint saved for fold 5, session 1, epoch 7\n",
      "Session 1 Epoch 8 - Train Loss: 0.030311\n",
      "Checkpoint saved for fold 5, session 1, epoch 8\n",
      "Session 1 Epoch 9 - Train Loss: 0.028494\n",
      "Checkpoint saved for fold 5, session 1, epoch 9\n",
      "Session 1 Epoch 10 - Train Loss: 0.026181\n",
      "Checkpoint saved for fold 5, session 1, epoch 10\n",
      "Session 1 Epoch 11 - Train Loss: 0.023689\n",
      "Checkpoint saved for fold 5, session 1, epoch 11\n",
      "Session 1 Epoch 12 - Train Loss: 0.021519\n",
      "Checkpoint saved for fold 5, session 1, epoch 12\n",
      "Session 1 Epoch 13 - Train Loss: 0.021003\n",
      "Checkpoint saved for fold 5, session 1, epoch 13\n",
      "Session 1 Epoch 14 - Train Loss: 0.020706\n",
      "Checkpoint saved for fold 5, session 1, epoch 14\n",
      "Session 1 Epoch 15 - Train Loss: 0.020280\n",
      "Checkpoint saved for fold 5, session 1, epoch 15\n",
      "Session 1 Epoch 16 - Train Loss: 0.020043\n",
      "Checkpoint saved for fold 5, session 1, epoch 16\n",
      "Session 1 Epoch 17 - Train Loss: 0.019491\n",
      "Checkpoint saved for fold 5, session 1, epoch 17\n",
      "Session 1 Epoch 18 - Train Loss: 0.018455\n",
      "Checkpoint saved for fold 5, session 1, epoch 18\n",
      "Session 1 Epoch 19 - Train Loss: 0.017978\n",
      "Checkpoint saved for fold 5, session 1, epoch 19\n",
      "Session 1 Epoch 20 - Train Loss: 0.017488\n",
      "Checkpoint saved for fold 5, session 1, epoch 20\n",
      "Session 1 Epoch 21 - Train Loss: 0.016848\n",
      "Checkpoint saved for fold 5, session 1, epoch 21\n",
      "Session 1 Epoch 22 - Train Loss: 0.016597\n",
      "Checkpoint saved for fold 5, session 1, epoch 22\n",
      "Session 1 Epoch 23 - Train Loss: 0.016040\n",
      "Checkpoint saved for fold 5, session 1, epoch 23\n",
      "Session 1 Epoch 24 - Train Loss: 0.015947\n",
      "Session 1 Epoch 25 - Train Loss: 0.015613\n",
      "Checkpoint saved for fold 5, session 1, epoch 25\n",
      "Session 1 Epoch 26 - Train Loss: 0.015417\n",
      "Checkpoint saved for fold 5, session 1, epoch 26\n",
      "Session 1 Epoch 27 - Train Loss: 0.015362\n",
      "Session 1 Epoch 28 - Train Loss: 0.015298\n",
      "Checkpoint saved for fold 5, session 1, epoch 28\n",
      "Session 1 Epoch 29 - Train Loss: 0.015292\n",
      "Session 1 Epoch 30 - Train Loss: 0.015072\n",
      "Checkpoint saved for fold 5, session 1, epoch 30\n",
      "Session 1 Epoch 31 - Train Loss: 0.015136\n",
      "Session 1 Epoch 32 - Train Loss: 0.015091\n",
      "Session 1 Epoch 33 - Train Loss: 0.014815\n",
      "Checkpoint saved for fold 5, session 1, epoch 33\n",
      "Session 1 Epoch 34 - Train Loss: 0.015028\n",
      "Session 1 Epoch 35 - Train Loss: 0.014931\n",
      "Session 1 Epoch 36 - Train Loss: 0.014801\n",
      "Session 1 Epoch 37 - Train Loss: 0.015017\n",
      "Session 1 Epoch 38 - Train Loss: 0.014553\n",
      "Checkpoint saved for fold 5, session 1, epoch 38\n",
      "Session 1 Epoch 39 - Train Loss: 0.015106\n",
      "Session 1 Epoch 40 - Train Loss: 0.014707\n",
      "Session 1 Epoch 41 - Train Loss: 0.014627\n",
      "Session 1 Epoch 42 - Train Loss: 0.014613\n",
      "Session 1 Epoch 43 - Train Loss: 0.014338\n",
      "Checkpoint saved for fold 5, session 1, epoch 43\n",
      "Session 1 Epoch 44 - Train Loss: 0.014666\n",
      "Session 1 Epoch 45 - Train Loss: 0.014306\n",
      "Session 1 Epoch 46 - Train Loss: 0.014701\n",
      "Session 1 Epoch 47 - Train Loss: 0.014511\n",
      "Session 1 Epoch 48 - Train Loss: 0.014682\n",
      "Session 1 Epoch 49 - Train Loss: 0.014769\n",
      "Session 1 Epoch 50 - Train Loss: 0.014550\n",
      "Session 1 Epoch 51 - Train Loss: 0.014565\n",
      "Session 1 Epoch 52 - Train Loss: 0.014633\n",
      "Session 1 Epoch 53 - Train Loss: 0.014437\n",
      "Early stopping at epoch 53 for session 1\n",
      "Training on Session 3/9\n",
      "Session 2 Epoch 1 - Train Loss: 0.272758\n",
      "Checkpoint saved for fold 5, session 2, epoch 1\n",
      "Session 2 Epoch 2 - Train Loss: 0.264353\n",
      "Checkpoint saved for fold 5, session 2, epoch 2\n",
      "Session 2 Epoch 3 - Train Loss: 0.254818\n",
      "Checkpoint saved for fold 5, session 2, epoch 3\n",
      "Session 2 Epoch 4 - Train Loss: 0.240828\n",
      "Checkpoint saved for fold 5, session 2, epoch 4\n",
      "Session 2 Epoch 5 - Train Loss: 0.221016\n",
      "Checkpoint saved for fold 5, session 2, epoch 5\n",
      "Session 2 Epoch 6 - Train Loss: 0.197038\n",
      "Checkpoint saved for fold 5, session 2, epoch 6\n",
      "Session 2 Epoch 7 - Train Loss: 0.178695\n",
      "Checkpoint saved for fold 5, session 2, epoch 7\n",
      "Session 2 Epoch 8 - Train Loss: 0.166504\n",
      "Checkpoint saved for fold 5, session 2, epoch 8\n",
      "Session 2 Epoch 9 - Train Loss: 0.155459\n",
      "Checkpoint saved for fold 5, session 2, epoch 9\n",
      "Session 2 Epoch 10 - Train Loss: 0.143956\n",
      "Checkpoint saved for fold 5, session 2, epoch 10\n",
      "Session 2 Epoch 11 - Train Loss: 0.134076\n",
      "Checkpoint saved for fold 5, session 2, epoch 11\n",
      "Session 2 Epoch 12 - Train Loss: 0.125850\n",
      "Checkpoint saved for fold 5, session 2, epoch 12\n",
      "Session 2 Epoch 13 - Train Loss: 0.119511\n",
      "Checkpoint saved for fold 5, session 2, epoch 13\n",
      "Session 2 Epoch 14 - Train Loss: 0.116772\n",
      "Checkpoint saved for fold 5, session 2, epoch 14\n",
      "Session 2 Epoch 15 - Train Loss: 0.113699\n",
      "Checkpoint saved for fold 5, session 2, epoch 15\n",
      "Session 2 Epoch 16 - Train Loss: 0.111796\n",
      "Checkpoint saved for fold 5, session 2, epoch 16\n",
      "Session 2 Epoch 17 - Train Loss: 0.109841\n",
      "Checkpoint saved for fold 5, session 2, epoch 17\n",
      "Session 2 Epoch 18 - Train Loss: 0.107792\n",
      "Checkpoint saved for fold 5, session 2, epoch 18\n",
      "Session 2 Epoch 19 - Train Loss: 0.106741\n",
      "Checkpoint saved for fold 5, session 2, epoch 19\n",
      "Session 2 Epoch 20 - Train Loss: 0.105917\n",
      "Checkpoint saved for fold 5, session 2, epoch 20\n",
      "Session 2 Epoch 21 - Train Loss: 0.105854\n",
      "Session 2 Epoch 22 - Train Loss: 0.104762\n",
      "Checkpoint saved for fold 5, session 2, epoch 22\n",
      "Session 2 Epoch 23 - Train Loss: 0.104230\n",
      "Checkpoint saved for fold 5, session 2, epoch 23\n",
      "Session 2 Epoch 24 - Train Loss: 0.103814\n",
      "Checkpoint saved for fold 5, session 2, epoch 24\n",
      "Session 2 Epoch 25 - Train Loss: 0.103503\n",
      "Checkpoint saved for fold 5, session 2, epoch 25\n",
      "Session 2 Epoch 26 - Train Loss: 0.103381\n",
      "Checkpoint saved for fold 5, session 2, epoch 26\n",
      "Session 2 Epoch 27 - Train Loss: 0.103412\n",
      "Session 2 Epoch 28 - Train Loss: 0.102752\n",
      "Checkpoint saved for fold 5, session 2, epoch 28\n",
      "Session 2 Epoch 29 - Train Loss: 0.102778\n",
      "Session 2 Epoch 30 - Train Loss: 0.102594\n",
      "Checkpoint saved for fold 5, session 2, epoch 30\n",
      "Session 2 Epoch 31 - Train Loss: 0.102537\n",
      "Session 2 Epoch 32 - Train Loss: 0.102218\n",
      "Checkpoint saved for fold 5, session 2, epoch 32\n",
      "Session 2 Epoch 33 - Train Loss: 0.102116\n",
      "Checkpoint saved for fold 5, session 2, epoch 33\n",
      "Session 2 Epoch 34 - Train Loss: 0.102120\n",
      "Session 2 Epoch 35 - Train Loss: 0.102106\n",
      "Session 2 Epoch 36 - Train Loss: 0.101346\n",
      "Checkpoint saved for fold 5, session 2, epoch 36\n",
      "Session 2 Epoch 37 - Train Loss: 0.101680\n",
      "Session 2 Epoch 38 - Train Loss: 0.101595\n",
      "Session 2 Epoch 39 - Train Loss: 0.101430\n",
      "Session 2 Epoch 40 - Train Loss: 0.101171\n",
      "Checkpoint saved for fold 5, session 2, epoch 40\n",
      "Session 2 Epoch 41 - Train Loss: 0.101479\n",
      "Session 2 Epoch 42 - Train Loss: 0.101417\n",
      "Session 2 Epoch 43 - Train Loss: 0.101311\n",
      "Session 2 Epoch 44 - Train Loss: 0.101704\n",
      "Session 2 Epoch 45 - Train Loss: 0.101411\n",
      "Session 2 Epoch 46 - Train Loss: 0.101687\n",
      "Session 2 Epoch 47 - Train Loss: 0.101423\n",
      "Session 2 Epoch 48 - Train Loss: 0.101329\n",
      "Session 2 Epoch 49 - Train Loss: 0.101321\n",
      "Session 2 Epoch 50 - Train Loss: 0.101422\n",
      "Early stopping at epoch 50 for session 2\n",
      "Training on Session 4/9\n",
      "Session 3 Epoch 1 - Train Loss: 0.512163\n",
      "Checkpoint saved for fold 5, session 3, epoch 1\n",
      "Session 3 Epoch 2 - Train Loss: 0.510134\n",
      "Checkpoint saved for fold 5, session 3, epoch 2\n",
      "Session 3 Epoch 3 - Train Loss: 0.508631\n",
      "Checkpoint saved for fold 5, session 3, epoch 3\n",
      "Session 3 Epoch 4 - Train Loss: 0.506586\n",
      "Checkpoint saved for fold 5, session 3, epoch 4\n",
      "Session 3 Epoch 5 - Train Loss: 0.506137\n",
      "Checkpoint saved for fold 5, session 3, epoch 5\n",
      "Session 3 Epoch 6 - Train Loss: 0.504946\n",
      "Checkpoint saved for fold 5, session 3, epoch 6\n",
      "Session 3 Epoch 7 - Train Loss: 0.502972\n",
      "Checkpoint saved for fold 5, session 3, epoch 7\n",
      "Session 3 Epoch 8 - Train Loss: 0.501490\n",
      "Checkpoint saved for fold 5, session 3, epoch 8\n",
      "Session 3 Epoch 9 - Train Loss: 0.500572\n",
      "Checkpoint saved for fold 5, session 3, epoch 9\n",
      "Session 3 Epoch 10 - Train Loss: 0.500837\n",
      "Session 3 Epoch 11 - Train Loss: 0.498918\n",
      "Checkpoint saved for fold 5, session 3, epoch 11\n",
      "Session 3 Epoch 12 - Train Loss: 0.496191\n",
      "Checkpoint saved for fold 5, session 3, epoch 12\n",
      "Session 3 Epoch 13 - Train Loss: 0.496059\n",
      "Checkpoint saved for fold 5, session 3, epoch 13\n",
      "Session 3 Epoch 14 - Train Loss: 0.494450\n",
      "Checkpoint saved for fold 5, session 3, epoch 14\n",
      "Session 3 Epoch 15 - Train Loss: 0.492732\n",
      "Checkpoint saved for fold 5, session 3, epoch 15\n",
      "Session 3 Epoch 16 - Train Loss: 0.491346\n",
      "Checkpoint saved for fold 5, session 3, epoch 16\n",
      "Session 3 Epoch 17 - Train Loss: 0.489633\n",
      "Checkpoint saved for fold 5, session 3, epoch 17\n",
      "Session 3 Epoch 18 - Train Loss: 0.488759\n",
      "Checkpoint saved for fold 5, session 3, epoch 18\n",
      "Session 3 Epoch 19 - Train Loss: 0.487614\n",
      "Checkpoint saved for fold 5, session 3, epoch 19\n",
      "Session 3 Epoch 20 - Train Loss: 0.487358\n",
      "Checkpoint saved for fold 5, session 3, epoch 20\n",
      "Session 3 Epoch 21 - Train Loss: 0.484725\n",
      "Checkpoint saved for fold 5, session 3, epoch 21\n",
      "Session 3 Epoch 22 - Train Loss: 0.482903\n",
      "Checkpoint saved for fold 5, session 3, epoch 22\n",
      "Session 3 Epoch 23 - Train Loss: 0.482806\n",
      "Session 3 Epoch 24 - Train Loss: 0.480601\n",
      "Checkpoint saved for fold 5, session 3, epoch 24\n",
      "Session 3 Epoch 25 - Train Loss: 0.479470\n",
      "Checkpoint saved for fold 5, session 3, epoch 25\n",
      "Session 3 Epoch 26 - Train Loss: 0.479004\n",
      "Checkpoint saved for fold 5, session 3, epoch 26\n",
      "Session 3 Epoch 27 - Train Loss: 0.476609\n",
      "Checkpoint saved for fold 5, session 3, epoch 27\n",
      "Session 3 Epoch 28 - Train Loss: 0.477306\n",
      "Session 3 Epoch 29 - Train Loss: 0.475626\n",
      "Checkpoint saved for fold 5, session 3, epoch 29\n",
      "Session 3 Epoch 30 - Train Loss: 0.473172\n",
      "Checkpoint saved for fold 5, session 3, epoch 30\n",
      "Session 3 Epoch 31 - Train Loss: 0.472533\n",
      "Checkpoint saved for fold 5, session 3, epoch 31\n",
      "Session 3 Epoch 32 - Train Loss: 0.471442\n",
      "Checkpoint saved for fold 5, session 3, epoch 32\n",
      "Session 3 Epoch 33 - Train Loss: 0.470210\n",
      "Checkpoint saved for fold 5, session 3, epoch 33\n",
      "Session 3 Epoch 34 - Train Loss: 0.467548\n",
      "Checkpoint saved for fold 5, session 3, epoch 34\n",
      "Session 3 Epoch 35 - Train Loss: 0.467933\n",
      "Session 3 Epoch 36 - Train Loss: 0.466150\n",
      "Checkpoint saved for fold 5, session 3, epoch 36\n",
      "Session 3 Epoch 37 - Train Loss: 0.465093\n",
      "Checkpoint saved for fold 5, session 3, epoch 37\n",
      "Session 3 Epoch 38 - Train Loss: 0.463300\n",
      "Checkpoint saved for fold 5, session 3, epoch 38\n",
      "Session 3 Epoch 39 - Train Loss: 0.462377\n",
      "Checkpoint saved for fold 5, session 3, epoch 39\n",
      "Session 3 Epoch 40 - Train Loss: 0.461399\n",
      "Checkpoint saved for fold 5, session 3, epoch 40\n",
      "Session 3 Epoch 41 - Train Loss: 0.459996\n",
      "Checkpoint saved for fold 5, session 3, epoch 41\n",
      "Session 3 Epoch 42 - Train Loss: 0.458623\n",
      "Checkpoint saved for fold 5, session 3, epoch 42\n",
      "Session 3 Epoch 43 - Train Loss: 0.457356\n",
      "Checkpoint saved for fold 5, session 3, epoch 43\n",
      "Session 3 Epoch 44 - Train Loss: 0.456226\n",
      "Checkpoint saved for fold 5, session 3, epoch 44\n",
      "Session 3 Epoch 45 - Train Loss: 0.455795\n",
      "Checkpoint saved for fold 5, session 3, epoch 45\n",
      "Session 3 Epoch 46 - Train Loss: 0.454574\n",
      "Checkpoint saved for fold 5, session 3, epoch 46\n",
      "Session 3 Epoch 47 - Train Loss: 0.452273\n",
      "Checkpoint saved for fold 5, session 3, epoch 47\n",
      "Session 3 Epoch 48 - Train Loss: 0.451294\n",
      "Checkpoint saved for fold 5, session 3, epoch 48\n",
      "Session 3 Epoch 49 - Train Loss: 0.450839\n",
      "Checkpoint saved for fold 5, session 3, epoch 49\n",
      "Session 3 Epoch 50 - Train Loss: 0.448257\n",
      "Checkpoint saved for fold 5, session 3, epoch 50\n",
      "Session 3 Epoch 51 - Train Loss: 0.447728\n",
      "Checkpoint saved for fold 5, session 3, epoch 51\n",
      "Session 3 Epoch 52 - Train Loss: 0.446999\n",
      "Checkpoint saved for fold 5, session 3, epoch 52\n",
      "Session 3 Epoch 53 - Train Loss: 0.445785\n",
      "Checkpoint saved for fold 5, session 3, epoch 53\n",
      "Session 3 Epoch 54 - Train Loss: 0.443668\n",
      "Checkpoint saved for fold 5, session 3, epoch 54\n",
      "Session 3 Epoch 55 - Train Loss: 0.442516\n",
      "Checkpoint saved for fold 5, session 3, epoch 55\n",
      "Session 3 Epoch 56 - Train Loss: 0.442968\n",
      "Session 3 Epoch 57 - Train Loss: 0.441158\n",
      "Checkpoint saved for fold 5, session 3, epoch 57\n",
      "Session 3 Epoch 58 - Train Loss: 0.439373\n",
      "Checkpoint saved for fold 5, session 3, epoch 58\n",
      "Session 3 Epoch 59 - Train Loss: 0.438920\n",
      "Checkpoint saved for fold 5, session 3, epoch 59\n",
      "Session 3 Epoch 60 - Train Loss: 0.437859\n",
      "Checkpoint saved for fold 5, session 3, epoch 60\n",
      "Session 3 Epoch 61 - Train Loss: 0.436313\n",
      "Checkpoint saved for fold 5, session 3, epoch 61\n",
      "Session 3 Epoch 62 - Train Loss: 0.434070\n",
      "Checkpoint saved for fold 5, session 3, epoch 62\n",
      "Session 3 Epoch 63 - Train Loss: 0.434909\n",
      "Session 3 Epoch 64 - Train Loss: 0.432206\n",
      "Checkpoint saved for fold 5, session 3, epoch 64\n",
      "Session 3 Epoch 65 - Train Loss: 0.431025\n",
      "Checkpoint saved for fold 5, session 3, epoch 65\n",
      "Session 3 Epoch 66 - Train Loss: 0.430940\n",
      "Session 3 Epoch 67 - Train Loss: 0.429940\n",
      "Checkpoint saved for fold 5, session 3, epoch 67\n",
      "Session 3 Epoch 68 - Train Loss: 0.428624\n",
      "Checkpoint saved for fold 5, session 3, epoch 68\n",
      "Session 3 Epoch 69 - Train Loss: 0.426880\n",
      "Checkpoint saved for fold 5, session 3, epoch 69\n",
      "Session 3 Epoch 70 - Train Loss: 0.425528\n",
      "Checkpoint saved for fold 5, session 3, epoch 70\n",
      "Session 3 Epoch 71 - Train Loss: 0.425041\n",
      "Checkpoint saved for fold 5, session 3, epoch 71\n",
      "Session 3 Epoch 72 - Train Loss: 0.424353\n",
      "Checkpoint saved for fold 5, session 3, epoch 72\n",
      "Session 3 Epoch 73 - Train Loss: 0.422578\n",
      "Checkpoint saved for fold 5, session 3, epoch 73\n",
      "Session 3 Epoch 74 - Train Loss: 0.422095\n",
      "Checkpoint saved for fold 5, session 3, epoch 74\n",
      "Session 3 Epoch 75 - Train Loss: 0.420731\n",
      "Checkpoint saved for fold 5, session 3, epoch 75\n",
      "Session 3 Epoch 76 - Train Loss: 0.419344\n",
      "Checkpoint saved for fold 5, session 3, epoch 76\n",
      "Session 3 Epoch 77 - Train Loss: 0.418467\n",
      "Checkpoint saved for fold 5, session 3, epoch 77\n",
      "Session 3 Epoch 78 - Train Loss: 0.417156\n",
      "Checkpoint saved for fold 5, session 3, epoch 78\n",
      "Session 3 Epoch 79 - Train Loss: 0.415346\n",
      "Checkpoint saved for fold 5, session 3, epoch 79\n",
      "Session 3 Epoch 80 - Train Loss: 0.415256\n",
      "Training on Session 5/9\n",
      "Session 4 Epoch 1 - Train Loss: 0.169762\n",
      "Checkpoint saved for fold 5, session 4, epoch 1\n",
      "Session 4 Epoch 2 - Train Loss: 0.170043\n",
      "Session 4 Epoch 3 - Train Loss: 0.169767\n",
      "Session 4 Epoch 4 - Train Loss: 0.168622\n",
      "Checkpoint saved for fold 5, session 4, epoch 4\n",
      "Session 4 Epoch 5 - Train Loss: 0.167620\n",
      "Checkpoint saved for fold 5, session 4, epoch 5\n",
      "Session 4 Epoch 6 - Train Loss: 0.167058\n",
      "Checkpoint saved for fold 5, session 4, epoch 6\n",
      "Session 4 Epoch 7 - Train Loss: 0.166987\n",
      "Session 4 Epoch 8 - Train Loss: 0.166083\n",
      "Checkpoint saved for fold 5, session 4, epoch 8\n",
      "Session 4 Epoch 9 - Train Loss: 0.165557\n",
      "Checkpoint saved for fold 5, session 4, epoch 9\n",
      "Session 4 Epoch 10 - Train Loss: 0.164923\n",
      "Checkpoint saved for fold 5, session 4, epoch 10\n",
      "Session 4 Epoch 11 - Train Loss: 0.164801\n",
      "Checkpoint saved for fold 5, session 4, epoch 11\n",
      "Session 4 Epoch 12 - Train Loss: 0.163594\n",
      "Checkpoint saved for fold 5, session 4, epoch 12\n",
      "Session 4 Epoch 13 - Train Loss: 0.163375\n",
      "Checkpoint saved for fold 5, session 4, epoch 13\n",
      "Session 4 Epoch 14 - Train Loss: 0.162571\n",
      "Checkpoint saved for fold 5, session 4, epoch 14\n",
      "Session 4 Epoch 15 - Train Loss: 0.162377\n",
      "Checkpoint saved for fold 5, session 4, epoch 15\n",
      "Session 4 Epoch 16 - Train Loss: 0.161868\n",
      "Checkpoint saved for fold 5, session 4, epoch 16\n",
      "Session 4 Epoch 17 - Train Loss: 0.161173\n",
      "Checkpoint saved for fold 5, session 4, epoch 17\n",
      "Session 4 Epoch 18 - Train Loss: 0.161195\n",
      "Session 4 Epoch 19 - Train Loss: 0.160157\n",
      "Checkpoint saved for fold 5, session 4, epoch 19\n",
      "Session 4 Epoch 20 - Train Loss: 0.159684\n",
      "Checkpoint saved for fold 5, session 4, epoch 20\n",
      "Session 4 Epoch 21 - Train Loss: 0.158449\n",
      "Checkpoint saved for fold 5, session 4, epoch 21\n",
      "Session 4 Epoch 22 - Train Loss: 0.157961\n",
      "Checkpoint saved for fold 5, session 4, epoch 22\n",
      "Session 4 Epoch 23 - Train Loss: 0.157532\n",
      "Checkpoint saved for fold 5, session 4, epoch 23\n",
      "Session 4 Epoch 24 - Train Loss: 0.157406\n",
      "Checkpoint saved for fold 5, session 4, epoch 24\n",
      "Session 4 Epoch 25 - Train Loss: 0.156409\n",
      "Checkpoint saved for fold 5, session 4, epoch 25\n",
      "Session 4 Epoch 26 - Train Loss: 0.155556\n",
      "Checkpoint saved for fold 5, session 4, epoch 26\n",
      "Session 4 Epoch 27 - Train Loss: 0.155293\n",
      "Checkpoint saved for fold 5, session 4, epoch 27\n",
      "Session 4 Epoch 28 - Train Loss: 0.154448\n",
      "Checkpoint saved for fold 5, session 4, epoch 28\n",
      "Session 4 Epoch 29 - Train Loss: 0.153927\n",
      "Checkpoint saved for fold 5, session 4, epoch 29\n",
      "Session 4 Epoch 30 - Train Loss: 0.153413\n",
      "Checkpoint saved for fold 5, session 4, epoch 30\n",
      "Session 4 Epoch 31 - Train Loss: 0.152969\n",
      "Checkpoint saved for fold 5, session 4, epoch 31\n",
      "Session 4 Epoch 32 - Train Loss: 0.152845\n",
      "Checkpoint saved for fold 5, session 4, epoch 32\n",
      "Session 4 Epoch 33 - Train Loss: 0.152334\n",
      "Checkpoint saved for fold 5, session 4, epoch 33\n",
      "Session 4 Epoch 34 - Train Loss: 0.151367\n",
      "Checkpoint saved for fold 5, session 4, epoch 34\n",
      "Session 4 Epoch 35 - Train Loss: 0.151233\n",
      "Checkpoint saved for fold 5, session 4, epoch 35\n",
      "Session 4 Epoch 36 - Train Loss: 0.150595\n",
      "Checkpoint saved for fold 5, session 4, epoch 36\n",
      "Session 4 Epoch 37 - Train Loss: 0.150256\n",
      "Checkpoint saved for fold 5, session 4, epoch 37\n",
      "Session 4 Epoch 38 - Train Loss: 0.149478\n",
      "Checkpoint saved for fold 5, session 4, epoch 38\n",
      "Session 4 Epoch 39 - Train Loss: 0.148991\n",
      "Checkpoint saved for fold 5, session 4, epoch 39\n",
      "Session 4 Epoch 40 - Train Loss: 0.148233\n",
      "Checkpoint saved for fold 5, session 4, epoch 40\n",
      "Session 4 Epoch 41 - Train Loss: 0.148179\n",
      "Session 4 Epoch 42 - Train Loss: 0.147576\n",
      "Checkpoint saved for fold 5, session 4, epoch 42\n",
      "Session 4 Epoch 43 - Train Loss: 0.146358\n",
      "Checkpoint saved for fold 5, session 4, epoch 43\n",
      "Session 4 Epoch 44 - Train Loss: 0.146422\n",
      "Session 4 Epoch 45 - Train Loss: 0.145903\n",
      "Checkpoint saved for fold 5, session 4, epoch 45\n",
      "Session 4 Epoch 46 - Train Loss: 0.145279\n",
      "Checkpoint saved for fold 5, session 4, epoch 46\n",
      "Session 4 Epoch 47 - Train Loss: 0.144649\n",
      "Checkpoint saved for fold 5, session 4, epoch 47\n",
      "Session 4 Epoch 48 - Train Loss: 0.144505\n",
      "Checkpoint saved for fold 5, session 4, epoch 48\n",
      "Session 4 Epoch 49 - Train Loss: 0.143858\n",
      "Checkpoint saved for fold 5, session 4, epoch 49\n",
      "Session 4 Epoch 50 - Train Loss: 0.143668\n",
      "Checkpoint saved for fold 5, session 4, epoch 50\n",
      "Session 4 Epoch 51 - Train Loss: 0.143113\n",
      "Checkpoint saved for fold 5, session 4, epoch 51\n",
      "Session 4 Epoch 52 - Train Loss: 0.142245\n",
      "Checkpoint saved for fold 5, session 4, epoch 52\n",
      "Session 4 Epoch 53 - Train Loss: 0.141624\n",
      "Checkpoint saved for fold 5, session 4, epoch 53\n",
      "Session 4 Epoch 54 - Train Loss: 0.141580\n",
      "Session 4 Epoch 55 - Train Loss: 0.140867\n",
      "Checkpoint saved for fold 5, session 4, epoch 55\n",
      "Session 4 Epoch 56 - Train Loss: 0.140610\n",
      "Checkpoint saved for fold 5, session 4, epoch 56\n",
      "Session 4 Epoch 57 - Train Loss: 0.140107\n",
      "Checkpoint saved for fold 5, session 4, epoch 57\n",
      "Session 4 Epoch 58 - Train Loss: 0.139624\n",
      "Checkpoint saved for fold 5, session 4, epoch 58\n",
      "Session 4 Epoch 59 - Train Loss: 0.138931\n",
      "Checkpoint saved for fold 5, session 4, epoch 59\n",
      "Session 4 Epoch 60 - Train Loss: 0.138420\n",
      "Checkpoint saved for fold 5, session 4, epoch 60\n",
      "Session 4 Epoch 61 - Train Loss: 0.138317\n",
      "Checkpoint saved for fold 5, session 4, epoch 61\n",
      "Session 4 Epoch 62 - Train Loss: 0.137932\n",
      "Checkpoint saved for fold 5, session 4, epoch 62\n",
      "Session 4 Epoch 63 - Train Loss: 0.137470\n",
      "Checkpoint saved for fold 5, session 4, epoch 63\n",
      "Session 4 Epoch 64 - Train Loss: 0.136702\n",
      "Checkpoint saved for fold 5, session 4, epoch 64\n",
      "Session 4 Epoch 65 - Train Loss: 0.136389\n",
      "Checkpoint saved for fold 5, session 4, epoch 65\n",
      "Session 4 Epoch 66 - Train Loss: 0.135742\n",
      "Checkpoint saved for fold 5, session 4, epoch 66\n",
      "Session 4 Epoch 67 - Train Loss: 0.135599\n",
      "Checkpoint saved for fold 5, session 4, epoch 67\n",
      "Session 4 Epoch 68 - Train Loss: 0.134806\n",
      "Checkpoint saved for fold 5, session 4, epoch 68\n",
      "Session 4 Epoch 69 - Train Loss: 0.134614\n",
      "Checkpoint saved for fold 5, session 4, epoch 69\n",
      "Session 4 Epoch 70 - Train Loss: 0.133856\n",
      "Checkpoint saved for fold 5, session 4, epoch 70\n",
      "Session 4 Epoch 71 - Train Loss: 0.133428\n",
      "Checkpoint saved for fold 5, session 4, epoch 71\n",
      "Session 4 Epoch 72 - Train Loss: 0.133173\n",
      "Checkpoint saved for fold 5, session 4, epoch 72\n",
      "Session 4 Epoch 73 - Train Loss: 0.132277\n",
      "Checkpoint saved for fold 5, session 4, epoch 73\n",
      "Session 4 Epoch 74 - Train Loss: 0.132219\n",
      "Session 4 Epoch 75 - Train Loss: 0.131752\n",
      "Checkpoint saved for fold 5, session 4, epoch 75\n",
      "Session 4 Epoch 76 - Train Loss: 0.131253\n",
      "Checkpoint saved for fold 5, session 4, epoch 76\n",
      "Session 4 Epoch 77 - Train Loss: 0.130644\n",
      "Checkpoint saved for fold 5, session 4, epoch 77\n",
      "Session 4 Epoch 78 - Train Loss: 0.130351\n",
      "Checkpoint saved for fold 5, session 4, epoch 78\n",
      "Session 4 Epoch 79 - Train Loss: 0.129646\n",
      "Checkpoint saved for fold 5, session 4, epoch 79\n",
      "Session 4 Epoch 80 - Train Loss: 0.129718\n",
      "Training on Session 6/9\n",
      "Session 5 Epoch 1 - Train Loss: 0.359808\n",
      "Checkpoint saved for fold 5, session 5, epoch 1\n",
      "Session 5 Epoch 2 - Train Loss: 0.358935\n",
      "Checkpoint saved for fold 5, session 5, epoch 2\n",
      "Session 5 Epoch 3 - Train Loss: 0.358715\n",
      "Checkpoint saved for fold 5, session 5, epoch 3\n",
      "Session 5 Epoch 4 - Train Loss: 0.357954\n",
      "Checkpoint saved for fold 5, session 5, epoch 4\n",
      "Session 5 Epoch 5 - Train Loss: 0.356089\n",
      "Checkpoint saved for fold 5, session 5, epoch 5\n",
      "Session 5 Epoch 6 - Train Loss: 0.355631\n",
      "Checkpoint saved for fold 5, session 5, epoch 6\n",
      "Session 5 Epoch 7 - Train Loss: 0.355326\n",
      "Checkpoint saved for fold 5, session 5, epoch 7\n",
      "Session 5 Epoch 8 - Train Loss: 0.354177\n",
      "Checkpoint saved for fold 5, session 5, epoch 8\n",
      "Session 5 Epoch 9 - Train Loss: 0.353488\n",
      "Checkpoint saved for fold 5, session 5, epoch 9\n",
      "Session 5 Epoch 10 - Train Loss: 0.352803\n",
      "Checkpoint saved for fold 5, session 5, epoch 10\n",
      "Session 5 Epoch 11 - Train Loss: 0.351922\n",
      "Checkpoint saved for fold 5, session 5, epoch 11\n",
      "Session 5 Epoch 12 - Train Loss: 0.350984\n",
      "Checkpoint saved for fold 5, session 5, epoch 12\n",
      "Session 5 Epoch 13 - Train Loss: 0.349977\n",
      "Checkpoint saved for fold 5, session 5, epoch 13\n",
      "Session 5 Epoch 14 - Train Loss: 0.349399\n",
      "Checkpoint saved for fold 5, session 5, epoch 14\n",
      "Session 5 Epoch 15 - Train Loss: 0.348723\n",
      "Checkpoint saved for fold 5, session 5, epoch 15\n",
      "Session 5 Epoch 16 - Train Loss: 0.348251\n",
      "Checkpoint saved for fold 5, session 5, epoch 16\n",
      "Session 5 Epoch 17 - Train Loss: 0.346860\n",
      "Checkpoint saved for fold 5, session 5, epoch 17\n",
      "Session 5 Epoch 18 - Train Loss: 0.345978\n",
      "Checkpoint saved for fold 5, session 5, epoch 18\n",
      "Session 5 Epoch 19 - Train Loss: 0.345451\n",
      "Checkpoint saved for fold 5, session 5, epoch 19\n",
      "Session 5 Epoch 20 - Train Loss: 0.344260\n",
      "Checkpoint saved for fold 5, session 5, epoch 20\n",
      "Session 5 Epoch 21 - Train Loss: 0.343386\n",
      "Checkpoint saved for fold 5, session 5, epoch 21\n",
      "Session 5 Epoch 22 - Train Loss: 0.342852\n",
      "Checkpoint saved for fold 5, session 5, epoch 22\n",
      "Session 5 Epoch 23 - Train Loss: 0.342056\n",
      "Checkpoint saved for fold 5, session 5, epoch 23\n",
      "Session 5 Epoch 24 - Train Loss: 0.341188\n",
      "Checkpoint saved for fold 5, session 5, epoch 24\n",
      "Session 5 Epoch 25 - Train Loss: 0.340803\n",
      "Checkpoint saved for fold 5, session 5, epoch 25\n",
      "Session 5 Epoch 26 - Train Loss: 0.340062\n",
      "Checkpoint saved for fold 5, session 5, epoch 26\n",
      "Session 5 Epoch 27 - Train Loss: 0.339731\n",
      "Checkpoint saved for fold 5, session 5, epoch 27\n",
      "Session 5 Epoch 28 - Train Loss: 0.338972\n",
      "Checkpoint saved for fold 5, session 5, epoch 28\n",
      "Session 5 Epoch 29 - Train Loss: 0.337203\n",
      "Checkpoint saved for fold 5, session 5, epoch 29\n",
      "Session 5 Epoch 30 - Train Loss: 0.336828\n",
      "Checkpoint saved for fold 5, session 5, epoch 30\n",
      "Session 5 Epoch 31 - Train Loss: 0.335625\n",
      "Checkpoint saved for fold 5, session 5, epoch 31\n",
      "Session 5 Epoch 32 - Train Loss: 0.335685\n",
      "Session 5 Epoch 33 - Train Loss: 0.334370\n",
      "Checkpoint saved for fold 5, session 5, epoch 33\n",
      "Session 5 Epoch 34 - Train Loss: 0.334337\n",
      "Session 5 Epoch 35 - Train Loss: 0.333138\n",
      "Checkpoint saved for fold 5, session 5, epoch 35\n",
      "Session 5 Epoch 36 - Train Loss: 0.333371\n",
      "Session 5 Epoch 37 - Train Loss: 0.331514\n",
      "Checkpoint saved for fold 5, session 5, epoch 37\n",
      "Session 5 Epoch 38 - Train Loss: 0.331541\n",
      "Session 5 Epoch 39 - Train Loss: 0.331084\n",
      "Checkpoint saved for fold 5, session 5, epoch 39\n",
      "Session 5 Epoch 40 - Train Loss: 0.329466\n",
      "Checkpoint saved for fold 5, session 5, epoch 40\n",
      "Session 5 Epoch 41 - Train Loss: 0.329176\n",
      "Checkpoint saved for fold 5, session 5, epoch 41\n",
      "Session 5 Epoch 42 - Train Loss: 0.328968\n",
      "Checkpoint saved for fold 5, session 5, epoch 42\n",
      "Session 5 Epoch 43 - Train Loss: 0.327631\n",
      "Checkpoint saved for fold 5, session 5, epoch 43\n",
      "Session 5 Epoch 44 - Train Loss: 0.326563\n",
      "Checkpoint saved for fold 5, session 5, epoch 44\n",
      "Session 5 Epoch 45 - Train Loss: 0.325911\n",
      "Checkpoint saved for fold 5, session 5, epoch 45\n",
      "Session 5 Epoch 46 - Train Loss: 0.325245\n",
      "Checkpoint saved for fold 5, session 5, epoch 46\n",
      "Session 5 Epoch 47 - Train Loss: 0.324437\n",
      "Checkpoint saved for fold 5, session 5, epoch 47\n",
      "Session 5 Epoch 48 - Train Loss: 0.323986\n",
      "Checkpoint saved for fold 5, session 5, epoch 48\n",
      "Session 5 Epoch 49 - Train Loss: 0.323197\n",
      "Checkpoint saved for fold 5, session 5, epoch 49\n",
      "Session 5 Epoch 50 - Train Loss: 0.321849\n",
      "Checkpoint saved for fold 5, session 5, epoch 50\n",
      "Session 5 Epoch 51 - Train Loss: 0.321653\n",
      "Checkpoint saved for fold 5, session 5, epoch 51\n",
      "Session 5 Epoch 52 - Train Loss: 0.320805\n",
      "Checkpoint saved for fold 5, session 5, epoch 52\n",
      "Session 5 Epoch 53 - Train Loss: 0.320598\n",
      "Checkpoint saved for fold 5, session 5, epoch 53\n",
      "Session 5 Epoch 54 - Train Loss: 0.319784\n",
      "Checkpoint saved for fold 5, session 5, epoch 54\n",
      "Session 5 Epoch 55 - Train Loss: 0.319545\n",
      "Checkpoint saved for fold 5, session 5, epoch 55\n",
      "Session 5 Epoch 56 - Train Loss: 0.318209\n",
      "Checkpoint saved for fold 5, session 5, epoch 56\n",
      "Session 5 Epoch 57 - Train Loss: 0.317581\n",
      "Checkpoint saved for fold 5, session 5, epoch 57\n",
      "Session 5 Epoch 58 - Train Loss: 0.316418\n",
      "Checkpoint saved for fold 5, session 5, epoch 58\n",
      "Session 5 Epoch 59 - Train Loss: 0.316717\n",
      "Session 5 Epoch 60 - Train Loss: 0.315688\n",
      "Checkpoint saved for fold 5, session 5, epoch 60\n",
      "Session 5 Epoch 61 - Train Loss: 0.314168\n",
      "Checkpoint saved for fold 5, session 5, epoch 61\n",
      "Session 5 Epoch 62 - Train Loss: 0.313663\n",
      "Checkpoint saved for fold 5, session 5, epoch 62\n",
      "Session 5 Epoch 63 - Train Loss: 0.313604\n",
      "Session 5 Epoch 64 - Train Loss: 0.313126\n",
      "Checkpoint saved for fold 5, session 5, epoch 64\n",
      "Session 5 Epoch 65 - Train Loss: 0.311484\n",
      "Checkpoint saved for fold 5, session 5, epoch 65\n",
      "Session 5 Epoch 66 - Train Loss: 0.312159\n",
      "Session 5 Epoch 67 - Train Loss: 0.311427\n",
      "Session 5 Epoch 68 - Train Loss: 0.310273\n",
      "Checkpoint saved for fold 5, session 5, epoch 68\n",
      "Session 5 Epoch 69 - Train Loss: 0.309687\n",
      "Checkpoint saved for fold 5, session 5, epoch 69\n",
      "Session 5 Epoch 70 - Train Loss: 0.309179\n",
      "Checkpoint saved for fold 5, session 5, epoch 70\n",
      "Session 5 Epoch 71 - Train Loss: 0.308628\n",
      "Checkpoint saved for fold 5, session 5, epoch 71\n",
      "Session 5 Epoch 72 - Train Loss: 0.307520\n",
      "Checkpoint saved for fold 5, session 5, epoch 72\n",
      "Session 5 Epoch 73 - Train Loss: 0.307509\n",
      "Session 5 Epoch 74 - Train Loss: 0.306174\n",
      "Checkpoint saved for fold 5, session 5, epoch 74\n",
      "Session 5 Epoch 75 - Train Loss: 0.305893\n",
      "Checkpoint saved for fold 5, session 5, epoch 75\n",
      "Session 5 Epoch 76 - Train Loss: 0.305204\n",
      "Checkpoint saved for fold 5, session 5, epoch 76\n",
      "Session 5 Epoch 77 - Train Loss: 0.303824\n",
      "Checkpoint saved for fold 5, session 5, epoch 77\n",
      "Session 5 Epoch 78 - Train Loss: 0.304122\n",
      "Session 5 Epoch 79 - Train Loss: 0.303322\n",
      "Checkpoint saved for fold 5, session 5, epoch 79\n",
      "Session 5 Epoch 80 - Train Loss: 0.302787\n",
      "Checkpoint saved for fold 5, session 5, epoch 80\n",
      "Training on Session 7/9\n",
      "Session 6 Epoch 1 - Train Loss: 0.118683\n",
      "Checkpoint saved for fold 5, session 6, epoch 1\n",
      "Session 6 Epoch 2 - Train Loss: 0.118239\n",
      "Checkpoint saved for fold 5, session 6, epoch 2\n",
      "Session 6 Epoch 3 - Train Loss: 0.118044\n",
      "Checkpoint saved for fold 5, session 6, epoch 3\n",
      "Session 6 Epoch 4 - Train Loss: 0.117737\n",
      "Checkpoint saved for fold 5, session 6, epoch 4\n",
      "Session 6 Epoch 5 - Train Loss: 0.117596\n",
      "Checkpoint saved for fold 5, session 6, epoch 5\n",
      "Session 6 Epoch 6 - Train Loss: 0.117274\n",
      "Checkpoint saved for fold 5, session 6, epoch 6\n",
      "Session 6 Epoch 7 - Train Loss: 0.116926\n",
      "Checkpoint saved for fold 5, session 6, epoch 7\n",
      "Session 6 Epoch 8 - Train Loss: 0.116557\n",
      "Checkpoint saved for fold 5, session 6, epoch 8\n",
      "Session 6 Epoch 9 - Train Loss: 0.116147\n",
      "Checkpoint saved for fold 5, session 6, epoch 9\n",
      "Session 6 Epoch 10 - Train Loss: 0.116221\n",
      "Session 6 Epoch 11 - Train Loss: 0.115465\n",
      "Checkpoint saved for fold 5, session 6, epoch 11\n",
      "Session 6 Epoch 12 - Train Loss: 0.115227\n",
      "Checkpoint saved for fold 5, session 6, epoch 12\n",
      "Session 6 Epoch 13 - Train Loss: 0.114952\n",
      "Checkpoint saved for fold 5, session 6, epoch 13\n",
      "Session 6 Epoch 14 - Train Loss: 0.114911\n",
      "Session 6 Epoch 15 - Train Loss: 0.114386\n",
      "Checkpoint saved for fold 5, session 6, epoch 15\n",
      "Session 6 Epoch 16 - Train Loss: 0.114186\n",
      "Checkpoint saved for fold 5, session 6, epoch 16\n",
      "Session 6 Epoch 17 - Train Loss: 0.113631\n",
      "Checkpoint saved for fold 5, session 6, epoch 17\n",
      "Session 6 Epoch 18 - Train Loss: 0.113665\n",
      "Session 6 Epoch 19 - Train Loss: 0.113425\n",
      "Checkpoint saved for fold 5, session 6, epoch 19\n",
      "Session 6 Epoch 20 - Train Loss: 0.112961\n",
      "Checkpoint saved for fold 5, session 6, epoch 20\n",
      "Session 6 Epoch 21 - Train Loss: 0.112734\n",
      "Checkpoint saved for fold 5, session 6, epoch 21\n",
      "Session 6 Epoch 22 - Train Loss: 0.112338\n",
      "Checkpoint saved for fold 5, session 6, epoch 22\n",
      "Session 6 Epoch 23 - Train Loss: 0.112176\n",
      "Checkpoint saved for fold 5, session 6, epoch 23\n",
      "Session 6 Epoch 24 - Train Loss: 0.111729\n",
      "Checkpoint saved for fold 5, session 6, epoch 24\n",
      "Session 6 Epoch 25 - Train Loss: 0.111941\n",
      "Session 6 Epoch 26 - Train Loss: 0.111160\n",
      "Checkpoint saved for fold 5, session 6, epoch 26\n",
      "Session 6 Epoch 27 - Train Loss: 0.111299\n",
      "Session 6 Epoch 28 - Train Loss: 0.110753\n",
      "Checkpoint saved for fold 5, session 6, epoch 28\n",
      "Session 6 Epoch 29 - Train Loss: 0.110547\n",
      "Checkpoint saved for fold 5, session 6, epoch 29\n",
      "Session 6 Epoch 30 - Train Loss: 0.110282\n",
      "Checkpoint saved for fold 5, session 6, epoch 30\n",
      "Session 6 Epoch 31 - Train Loss: 0.109929\n",
      "Checkpoint saved for fold 5, session 6, epoch 31\n",
      "Session 6 Epoch 32 - Train Loss: 0.109841\n",
      "Session 6 Epoch 33 - Train Loss: 0.109589\n",
      "Checkpoint saved for fold 5, session 6, epoch 33\n",
      "Session 6 Epoch 34 - Train Loss: 0.109208\n",
      "Checkpoint saved for fold 5, session 6, epoch 34\n",
      "Session 6 Epoch 35 - Train Loss: 0.109012\n",
      "Checkpoint saved for fold 5, session 6, epoch 35\n",
      "Session 6 Epoch 36 - Train Loss: 0.108694\n",
      "Checkpoint saved for fold 5, session 6, epoch 36\n",
      "Session 6 Epoch 37 - Train Loss: 0.108603\n",
      "Session 6 Epoch 38 - Train Loss: 0.108510\n",
      "Checkpoint saved for fold 5, session 6, epoch 38\n",
      "Session 6 Epoch 39 - Train Loss: 0.107980\n",
      "Checkpoint saved for fold 5, session 6, epoch 39\n",
      "Session 6 Epoch 40 - Train Loss: 0.107625\n",
      "Checkpoint saved for fold 5, session 6, epoch 40\n",
      "Session 6 Epoch 41 - Train Loss: 0.107352\n",
      "Checkpoint saved for fold 5, session 6, epoch 41\n",
      "Session 6 Epoch 42 - Train Loss: 0.107300\n",
      "Session 6 Epoch 43 - Train Loss: 0.106967\n",
      "Checkpoint saved for fold 5, session 6, epoch 43\n",
      "Session 6 Epoch 44 - Train Loss: 0.106635\n",
      "Checkpoint saved for fold 5, session 6, epoch 44\n",
      "Session 6 Epoch 45 - Train Loss: 0.106261\n",
      "Checkpoint saved for fold 5, session 6, epoch 45\n",
      "Session 6 Epoch 46 - Train Loss: 0.106214\n",
      "Session 6 Epoch 47 - Train Loss: 0.105717\n",
      "Checkpoint saved for fold 5, session 6, epoch 47\n",
      "Session 6 Epoch 48 - Train Loss: 0.105683\n",
      "Session 6 Epoch 49 - Train Loss: 0.105485\n",
      "Checkpoint saved for fold 5, session 6, epoch 49\n",
      "Session 6 Epoch 50 - Train Loss: 0.105367\n",
      "Checkpoint saved for fold 5, session 6, epoch 50\n",
      "Session 6 Epoch 51 - Train Loss: 0.104847\n",
      "Checkpoint saved for fold 5, session 6, epoch 51\n",
      "Session 6 Epoch 52 - Train Loss: 0.104815\n",
      "Session 6 Epoch 53 - Train Loss: 0.104397\n",
      "Checkpoint saved for fold 5, session 6, epoch 53\n",
      "Session 6 Epoch 54 - Train Loss: 0.104337\n",
      "Session 6 Epoch 55 - Train Loss: 0.103953\n",
      "Checkpoint saved for fold 5, session 6, epoch 55\n",
      "Session 6 Epoch 56 - Train Loss: 0.103595\n",
      "Checkpoint saved for fold 5, session 6, epoch 56\n",
      "Session 6 Epoch 57 - Train Loss: 0.103399\n",
      "Checkpoint saved for fold 5, session 6, epoch 57\n",
      "Session 6 Epoch 58 - Train Loss: 0.103167\n",
      "Checkpoint saved for fold 5, session 6, epoch 58\n",
      "Session 6 Epoch 59 - Train Loss: 0.102990\n",
      "Checkpoint saved for fold 5, session 6, epoch 59\n",
      "Session 6 Epoch 60 - Train Loss: 0.102965\n",
      "Session 6 Epoch 61 - Train Loss: 0.102693\n",
      "Checkpoint saved for fold 5, session 6, epoch 61\n",
      "Session 6 Epoch 62 - Train Loss: 0.102252\n",
      "Checkpoint saved for fold 5, session 6, epoch 62\n",
      "Session 6 Epoch 63 - Train Loss: 0.102065\n",
      "Checkpoint saved for fold 5, session 6, epoch 63\n",
      "Session 6 Epoch 64 - Train Loss: 0.101718\n",
      "Checkpoint saved for fold 5, session 6, epoch 64\n",
      "Session 6 Epoch 65 - Train Loss: 0.101446\n",
      "Checkpoint saved for fold 5, session 6, epoch 65\n",
      "Session 6 Epoch 66 - Train Loss: 0.101528\n",
      "Session 6 Epoch 67 - Train Loss: 0.101393\n",
      "Session 6 Epoch 68 - Train Loss: 0.101229\n",
      "Checkpoint saved for fold 5, session 6, epoch 68\n",
      "Session 6 Epoch 69 - Train Loss: 0.100837\n",
      "Checkpoint saved for fold 5, session 6, epoch 69\n",
      "Session 6 Epoch 70 - Train Loss: 0.100553\n",
      "Checkpoint saved for fold 5, session 6, epoch 70\n",
      "Session 6 Epoch 71 - Train Loss: 0.100500\n",
      "Session 6 Epoch 72 - Train Loss: 0.100209\n",
      "Checkpoint saved for fold 5, session 6, epoch 72\n",
      "Session 6 Epoch 73 - Train Loss: 0.099918\n",
      "Checkpoint saved for fold 5, session 6, epoch 73\n",
      "Session 6 Epoch 74 - Train Loss: 0.100037\n",
      "Session 6 Epoch 75 - Train Loss: 0.099766\n",
      "Checkpoint saved for fold 5, session 6, epoch 75\n",
      "Session 6 Epoch 76 - Train Loss: 0.099633\n",
      "Checkpoint saved for fold 5, session 6, epoch 76\n",
      "Session 6 Epoch 77 - Train Loss: 0.099324\n",
      "Checkpoint saved for fold 5, session 6, epoch 77\n",
      "Session 6 Epoch 78 - Train Loss: 0.098780\n",
      "Checkpoint saved for fold 5, session 6, epoch 78\n",
      "Session 6 Epoch 79 - Train Loss: 0.098955\n",
      "Session 6 Epoch 80 - Train Loss: 0.098456\n",
      "Checkpoint saved for fold 5, session 6, epoch 80\n",
      "Training on Session 8/9\n",
      "Session 7 Epoch 1 - Train Loss: 0.188368\n",
      "Checkpoint saved for fold 5, session 7, epoch 1\n",
      "Session 7 Epoch 2 - Train Loss: 0.188360\n",
      "Session 7 Epoch 3 - Train Loss: 0.187879\n",
      "Checkpoint saved for fold 5, session 7, epoch 3\n",
      "Session 7 Epoch 4 - Train Loss: 0.187707\n",
      "Checkpoint saved for fold 5, session 7, epoch 4\n",
      "Session 7 Epoch 5 - Train Loss: 0.186920\n",
      "Checkpoint saved for fold 5, session 7, epoch 5\n",
      "Session 7 Epoch 6 - Train Loss: 0.186897\n",
      "Session 7 Epoch 7 - Train Loss: 0.186401\n",
      "Checkpoint saved for fold 5, session 7, epoch 7\n",
      "Session 7 Epoch 8 - Train Loss: 0.186396\n",
      "Session 7 Epoch 9 - Train Loss: 0.186302\n",
      "Session 7 Epoch 10 - Train Loss: 0.185809\n",
      "Checkpoint saved for fold 5, session 7, epoch 10\n",
      "Session 7 Epoch 11 - Train Loss: 0.185150\n",
      "Checkpoint saved for fold 5, session 7, epoch 11\n",
      "Session 7 Epoch 12 - Train Loss: 0.185171\n",
      "Session 7 Epoch 13 - Train Loss: 0.184735\n",
      "Checkpoint saved for fold 5, session 7, epoch 13\n",
      "Session 7 Epoch 14 - Train Loss: 0.184293\n",
      "Checkpoint saved for fold 5, session 7, epoch 14\n",
      "Session 7 Epoch 15 - Train Loss: 0.184232\n",
      "Session 7 Epoch 16 - Train Loss: 0.184114\n",
      "Checkpoint saved for fold 5, session 7, epoch 16\n",
      "Session 7 Epoch 17 - Train Loss: 0.183605\n",
      "Checkpoint saved for fold 5, session 7, epoch 17\n",
      "Session 7 Epoch 18 - Train Loss: 0.183052\n",
      "Checkpoint saved for fold 5, session 7, epoch 18\n",
      "Session 7 Epoch 19 - Train Loss: 0.182905\n",
      "Checkpoint saved for fold 5, session 7, epoch 19\n",
      "Session 7 Epoch 20 - Train Loss: 0.182747\n",
      "Checkpoint saved for fold 5, session 7, epoch 20\n",
      "Session 7 Epoch 21 - Train Loss: 0.182020\n",
      "Checkpoint saved for fold 5, session 7, epoch 21\n",
      "Session 7 Epoch 22 - Train Loss: 0.181985\n",
      "Session 7 Epoch 23 - Train Loss: 0.181803\n",
      "Checkpoint saved for fold 5, session 7, epoch 23\n",
      "Session 7 Epoch 24 - Train Loss: 0.181667\n",
      "Checkpoint saved for fold 5, session 7, epoch 24\n",
      "Session 7 Epoch 25 - Train Loss: 0.181473\n",
      "Checkpoint saved for fold 5, session 7, epoch 25\n",
      "Session 7 Epoch 26 - Train Loss: 0.180570\n",
      "Checkpoint saved for fold 5, session 7, epoch 26\n",
      "Session 7 Epoch 27 - Train Loss: 0.180707\n",
      "Session 7 Epoch 28 - Train Loss: 0.180412\n",
      "Checkpoint saved for fold 5, session 7, epoch 28\n",
      "Session 7 Epoch 29 - Train Loss: 0.180160\n",
      "Checkpoint saved for fold 5, session 7, epoch 29\n",
      "Session 7 Epoch 30 - Train Loss: 0.179652\n",
      "Checkpoint saved for fold 5, session 7, epoch 30\n",
      "Session 7 Epoch 31 - Train Loss: 0.179252\n",
      "Checkpoint saved for fold 5, session 7, epoch 31\n",
      "Session 7 Epoch 32 - Train Loss: 0.178931\n",
      "Checkpoint saved for fold 5, session 7, epoch 32\n",
      "Session 7 Epoch 33 - Train Loss: 0.179457\n",
      "Session 7 Epoch 34 - Train Loss: 0.178456\n",
      "Checkpoint saved for fold 5, session 7, epoch 34\n",
      "Session 7 Epoch 35 - Train Loss: 0.178416\n",
      "Session 7 Epoch 36 - Train Loss: 0.177853\n",
      "Checkpoint saved for fold 5, session 7, epoch 36\n",
      "Session 7 Epoch 37 - Train Loss: 0.178122\n",
      "Session 7 Epoch 38 - Train Loss: 0.177766\n",
      "Session 7 Epoch 39 - Train Loss: 0.177190\n",
      "Checkpoint saved for fold 5, session 7, epoch 39\n",
      "Session 7 Epoch 40 - Train Loss: 0.176978\n",
      "Checkpoint saved for fold 5, session 7, epoch 40\n",
      "Session 7 Epoch 41 - Train Loss: 0.176348\n",
      "Checkpoint saved for fold 5, session 7, epoch 41\n",
      "Session 7 Epoch 42 - Train Loss: 0.176624\n",
      "Session 7 Epoch 43 - Train Loss: 0.176072\n",
      "Checkpoint saved for fold 5, session 7, epoch 43\n",
      "Session 7 Epoch 44 - Train Loss: 0.175957\n",
      "Checkpoint saved for fold 5, session 7, epoch 44\n",
      "Session 7 Epoch 45 - Train Loss: 0.175723\n",
      "Checkpoint saved for fold 5, session 7, epoch 45\n",
      "Session 7 Epoch 46 - Train Loss: 0.175447\n",
      "Checkpoint saved for fold 5, session 7, epoch 46\n",
      "Session 7 Epoch 47 - Train Loss: 0.174972\n",
      "Checkpoint saved for fold 5, session 7, epoch 47\n",
      "Session 7 Epoch 48 - Train Loss: 0.174932\n",
      "Session 7 Epoch 49 - Train Loss: 0.174662\n",
      "Checkpoint saved for fold 5, session 7, epoch 49\n",
      "Session 7 Epoch 50 - Train Loss: 0.174337\n",
      "Checkpoint saved for fold 5, session 7, epoch 50\n",
      "Session 7 Epoch 51 - Train Loss: 0.174029\n",
      "Checkpoint saved for fold 5, session 7, epoch 51\n",
      "Session 7 Epoch 52 - Train Loss: 0.173851\n",
      "Checkpoint saved for fold 5, session 7, epoch 52\n",
      "Session 7 Epoch 53 - Train Loss: 0.173979\n",
      "Session 7 Epoch 54 - Train Loss: 0.173296\n",
      "Checkpoint saved for fold 5, session 7, epoch 54\n",
      "Session 7 Epoch 55 - Train Loss: 0.173010\n",
      "Checkpoint saved for fold 5, session 7, epoch 55\n",
      "Session 7 Epoch 56 - Train Loss: 0.173079\n",
      "Session 7 Epoch 57 - Train Loss: 0.172374\n",
      "Checkpoint saved for fold 5, session 7, epoch 57\n",
      "Session 7 Epoch 58 - Train Loss: 0.172458\n",
      "Session 7 Epoch 59 - Train Loss: 0.172062\n",
      "Checkpoint saved for fold 5, session 7, epoch 59\n",
      "Session 7 Epoch 60 - Train Loss: 0.171694\n",
      "Checkpoint saved for fold 5, session 7, epoch 60\n",
      "Session 7 Epoch 61 - Train Loss: 0.171803\n",
      "Session 7 Epoch 62 - Train Loss: 0.171183\n",
      "Checkpoint saved for fold 5, session 7, epoch 62\n",
      "Session 7 Epoch 63 - Train Loss: 0.171154\n",
      "Session 7 Epoch 64 - Train Loss: 0.170825\n",
      "Checkpoint saved for fold 5, session 7, epoch 64\n",
      "Session 7 Epoch 65 - Train Loss: 0.170449\n",
      "Checkpoint saved for fold 5, session 7, epoch 65\n",
      "Session 7 Epoch 66 - Train Loss: 0.170566\n",
      "Session 7 Epoch 67 - Train Loss: 0.170024\n",
      "Checkpoint saved for fold 5, session 7, epoch 67\n",
      "Session 7 Epoch 68 - Train Loss: 0.169708\n",
      "Checkpoint saved for fold 5, session 7, epoch 68\n",
      "Session 7 Epoch 69 - Train Loss: 0.169399\n",
      "Checkpoint saved for fold 5, session 7, epoch 69\n",
      "Session 7 Epoch 70 - Train Loss: 0.168945\n",
      "Checkpoint saved for fold 5, session 7, epoch 70\n",
      "Session 7 Epoch 71 - Train Loss: 0.169026\n",
      "Session 7 Epoch 72 - Train Loss: 0.169218\n",
      "Session 7 Epoch 73 - Train Loss: 0.168628\n",
      "Checkpoint saved for fold 5, session 7, epoch 73\n",
      "Session 7 Epoch 74 - Train Loss: 0.168465\n",
      "Checkpoint saved for fold 5, session 7, epoch 74\n",
      "Session 7 Epoch 75 - Train Loss: 0.167887\n",
      "Checkpoint saved for fold 5, session 7, epoch 75\n",
      "Session 7 Epoch 76 - Train Loss: 0.167710\n",
      "Checkpoint saved for fold 5, session 7, epoch 76\n",
      "Session 7 Epoch 77 - Train Loss: 0.167491\n",
      "Checkpoint saved for fold 5, session 7, epoch 77\n",
      "Session 7 Epoch 78 - Train Loss: 0.167406\n",
      "Session 7 Epoch 79 - Train Loss: 0.167047\n",
      "Checkpoint saved for fold 5, session 7, epoch 79\n",
      "Session 7 Epoch 80 - Train Loss: 0.167002\n",
      "Training on Session 9/9\n",
      "Session 8 Epoch 1 - Train Loss: 0.126440\n",
      "Checkpoint saved for fold 5, session 8, epoch 1\n",
      "Session 8 Epoch 2 - Train Loss: 0.126471\n",
      "Session 8 Epoch 3 - Train Loss: 0.126444\n",
      "Session 8 Epoch 4 - Train Loss: 0.125962\n",
      "Checkpoint saved for fold 5, session 8, epoch 4\n",
      "Session 8 Epoch 5 - Train Loss: 0.125753\n",
      "Checkpoint saved for fold 5, session 8, epoch 5\n",
      "Session 8 Epoch 6 - Train Loss: 0.125574\n",
      "Checkpoint saved for fold 5, session 8, epoch 6\n",
      "Session 8 Epoch 7 - Train Loss: 0.125327\n",
      "Checkpoint saved for fold 5, session 8, epoch 7\n",
      "Session 8 Epoch 8 - Train Loss: 0.125344\n",
      "Session 8 Epoch 9 - Train Loss: 0.125222\n",
      "Checkpoint saved for fold 5, session 8, epoch 9\n",
      "Session 8 Epoch 10 - Train Loss: 0.124722\n",
      "Checkpoint saved for fold 5, session 8, epoch 10\n",
      "Session 8 Epoch 11 - Train Loss: 0.124354\n",
      "Checkpoint saved for fold 5, session 8, epoch 11\n",
      "Session 8 Epoch 12 - Train Loss: 0.124202\n",
      "Checkpoint saved for fold 5, session 8, epoch 12\n",
      "Session 8 Epoch 13 - Train Loss: 0.124215\n",
      "Session 8 Epoch 14 - Train Loss: 0.123801\n",
      "Checkpoint saved for fold 5, session 8, epoch 14\n",
      "Session 8 Epoch 15 - Train Loss: 0.123466\n",
      "Checkpoint saved for fold 5, session 8, epoch 15\n",
      "Session 8 Epoch 16 - Train Loss: 0.123647\n",
      "Session 8 Epoch 17 - Train Loss: 0.123770\n",
      "Session 8 Epoch 18 - Train Loss: 0.123125\n",
      "Checkpoint saved for fold 5, session 8, epoch 18\n",
      "Session 8 Epoch 19 - Train Loss: 0.123007\n",
      "Checkpoint saved for fold 5, session 8, epoch 19\n",
      "Session 8 Epoch 20 - Train Loss: 0.122975\n",
      "Session 8 Epoch 21 - Train Loss: 0.122753\n",
      "Checkpoint saved for fold 5, session 8, epoch 21\n",
      "Session 8 Epoch 22 - Train Loss: 0.122315\n",
      "Checkpoint saved for fold 5, session 8, epoch 22\n",
      "Session 8 Epoch 23 - Train Loss: 0.122068\n",
      "Checkpoint saved for fold 5, session 8, epoch 23\n",
      "Session 8 Epoch 24 - Train Loss: 0.121789\n",
      "Checkpoint saved for fold 5, session 8, epoch 24\n",
      "Session 8 Epoch 25 - Train Loss: 0.121589\n",
      "Checkpoint saved for fold 5, session 8, epoch 25\n",
      "Session 8 Epoch 26 - Train Loss: 0.121567\n",
      "Session 8 Epoch 27 - Train Loss: 0.121587\n",
      "Session 8 Epoch 28 - Train Loss: 0.121204\n",
      "Checkpoint saved for fold 5, session 8, epoch 28\n",
      "Session 8 Epoch 29 - Train Loss: 0.120788\n",
      "Checkpoint saved for fold 5, session 8, epoch 29\n",
      "Session 8 Epoch 30 - Train Loss: 0.120878\n",
      "Session 8 Epoch 31 - Train Loss: 0.120431\n",
      "Checkpoint saved for fold 5, session 8, epoch 31\n",
      "Session 8 Epoch 32 - Train Loss: 0.120196\n",
      "Checkpoint saved for fold 5, session 8, epoch 32\n",
      "Session 8 Epoch 33 - Train Loss: 0.120021\n",
      "Checkpoint saved for fold 5, session 8, epoch 33\n",
      "Session 8 Epoch 34 - Train Loss: 0.119779\n",
      "Checkpoint saved for fold 5, session 8, epoch 34\n",
      "Session 8 Epoch 35 - Train Loss: 0.119969\n",
      "Session 8 Epoch 36 - Train Loss: 0.119809\n",
      "Session 8 Epoch 37 - Train Loss: 0.119349\n",
      "Checkpoint saved for fold 5, session 8, epoch 37\n",
      "Session 8 Epoch 38 - Train Loss: 0.119335\n",
      "Session 8 Epoch 39 - Train Loss: 0.118985\n",
      "Checkpoint saved for fold 5, session 8, epoch 39\n",
      "Session 8 Epoch 40 - Train Loss: 0.118916\n",
      "Session 8 Epoch 41 - Train Loss: 0.118708\n",
      "Checkpoint saved for fold 5, session 8, epoch 41\n",
      "Session 8 Epoch 42 - Train Loss: 0.118301\n",
      "Checkpoint saved for fold 5, session 8, epoch 42\n",
      "Session 8 Epoch 43 - Train Loss: 0.118146\n",
      "Checkpoint saved for fold 5, session 8, epoch 43\n",
      "Session 8 Epoch 44 - Train Loss: 0.118122\n",
      "Session 8 Epoch 45 - Train Loss: 0.117869\n",
      "Checkpoint saved for fold 5, session 8, epoch 45\n",
      "Session 8 Epoch 46 - Train Loss: 0.117768\n",
      "Checkpoint saved for fold 5, session 8, epoch 46\n",
      "Session 8 Epoch 47 - Train Loss: 0.117245\n",
      "Checkpoint saved for fold 5, session 8, epoch 47\n",
      "Session 8 Epoch 48 - Train Loss: 0.117428\n",
      "Session 8 Epoch 49 - Train Loss: 0.117092\n",
      "Checkpoint saved for fold 5, session 8, epoch 49\n",
      "Session 8 Epoch 50 - Train Loss: 0.117057\n",
      "Session 8 Epoch 51 - Train Loss: 0.116971\n",
      "Checkpoint saved for fold 5, session 8, epoch 51\n",
      "Session 8 Epoch 52 - Train Loss: 0.116877\n",
      "Session 8 Epoch 53 - Train Loss: 0.116443\n",
      "Checkpoint saved for fold 5, session 8, epoch 53\n",
      "Session 8 Epoch 54 - Train Loss: 0.116429\n",
      "Session 8 Epoch 55 - Train Loss: 0.116423\n",
      "Session 8 Epoch 56 - Train Loss: 0.116338\n",
      "Checkpoint saved for fold 5, session 8, epoch 56\n",
      "Session 8 Epoch 57 - Train Loss: 0.115957\n",
      "Checkpoint saved for fold 5, session 8, epoch 57\n",
      "Session 8 Epoch 58 - Train Loss: 0.115635\n",
      "Checkpoint saved for fold 5, session 8, epoch 58\n",
      "Session 8 Epoch 59 - Train Loss: 0.115888\n",
      "Session 8 Epoch 60 - Train Loss: 0.115373\n",
      "Checkpoint saved for fold 5, session 8, epoch 60\n",
      "Session 8 Epoch 61 - Train Loss: 0.115283\n",
      "Session 8 Epoch 62 - Train Loss: 0.115042\n",
      "Checkpoint saved for fold 5, session 8, epoch 62\n",
      "Session 8 Epoch 63 - Train Loss: 0.114984\n",
      "Session 8 Epoch 64 - Train Loss: 0.114964\n",
      "Session 8 Epoch 65 - Train Loss: 0.114596\n",
      "Checkpoint saved for fold 5, session 8, epoch 65\n",
      "Session 8 Epoch 66 - Train Loss: 0.114407\n",
      "Checkpoint saved for fold 5, session 8, epoch 66\n",
      "Session 8 Epoch 67 - Train Loss: 0.114143\n",
      "Checkpoint saved for fold 5, session 8, epoch 67\n",
      "Session 8 Epoch 68 - Train Loss: 0.114105\n",
      "Session 8 Epoch 69 - Train Loss: 0.113831\n",
      "Checkpoint saved for fold 5, session 8, epoch 69\n",
      "Session 8 Epoch 70 - Train Loss: 0.113877\n",
      "Session 8 Epoch 71 - Train Loss: 0.113744\n",
      "Session 8 Epoch 72 - Train Loss: 0.113528\n",
      "Checkpoint saved for fold 5, session 8, epoch 72\n",
      "Session 8 Epoch 73 - Train Loss: 0.113434\n",
      "Session 8 Epoch 74 - Train Loss: 0.113129\n",
      "Checkpoint saved for fold 5, session 8, epoch 74\n",
      "Session 8 Epoch 75 - Train Loss: 0.112905\n",
      "Checkpoint saved for fold 5, session 8, epoch 75\n",
      "Session 8 Epoch 76 - Train Loss: 0.112920\n",
      "Session 8 Epoch 77 - Train Loss: 0.112714\n",
      "Checkpoint saved for fold 5, session 8, epoch 77\n",
      "Session 8 Epoch 78 - Train Loss: 0.112414\n",
      "Checkpoint saved for fold 5, session 8, epoch 78\n",
      "Session 8 Epoch 79 - Train Loss: 0.112700\n",
      "Session 8 Epoch 80 - Train Loss: 0.112307\n",
      "Checkpoint saved for fold 5, session 8, epoch 80\n",
      "Fold 5 - Test Loss: 0.2903, R^2: -2017141352979.1960\n",
      "\n",
      "=== Fold 6 ===\n",
      "Training on Session 1/9\n",
      "Session 0 Epoch 1 - Train Loss: 0.054579\n",
      "Checkpoint saved for fold 6, session 0, epoch 1\n",
      "Session 0 Epoch 2 - Train Loss: 0.053169\n",
      "Checkpoint saved for fold 6, session 0, epoch 2\n",
      "Session 0 Epoch 3 - Train Loss: 0.051467\n",
      "Checkpoint saved for fold 6, session 0, epoch 3\n",
      "Session 0 Epoch 4 - Train Loss: 0.047409\n",
      "Checkpoint saved for fold 6, session 0, epoch 4\n",
      "Session 0 Epoch 5 - Train Loss: 0.040314\n",
      "Checkpoint saved for fold 6, session 0, epoch 5\n",
      "Session 0 Epoch 6 - Train Loss: 0.035724\n",
      "Checkpoint saved for fold 6, session 0, epoch 6\n",
      "Session 0 Epoch 7 - Train Loss: 0.031487\n",
      "Checkpoint saved for fold 6, session 0, epoch 7\n",
      "Session 0 Epoch 8 - Train Loss: 0.029958\n",
      "Checkpoint saved for fold 6, session 0, epoch 8\n",
      "Session 0 Epoch 9 - Train Loss: 0.028214\n",
      "Checkpoint saved for fold 6, session 0, epoch 9\n",
      "Session 0 Epoch 10 - Train Loss: 0.027075\n",
      "Checkpoint saved for fold 6, session 0, epoch 10\n",
      "Session 0 Epoch 11 - Train Loss: 0.026170\n",
      "Checkpoint saved for fold 6, session 0, epoch 11\n",
      "Session 0 Epoch 12 - Train Loss: 0.025028\n",
      "Checkpoint saved for fold 6, session 0, epoch 12\n",
      "Session 0 Epoch 13 - Train Loss: 0.023848\n",
      "Checkpoint saved for fold 6, session 0, epoch 13\n",
      "Session 0 Epoch 14 - Train Loss: 0.023831\n",
      "Session 0 Epoch 15 - Train Loss: 0.022613\n",
      "Checkpoint saved for fold 6, session 0, epoch 15\n",
      "Session 0 Epoch 16 - Train Loss: 0.021019\n",
      "Checkpoint saved for fold 6, session 0, epoch 16\n",
      "Session 0 Epoch 17 - Train Loss: 0.019788\n",
      "Checkpoint saved for fold 6, session 0, epoch 17\n",
      "Session 0 Epoch 18 - Train Loss: 0.019973\n",
      "Session 0 Epoch 19 - Train Loss: 0.019163\n",
      "Checkpoint saved for fold 6, session 0, epoch 19\n",
      "Session 0 Epoch 20 - Train Loss: 0.018975\n",
      "Checkpoint saved for fold 6, session 0, epoch 20\n",
      "Session 0 Epoch 21 - Train Loss: 0.017891\n",
      "Checkpoint saved for fold 6, session 0, epoch 21\n",
      "Session 0 Epoch 22 - Train Loss: 0.017821\n",
      "Session 0 Epoch 23 - Train Loss: 0.017880\n",
      "Session 0 Epoch 24 - Train Loss: 0.017021\n",
      "Checkpoint saved for fold 6, session 0, epoch 24\n",
      "Session 0 Epoch 25 - Train Loss: 0.017120\n",
      "Session 0 Epoch 26 - Train Loss: 0.017892\n",
      "Session 0 Epoch 27 - Train Loss: 0.017091\n",
      "Session 0 Epoch 28 - Train Loss: 0.016621\n",
      "Checkpoint saved for fold 6, session 0, epoch 28\n",
      "Session 0 Epoch 29 - Train Loss: 0.016642\n",
      "Session 0 Epoch 30 - Train Loss: 0.018643\n",
      "Session 0 Epoch 31 - Train Loss: 0.016504\n",
      "Checkpoint saved for fold 6, session 0, epoch 31\n",
      "Session 0 Epoch 32 - Train Loss: 0.015410\n",
      "Checkpoint saved for fold 6, session 0, epoch 32\n",
      "Session 0 Epoch 33 - Train Loss: 0.015958\n",
      "Session 0 Epoch 34 - Train Loss: 0.016197\n",
      "Session 0 Epoch 35 - Train Loss: 0.015542\n",
      "Session 0 Epoch 36 - Train Loss: 0.015242\n",
      "Checkpoint saved for fold 6, session 0, epoch 36\n",
      "Session 0 Epoch 37 - Train Loss: 0.014939\n",
      "Checkpoint saved for fold 6, session 0, epoch 37\n",
      "Session 0 Epoch 38 - Train Loss: 0.015055\n",
      "Session 0 Epoch 39 - Train Loss: 0.015395\n",
      "Session 0 Epoch 40 - Train Loss: 0.014551\n",
      "Checkpoint saved for fold 6, session 0, epoch 40\n",
      "Session 0 Epoch 41 - Train Loss: 0.013861\n",
      "Checkpoint saved for fold 6, session 0, epoch 41\n",
      "Session 0 Epoch 42 - Train Loss: 0.014524\n",
      "Session 0 Epoch 43 - Train Loss: 0.014793\n",
      "Session 0 Epoch 44 - Train Loss: 0.013404\n",
      "Checkpoint saved for fold 6, session 0, epoch 44\n",
      "Session 0 Epoch 45 - Train Loss: 0.013234\n",
      "Checkpoint saved for fold 6, session 0, epoch 45\n",
      "Session 0 Epoch 46 - Train Loss: 0.014201\n",
      "Session 0 Epoch 47 - Train Loss: 0.013144\n",
      "Session 0 Epoch 48 - Train Loss: 0.013182\n",
      "Session 0 Epoch 49 - Train Loss: 0.013426\n",
      "Session 0 Epoch 50 - Train Loss: 0.011783\n",
      "Checkpoint saved for fold 6, session 0, epoch 50\n",
      "Session 0 Epoch 51 - Train Loss: 0.011211\n",
      "Checkpoint saved for fold 6, session 0, epoch 51\n",
      "Session 0 Epoch 52 - Train Loss: 0.011225\n",
      "Session 0 Epoch 53 - Train Loss: 0.010561\n",
      "Checkpoint saved for fold 6, session 0, epoch 53\n",
      "Session 0 Epoch 54 - Train Loss: 0.010732\n",
      "Session 0 Epoch 55 - Train Loss: 0.010035\n",
      "Checkpoint saved for fold 6, session 0, epoch 55\n",
      "Session 0 Epoch 56 - Train Loss: 0.009679\n",
      "Checkpoint saved for fold 6, session 0, epoch 56\n",
      "Session 0 Epoch 57 - Train Loss: 0.009770\n",
      "Session 0 Epoch 58 - Train Loss: 0.008669\n",
      "Checkpoint saved for fold 6, session 0, epoch 58\n",
      "Session 0 Epoch 59 - Train Loss: 0.009007\n",
      "Session 0 Epoch 60 - Train Loss: 0.008691\n",
      "Session 0 Epoch 61 - Train Loss: 0.008498\n",
      "Checkpoint saved for fold 6, session 0, epoch 61\n",
      "Session 0 Epoch 62 - Train Loss: 0.008167\n",
      "Checkpoint saved for fold 6, session 0, epoch 62\n",
      "Session 0 Epoch 63 - Train Loss: 0.008942\n",
      "Session 0 Epoch 64 - Train Loss: 0.008612\n",
      "Session 0 Epoch 65 - Train Loss: 0.007883\n",
      "Checkpoint saved for fold 6, session 0, epoch 65\n",
      "Session 0 Epoch 66 - Train Loss: 0.007811\n",
      "Session 0 Epoch 67 - Train Loss: 0.007809\n",
      "Session 0 Epoch 68 - Train Loss: 0.007735\n",
      "Checkpoint saved for fold 6, session 0, epoch 68\n",
      "Session 0 Epoch 69 - Train Loss: 0.007771\n",
      "Session 0 Epoch 70 - Train Loss: 0.007773\n",
      "Session 0 Epoch 71 - Train Loss: 0.007807\n",
      "Session 0 Epoch 72 - Train Loss: 0.007584\n",
      "Checkpoint saved for fold 6, session 0, epoch 72\n",
      "Session 0 Epoch 73 - Train Loss: 0.007132\n",
      "Checkpoint saved for fold 6, session 0, epoch 73\n",
      "Session 0 Epoch 74 - Train Loss: 0.007441\n",
      "Session 0 Epoch 75 - Train Loss: 0.007120\n",
      "Session 0 Epoch 76 - Train Loss: 0.006886\n",
      "Checkpoint saved for fold 6, session 0, epoch 76\n",
      "Session 0 Epoch 77 - Train Loss: 0.007148\n",
      "Session 0 Epoch 78 - Train Loss: 0.006835\n",
      "Session 0 Epoch 79 - Train Loss: 0.007144\n",
      "Session 0 Epoch 80 - Train Loss: 0.007192\n",
      "Training on Session 2/9\n",
      "Session 1 Epoch 1 - Train Loss: 0.051214\n",
      "Checkpoint saved for fold 6, session 1, epoch 1\n",
      "Session 1 Epoch 2 - Train Loss: 0.050867\n",
      "Checkpoint saved for fold 6, session 1, epoch 2\n",
      "Session 1 Epoch 3 - Train Loss: 0.050848\n",
      "Session 1 Epoch 4 - Train Loss: 0.050791\n",
      "Session 1 Epoch 5 - Train Loss: 0.049622\n",
      "Checkpoint saved for fold 6, session 1, epoch 5\n",
      "Session 1 Epoch 6 - Train Loss: 0.043349\n",
      "Checkpoint saved for fold 6, session 1, epoch 6\n",
      "Session 1 Epoch 7 - Train Loss: 0.036650\n",
      "Checkpoint saved for fold 6, session 1, epoch 7\n",
      "Session 1 Epoch 8 - Train Loss: 0.035373\n",
      "Checkpoint saved for fold 6, session 1, epoch 8\n",
      "Session 1 Epoch 9 - Train Loss: 0.032527\n",
      "Checkpoint saved for fold 6, session 1, epoch 9\n",
      "Session 1 Epoch 10 - Train Loss: 0.030138\n",
      "Checkpoint saved for fold 6, session 1, epoch 10\n",
      "Session 1 Epoch 11 - Train Loss: 0.027850\n",
      "Checkpoint saved for fold 6, session 1, epoch 11\n",
      "Session 1 Epoch 12 - Train Loss: 0.026016\n",
      "Checkpoint saved for fold 6, session 1, epoch 12\n",
      "Session 1 Epoch 13 - Train Loss: 0.025395\n",
      "Checkpoint saved for fold 6, session 1, epoch 13\n",
      "Session 1 Epoch 14 - Train Loss: 0.023903\n",
      "Checkpoint saved for fold 6, session 1, epoch 14\n",
      "Session 1 Epoch 15 - Train Loss: 0.023084\n",
      "Checkpoint saved for fold 6, session 1, epoch 15\n",
      "Session 1 Epoch 16 - Train Loss: 0.022548\n",
      "Checkpoint saved for fold 6, session 1, epoch 16\n",
      "Session 1 Epoch 17 - Train Loss: 0.021852\n",
      "Checkpoint saved for fold 6, session 1, epoch 17\n",
      "Session 1 Epoch 18 - Train Loss: 0.021574\n",
      "Checkpoint saved for fold 6, session 1, epoch 18\n",
      "Session 1 Epoch 19 - Train Loss: 0.021330\n",
      "Checkpoint saved for fold 6, session 1, epoch 19\n",
      "Session 1 Epoch 20 - Train Loss: 0.020890\n",
      "Checkpoint saved for fold 6, session 1, epoch 20\n",
      "Session 1 Epoch 21 - Train Loss: 0.020465\n",
      "Checkpoint saved for fold 6, session 1, epoch 21\n",
      "Session 1 Epoch 22 - Train Loss: 0.019843\n",
      "Checkpoint saved for fold 6, session 1, epoch 22\n",
      "Session 1 Epoch 23 - Train Loss: 0.019533\n",
      "Checkpoint saved for fold 6, session 1, epoch 23\n",
      "Session 1 Epoch 24 - Train Loss: 0.019359\n",
      "Checkpoint saved for fold 6, session 1, epoch 24\n",
      "Session 1 Epoch 25 - Train Loss: 0.019373\n",
      "Session 1 Epoch 26 - Train Loss: 0.019068\n",
      "Checkpoint saved for fold 6, session 1, epoch 26\n",
      "Session 1 Epoch 27 - Train Loss: 0.018821\n",
      "Checkpoint saved for fold 6, session 1, epoch 27\n",
      "Session 1 Epoch 28 - Train Loss: 0.018422\n",
      "Checkpoint saved for fold 6, session 1, epoch 28\n",
      "Session 1 Epoch 29 - Train Loss: 0.018213\n",
      "Checkpoint saved for fold 6, session 1, epoch 29\n",
      "Session 1 Epoch 30 - Train Loss: 0.018093\n",
      "Checkpoint saved for fold 6, session 1, epoch 30\n",
      "Session 1 Epoch 31 - Train Loss: 0.017855\n",
      "Checkpoint saved for fold 6, session 1, epoch 31\n",
      "Session 1 Epoch 32 - Train Loss: 0.018080\n",
      "Session 1 Epoch 33 - Train Loss: 0.017685\n",
      "Checkpoint saved for fold 6, session 1, epoch 33\n",
      "Session 1 Epoch 34 - Train Loss: 0.017589\n",
      "Session 1 Epoch 35 - Train Loss: 0.017326\n",
      "Checkpoint saved for fold 6, session 1, epoch 35\n",
      "Session 1 Epoch 36 - Train Loss: 0.017550\n",
      "Session 1 Epoch 37 - Train Loss: 0.017179\n",
      "Checkpoint saved for fold 6, session 1, epoch 37\n",
      "Session 1 Epoch 38 - Train Loss: 0.017397\n",
      "Session 1 Epoch 39 - Train Loss: 0.016982\n",
      "Checkpoint saved for fold 6, session 1, epoch 39\n",
      "Session 1 Epoch 40 - Train Loss: 0.017046\n",
      "Session 1 Epoch 41 - Train Loss: 0.017171\n",
      "Session 1 Epoch 42 - Train Loss: 0.017112\n",
      "Session 1 Epoch 43 - Train Loss: 0.016904\n",
      "Session 1 Epoch 44 - Train Loss: 0.016944\n",
      "Session 1 Epoch 45 - Train Loss: 0.016839\n",
      "Checkpoint saved for fold 6, session 1, epoch 45\n",
      "Session 1 Epoch 46 - Train Loss: 0.016805\n",
      "Session 1 Epoch 47 - Train Loss: 0.016853\n",
      "Session 1 Epoch 48 - Train Loss: 0.016917\n",
      "Session 1 Epoch 49 - Train Loss: 0.016830\n",
      "Session 1 Epoch 50 - Train Loss: 0.017018\n",
      "Session 1 Epoch 51 - Train Loss: 0.017002\n",
      "Session 1 Epoch 52 - Train Loss: 0.016836\n",
      "Session 1 Epoch 53 - Train Loss: 0.017006\n",
      "Session 1 Epoch 54 - Train Loss: 0.016722\n",
      "Checkpoint saved for fold 6, session 1, epoch 54\n",
      "Session 1 Epoch 55 - Train Loss: 0.016832\n",
      "Session 1 Epoch 56 - Train Loss: 0.016707\n",
      "Session 1 Epoch 57 - Train Loss: 0.016918\n",
      "Session 1 Epoch 58 - Train Loss: 0.016833\n",
      "Session 1 Epoch 59 - Train Loss: 0.017048\n",
      "Session 1 Epoch 60 - Train Loss: 0.017153\n",
      "Session 1 Epoch 61 - Train Loss: 0.016812\n",
      "Session 1 Epoch 62 - Train Loss: 0.016747\n",
      "Session 1 Epoch 63 - Train Loss: 0.016940\n",
      "Session 1 Epoch 64 - Train Loss: 0.017199\n",
      "Early stopping at epoch 64 for session 1\n",
      "Training on Session 3/9\n",
      "Session 2 Epoch 1 - Train Loss: 0.279321\n",
      "Checkpoint saved for fold 6, session 2, epoch 1\n",
      "Session 2 Epoch 2 - Train Loss: 0.277885\n",
      "Checkpoint saved for fold 6, session 2, epoch 2\n",
      "Session 2 Epoch 3 - Train Loss: 0.275965\n",
      "Checkpoint saved for fold 6, session 2, epoch 3\n",
      "Session 2 Epoch 4 - Train Loss: 0.274631\n",
      "Checkpoint saved for fold 6, session 2, epoch 4\n",
      "Session 2 Epoch 5 - Train Loss: 0.274164\n",
      "Checkpoint saved for fold 6, session 2, epoch 5\n",
      "Session 2 Epoch 6 - Train Loss: 0.273007\n",
      "Checkpoint saved for fold 6, session 2, epoch 6\n",
      "Session 2 Epoch 7 - Train Loss: 0.272282\n",
      "Checkpoint saved for fold 6, session 2, epoch 7\n",
      "Session 2 Epoch 8 - Train Loss: 0.271934\n",
      "Checkpoint saved for fold 6, session 2, epoch 8\n",
      "Session 2 Epoch 9 - Train Loss: 0.270687\n",
      "Checkpoint saved for fold 6, session 2, epoch 9\n",
      "Session 2 Epoch 10 - Train Loss: 0.270265\n",
      "Checkpoint saved for fold 6, session 2, epoch 10\n",
      "Session 2 Epoch 11 - Train Loss: 0.270086\n",
      "Checkpoint saved for fold 6, session 2, epoch 11\n",
      "Session 2 Epoch 12 - Train Loss: 0.269555\n",
      "Checkpoint saved for fold 6, session 2, epoch 12\n",
      "Session 2 Epoch 13 - Train Loss: 0.269382\n",
      "Checkpoint saved for fold 6, session 2, epoch 13\n",
      "Session 2 Epoch 14 - Train Loss: 0.269231\n",
      "Checkpoint saved for fold 6, session 2, epoch 14\n",
      "Session 2 Epoch 15 - Train Loss: 0.268648\n",
      "Checkpoint saved for fold 6, session 2, epoch 15\n",
      "Session 2 Epoch 16 - Train Loss: 0.268497\n",
      "Checkpoint saved for fold 6, session 2, epoch 16\n",
      "Session 2 Epoch 17 - Train Loss: 0.268486\n",
      "Session 2 Epoch 18 - Train Loss: 0.268106\n",
      "Checkpoint saved for fold 6, session 2, epoch 18\n",
      "Session 2 Epoch 19 - Train Loss: 0.267773\n",
      "Checkpoint saved for fold 6, session 2, epoch 19\n",
      "Session 2 Epoch 20 - Train Loss: 0.268161\n",
      "Session 2 Epoch 21 - Train Loss: 0.268115\n",
      "Session 2 Epoch 22 - Train Loss: 0.267404\n",
      "Checkpoint saved for fold 6, session 2, epoch 22\n",
      "Session 2 Epoch 23 - Train Loss: 0.267629\n",
      "Session 2 Epoch 24 - Train Loss: 0.267707\n",
      "Session 2 Epoch 25 - Train Loss: 0.267469\n",
      "Session 2 Epoch 26 - Train Loss: 0.267805\n",
      "Session 2 Epoch 27 - Train Loss: 0.267379\n",
      "Session 2 Epoch 28 - Train Loss: 0.267275\n",
      "Checkpoint saved for fold 6, session 2, epoch 28\n",
      "Session 2 Epoch 29 - Train Loss: 0.267324\n",
      "Session 2 Epoch 30 - Train Loss: 0.267688\n",
      "Session 2 Epoch 31 - Train Loss: 0.267366\n",
      "Session 2 Epoch 32 - Train Loss: 0.267077\n",
      "Checkpoint saved for fold 6, session 2, epoch 32\n",
      "Session 2 Epoch 33 - Train Loss: 0.267521\n",
      "Session 2 Epoch 34 - Train Loss: 0.267214\n",
      "Session 2 Epoch 35 - Train Loss: 0.267349\n",
      "Session 2 Epoch 36 - Train Loss: 0.266960\n",
      "Checkpoint saved for fold 6, session 2, epoch 36\n",
      "Session 2 Epoch 37 - Train Loss: 0.267152\n",
      "Session 2 Epoch 38 - Train Loss: 0.267422\n",
      "Session 2 Epoch 39 - Train Loss: 0.266954\n",
      "Session 2 Epoch 40 - Train Loss: 0.266934\n",
      "Session 2 Epoch 41 - Train Loss: 0.266818\n",
      "Checkpoint saved for fold 6, session 2, epoch 41\n",
      "Session 2 Epoch 42 - Train Loss: 0.266794\n",
      "Session 2 Epoch 43 - Train Loss: 0.267127\n",
      "Session 2 Epoch 44 - Train Loss: 0.267024\n",
      "Session 2 Epoch 45 - Train Loss: 0.267075\n",
      "Session 2 Epoch 46 - Train Loss: 0.266801\n",
      "Session 2 Epoch 47 - Train Loss: 0.266625\n",
      "Checkpoint saved for fold 6, session 2, epoch 47\n",
      "Session 2 Epoch 48 - Train Loss: 0.266987\n",
      "Session 2 Epoch 49 - Train Loss: 0.266592\n",
      "Session 2 Epoch 50 - Train Loss: 0.266692\n",
      "Session 2 Epoch 51 - Train Loss: 0.266841\n",
      "Session 2 Epoch 52 - Train Loss: 0.266520\n",
      "Checkpoint saved for fold 6, session 2, epoch 52\n",
      "Session 2 Epoch 53 - Train Loss: 0.266658\n",
      "Session 2 Epoch 54 - Train Loss: 0.266723\n",
      "Session 2 Epoch 55 - Train Loss: 0.266721\n",
      "Session 2 Epoch 56 - Train Loss: 0.266413\n",
      "Checkpoint saved for fold 6, session 2, epoch 56\n",
      "Session 2 Epoch 57 - Train Loss: 0.266854\n",
      "Session 2 Epoch 58 - Train Loss: 0.266498\n",
      "Session 2 Epoch 59 - Train Loss: 0.266852\n",
      "Session 2 Epoch 60 - Train Loss: 0.266825\n",
      "Session 2 Epoch 61 - Train Loss: 0.266812\n",
      "Session 2 Epoch 62 - Train Loss: 0.266560\n",
      "Session 2 Epoch 63 - Train Loss: 0.266374\n",
      "Session 2 Epoch 64 - Train Loss: 0.266739\n",
      "Session 2 Epoch 65 - Train Loss: 0.266364\n",
      "Session 2 Epoch 66 - Train Loss: 0.266772\n",
      "Early stopping at epoch 66 for session 2\n",
      "Training on Session 4/9\n",
      "Session 3 Epoch 1 - Train Loss: 0.148076\n",
      "Checkpoint saved for fold 6, session 3, epoch 1\n",
      "Session 3 Epoch 2 - Train Loss: 0.147595\n",
      "Checkpoint saved for fold 6, session 3, epoch 2\n",
      "Session 3 Epoch 3 - Train Loss: 0.147777\n",
      "Session 3 Epoch 4 - Train Loss: 0.147877\n",
      "Session 3 Epoch 5 - Train Loss: 0.148112\n",
      "Session 3 Epoch 6 - Train Loss: 0.147784\n",
      "Session 3 Epoch 7 - Train Loss: 0.148104\n",
      "Session 3 Epoch 8 - Train Loss: 0.148209\n",
      "Session 3 Epoch 9 - Train Loss: 0.147726\n",
      "Session 3 Epoch 10 - Train Loss: 0.147846\n",
      "Session 3 Epoch 11 - Train Loss: 0.147728\n",
      "Session 3 Epoch 12 - Train Loss: 0.147628\n",
      "Early stopping at epoch 12 for session 3\n",
      "Training on Session 5/9\n",
      "Session 4 Epoch 1 - Train Loss: 0.055133\n",
      "Checkpoint saved for fold 6, session 4, epoch 1\n",
      "Session 4 Epoch 2 - Train Loss: 0.054857\n",
      "Checkpoint saved for fold 6, session 4, epoch 2\n",
      "Session 4 Epoch 3 - Train Loss: 0.055380\n",
      "Session 4 Epoch 4 - Train Loss: 0.054870\n",
      "Session 4 Epoch 5 - Train Loss: 0.054889\n",
      "Session 4 Epoch 6 - Train Loss: 0.055038\n",
      "Session 4 Epoch 7 - Train Loss: 0.055308\n",
      "Session 4 Epoch 8 - Train Loss: 0.055128\n",
      "Session 4 Epoch 9 - Train Loss: 0.054740\n",
      "Checkpoint saved for fold 6, session 4, epoch 9\n",
      "Session 4 Epoch 10 - Train Loss: 0.055327\n",
      "Session 4 Epoch 11 - Train Loss: 0.055242\n",
      "Session 4 Epoch 12 - Train Loss: 0.055187\n",
      "Session 4 Epoch 13 - Train Loss: 0.055247\n",
      "Session 4 Epoch 14 - Train Loss: 0.055005\n",
      "Session 4 Epoch 15 - Train Loss: 0.055356\n",
      "Session 4 Epoch 16 - Train Loss: 0.055198\n",
      "Session 4 Epoch 17 - Train Loss: 0.054904\n",
      "Session 4 Epoch 18 - Train Loss: 0.055266\n",
      "Session 4 Epoch 19 - Train Loss: 0.055048\n",
      "Early stopping at epoch 19 for session 4\n",
      "Training on Session 6/9\n",
      "Session 5 Epoch 1 - Train Loss: 0.224819\n",
      "Checkpoint saved for fold 6, session 5, epoch 1\n",
      "Session 5 Epoch 2 - Train Loss: 0.224881\n",
      "Session 5 Epoch 3 - Train Loss: 0.224624\n",
      "Checkpoint saved for fold 6, session 5, epoch 3\n",
      "Session 5 Epoch 4 - Train Loss: 0.224555\n",
      "Session 5 Epoch 5 - Train Loss: 0.224419\n",
      "Checkpoint saved for fold 6, session 5, epoch 5\n",
      "Session 5 Epoch 6 - Train Loss: 0.224542\n",
      "Session 5 Epoch 7 - Train Loss: 0.224578\n",
      "Session 5 Epoch 8 - Train Loss: 0.224327\n",
      "Session 5 Epoch 9 - Train Loss: 0.224344\n",
      "Session 5 Epoch 10 - Train Loss: 0.223955\n",
      "Checkpoint saved for fold 6, session 5, epoch 10\n",
      "Session 5 Epoch 11 - Train Loss: 0.224389\n",
      "Session 5 Epoch 12 - Train Loss: 0.224121\n",
      "Session 5 Epoch 13 - Train Loss: 0.224815\n",
      "Session 5 Epoch 14 - Train Loss: 0.224984\n",
      "Session 5 Epoch 15 - Train Loss: 0.224618\n",
      "Session 5 Epoch 16 - Train Loss: 0.224431\n",
      "Session 5 Epoch 17 - Train Loss: 0.224335\n",
      "Session 5 Epoch 18 - Train Loss: 0.224551\n",
      "Session 5 Epoch 19 - Train Loss: 0.224264\n",
      "Session 5 Epoch 20 - Train Loss: 0.224193\n",
      "Early stopping at epoch 20 for session 5\n",
      "Training on Session 7/9\n",
      "Session 6 Epoch 1 - Train Loss: 0.063506\n",
      "Checkpoint saved for fold 6, session 6, epoch 1\n",
      "Session 6 Epoch 2 - Train Loss: 0.063961\n",
      "Session 6 Epoch 3 - Train Loss: 0.063986\n",
      "Session 6 Epoch 4 - Train Loss: 0.063808\n",
      "Session 6 Epoch 5 - Train Loss: 0.063535\n",
      "Session 6 Epoch 6 - Train Loss: 0.063590\n",
      "Session 6 Epoch 7 - Train Loss: 0.063978\n",
      "Session 6 Epoch 8 - Train Loss: 0.063658\n",
      "Session 6 Epoch 9 - Train Loss: 0.063867\n",
      "Session 6 Epoch 10 - Train Loss: 0.063619\n",
      "Session 6 Epoch 11 - Train Loss: 0.064393\n",
      "Early stopping at epoch 11 for session 6\n",
      "Training on Session 8/9\n",
      "Session 7 Epoch 1 - Train Loss: 0.124341\n",
      "Checkpoint saved for fold 6, session 7, epoch 1\n",
      "Session 7 Epoch 2 - Train Loss: 0.124325\n",
      "Session 7 Epoch 3 - Train Loss: 0.124619\n",
      "Session 7 Epoch 4 - Train Loss: 0.124395\n",
      "Session 7 Epoch 5 - Train Loss: 0.124015\n",
      "Checkpoint saved for fold 6, session 7, epoch 5\n",
      "Session 7 Epoch 6 - Train Loss: 0.124816\n",
      "Session 7 Epoch 7 - Train Loss: 0.123826\n",
      "Checkpoint saved for fold 6, session 7, epoch 7\n",
      "Session 7 Epoch 8 - Train Loss: 0.124064\n",
      "Session 7 Epoch 9 - Train Loss: 0.123797\n",
      "Session 7 Epoch 10 - Train Loss: 0.123445\n",
      "Checkpoint saved for fold 6, session 7, epoch 10\n",
      "Session 7 Epoch 11 - Train Loss: 0.124320\n",
      "Session 7 Epoch 12 - Train Loss: 0.124315\n",
      "Session 7 Epoch 13 - Train Loss: 0.123545\n",
      "Session 7 Epoch 14 - Train Loss: 0.123403\n",
      "Session 7 Epoch 15 - Train Loss: 0.123278\n",
      "Checkpoint saved for fold 6, session 7, epoch 15\n",
      "Session 7 Epoch 16 - Train Loss: 0.124213\n",
      "Session 7 Epoch 17 - Train Loss: 0.123603\n",
      "Session 7 Epoch 18 - Train Loss: 0.123926\n",
      "Session 7 Epoch 19 - Train Loss: 0.124038\n",
      "Session 7 Epoch 20 - Train Loss: 0.123438\n",
      "Session 7 Epoch 21 - Train Loss: 0.123403\n",
      "Session 7 Epoch 22 - Train Loss: 0.123683\n",
      "Session 7 Epoch 23 - Train Loss: 0.123893\n",
      "Session 7 Epoch 24 - Train Loss: 0.123321\n",
      "Session 7 Epoch 25 - Train Loss: 0.123716\n",
      "Early stopping at epoch 25 for session 7\n",
      "Training on Session 9/9\n",
      "Session 8 Epoch 1 - Train Loss: 0.081888\n",
      "Checkpoint saved for fold 6, session 8, epoch 1\n",
      "Session 8 Epoch 2 - Train Loss: 0.081747\n",
      "Checkpoint saved for fold 6, session 8, epoch 2\n",
      "Session 8 Epoch 3 - Train Loss: 0.081328\n",
      "Checkpoint saved for fold 6, session 8, epoch 3\n",
      "Session 8 Epoch 4 - Train Loss: 0.081457\n",
      "Session 8 Epoch 5 - Train Loss: 0.081560\n",
      "Session 8 Epoch 6 - Train Loss: 0.081987\n",
      "Session 8 Epoch 7 - Train Loss: 0.081999\n",
      "Session 8 Epoch 8 - Train Loss: 0.081620\n",
      "Session 8 Epoch 9 - Train Loss: 0.081884\n",
      "Session 8 Epoch 10 - Train Loss: 0.082082\n",
      "Session 8 Epoch 11 - Train Loss: 0.081457\n",
      "Session 8 Epoch 12 - Train Loss: 0.081410\n",
      "Session 8 Epoch 13 - Train Loss: 0.081415\n",
      "Early stopping at epoch 13 for session 8\n",
      "Fold 6 - Test Loss: 0.1641, R^2: -2746117809502.3521\n",
      "\n",
      "=== Fold 7 ===\n",
      "Training on Session 1/9\n",
      "Session 0 Epoch 1 - Train Loss: 0.054386\n",
      "Checkpoint saved for fold 7, session 0, epoch 1\n",
      "Session 0 Epoch 2 - Train Loss: 0.053195\n",
      "Checkpoint saved for fold 7, session 0, epoch 2\n",
      "Session 0 Epoch 3 - Train Loss: 0.051039\n",
      "Checkpoint saved for fold 7, session 0, epoch 3\n",
      "Session 0 Epoch 4 - Train Loss: 0.047466\n",
      "Checkpoint saved for fold 7, session 0, epoch 4\n",
      "Session 0 Epoch 5 - Train Loss: 0.039886\n",
      "Checkpoint saved for fold 7, session 0, epoch 5\n",
      "Session 0 Epoch 6 - Train Loss: 0.034003\n",
      "Checkpoint saved for fold 7, session 0, epoch 6\n",
      "Session 0 Epoch 7 - Train Loss: 0.031516\n",
      "Checkpoint saved for fold 7, session 0, epoch 7\n",
      "Session 0 Epoch 8 - Train Loss: 0.028894\n",
      "Checkpoint saved for fold 7, session 0, epoch 8\n",
      "Session 0 Epoch 9 - Train Loss: 0.028320\n",
      "Checkpoint saved for fold 7, session 0, epoch 9\n",
      "Session 0 Epoch 10 - Train Loss: 0.025630\n",
      "Checkpoint saved for fold 7, session 0, epoch 10\n",
      "Session 0 Epoch 11 - Train Loss: 0.022924\n",
      "Checkpoint saved for fold 7, session 0, epoch 11\n",
      "Session 0 Epoch 12 - Train Loss: 0.022556\n",
      "Checkpoint saved for fold 7, session 0, epoch 12\n",
      "Session 0 Epoch 13 - Train Loss: 0.022165\n",
      "Checkpoint saved for fold 7, session 0, epoch 13\n",
      "Session 0 Epoch 14 - Train Loss: 0.020784\n",
      "Checkpoint saved for fold 7, session 0, epoch 14\n",
      "Session 0 Epoch 15 - Train Loss: 0.020184\n",
      "Checkpoint saved for fold 7, session 0, epoch 15\n",
      "Session 0 Epoch 16 - Train Loss: 0.021091\n",
      "Session 0 Epoch 17 - Train Loss: 0.019324\n",
      "Checkpoint saved for fold 7, session 0, epoch 17\n",
      "Session 0 Epoch 18 - Train Loss: 0.018806\n",
      "Checkpoint saved for fold 7, session 0, epoch 18\n",
      "Session 0 Epoch 19 - Train Loss: 0.018195\n",
      "Checkpoint saved for fold 7, session 0, epoch 19\n",
      "Session 0 Epoch 20 - Train Loss: 0.018267\n",
      "Session 0 Epoch 21 - Train Loss: 0.018613\n",
      "Session 0 Epoch 22 - Train Loss: 0.017384\n",
      "Checkpoint saved for fold 7, session 0, epoch 22\n",
      "Session 0 Epoch 23 - Train Loss: 0.016392\n",
      "Checkpoint saved for fold 7, session 0, epoch 23\n",
      "Session 0 Epoch 24 - Train Loss: 0.016775\n",
      "Session 0 Epoch 25 - Train Loss: 0.015816\n",
      "Checkpoint saved for fold 7, session 0, epoch 25\n",
      "Session 0 Epoch 26 - Train Loss: 0.015829\n",
      "Session 0 Epoch 27 - Train Loss: 0.016367\n",
      "Session 0 Epoch 28 - Train Loss: 0.014496\n",
      "Checkpoint saved for fold 7, session 0, epoch 28\n",
      "Session 0 Epoch 29 - Train Loss: 0.015512\n",
      "Session 0 Epoch 30 - Train Loss: 0.015288\n",
      "Session 0 Epoch 31 - Train Loss: 0.014229\n",
      "Checkpoint saved for fold 7, session 0, epoch 31\n",
      "Session 0 Epoch 32 - Train Loss: 0.014830\n",
      "Session 0 Epoch 33 - Train Loss: 0.013427\n",
      "Checkpoint saved for fold 7, session 0, epoch 33\n",
      "Session 0 Epoch 34 - Train Loss: 0.014114\n",
      "Session 0 Epoch 35 - Train Loss: 0.013951\n",
      "Session 0 Epoch 36 - Train Loss: 0.013260\n",
      "Checkpoint saved for fold 7, session 0, epoch 36\n",
      "Session 0 Epoch 37 - Train Loss: 0.013336\n",
      "Session 0 Epoch 38 - Train Loss: 0.013551\n",
      "Session 0 Epoch 39 - Train Loss: 0.012791\n",
      "Checkpoint saved for fold 7, session 0, epoch 39\n",
      "Session 0 Epoch 40 - Train Loss: 0.013004\n",
      "Session 0 Epoch 41 - Train Loss: 0.012572\n",
      "Checkpoint saved for fold 7, session 0, epoch 41\n",
      "Session 0 Epoch 42 - Train Loss: 0.012106\n",
      "Checkpoint saved for fold 7, session 0, epoch 42\n",
      "Session 0 Epoch 43 - Train Loss: 0.012458\n",
      "Session 0 Epoch 44 - Train Loss: 0.012684\n",
      "Session 0 Epoch 45 - Train Loss: 0.012077\n",
      "Session 0 Epoch 46 - Train Loss: 0.011546\n",
      "Checkpoint saved for fold 7, session 0, epoch 46\n",
      "Session 0 Epoch 47 - Train Loss: 0.011880\n",
      "Session 0 Epoch 48 - Train Loss: 0.012141\n",
      "Session 0 Epoch 49 - Train Loss: 0.011401\n",
      "Checkpoint saved for fold 7, session 0, epoch 49\n",
      "Session 0 Epoch 50 - Train Loss: 0.011160\n",
      "Checkpoint saved for fold 7, session 0, epoch 50\n",
      "Session 0 Epoch 51 - Train Loss: 0.010916\n",
      "Checkpoint saved for fold 7, session 0, epoch 51\n",
      "Session 0 Epoch 52 - Train Loss: 0.011310\n",
      "Session 0 Epoch 53 - Train Loss: 0.010228\n",
      "Checkpoint saved for fold 7, session 0, epoch 53\n",
      "Session 0 Epoch 54 - Train Loss: 0.010857\n",
      "Session 0 Epoch 55 - Train Loss: 0.010448\n",
      "Session 0 Epoch 56 - Train Loss: 0.010531\n",
      "Session 0 Epoch 57 - Train Loss: 0.010268\n",
      "Session 0 Epoch 58 - Train Loss: 0.009946\n",
      "Checkpoint saved for fold 7, session 0, epoch 58\n",
      "Session 0 Epoch 59 - Train Loss: 0.009881\n",
      "Session 0 Epoch 60 - Train Loss: 0.009826\n",
      "Checkpoint saved for fold 7, session 0, epoch 60\n",
      "Session 0 Epoch 61 - Train Loss: 0.010251\n",
      "Session 0 Epoch 62 - Train Loss: 0.009531\n",
      "Checkpoint saved for fold 7, session 0, epoch 62\n",
      "Session 0 Epoch 63 - Train Loss: 0.010063\n",
      "Session 0 Epoch 64 - Train Loss: 0.008992\n",
      "Checkpoint saved for fold 7, session 0, epoch 64\n",
      "Session 0 Epoch 65 - Train Loss: 0.009588\n",
      "Session 0 Epoch 66 - Train Loss: 0.009522\n",
      "Session 0 Epoch 67 - Train Loss: 0.009062\n",
      "Session 0 Epoch 68 - Train Loss: 0.009018\n",
      "Session 0 Epoch 69 - Train Loss: 0.009526\n",
      "Session 0 Epoch 70 - Train Loss: 0.008905\n",
      "Session 0 Epoch 71 - Train Loss: 0.008854\n",
      "Checkpoint saved for fold 7, session 0, epoch 71\n",
      "Session 0 Epoch 72 - Train Loss: 0.008811\n",
      "Session 0 Epoch 73 - Train Loss: 0.009440\n",
      "Session 0 Epoch 74 - Train Loss: 0.008500\n",
      "Checkpoint saved for fold 7, session 0, epoch 74\n",
      "Session 0 Epoch 75 - Train Loss: 0.008566\n",
      "Session 0 Epoch 76 - Train Loss: 0.008335\n",
      "Checkpoint saved for fold 7, session 0, epoch 76\n",
      "Session 0 Epoch 77 - Train Loss: 0.008460\n",
      "Session 0 Epoch 78 - Train Loss: 0.007745\n",
      "Checkpoint saved for fold 7, session 0, epoch 78\n",
      "Session 0 Epoch 79 - Train Loss: 0.007764\n",
      "Session 0 Epoch 80 - Train Loss: 0.007992\n",
      "Training on Session 2/9\n",
      "Session 1 Epoch 1 - Train Loss: 0.051155\n",
      "Checkpoint saved for fold 7, session 1, epoch 1\n",
      "Session 1 Epoch 2 - Train Loss: 0.050868\n",
      "Checkpoint saved for fold 7, session 1, epoch 2\n",
      "Session 1 Epoch 3 - Train Loss: 0.050778\n",
      "Session 1 Epoch 4 - Train Loss: 0.049388\n",
      "Checkpoint saved for fold 7, session 1, epoch 4\n",
      "Session 1 Epoch 5 - Train Loss: 0.043858\n",
      "Checkpoint saved for fold 7, session 1, epoch 5\n",
      "Session 1 Epoch 6 - Train Loss: 0.040867\n",
      "Checkpoint saved for fold 7, session 1, epoch 6\n",
      "Session 1 Epoch 7 - Train Loss: 0.039327\n",
      "Checkpoint saved for fold 7, session 1, epoch 7\n",
      "Session 1 Epoch 8 - Train Loss: 0.039017\n",
      "Checkpoint saved for fold 7, session 1, epoch 8\n",
      "Session 1 Epoch 9 - Train Loss: 0.037717\n",
      "Checkpoint saved for fold 7, session 1, epoch 9\n",
      "Session 1 Epoch 10 - Train Loss: 0.036455\n",
      "Checkpoint saved for fold 7, session 1, epoch 10\n",
      "Session 1 Epoch 11 - Train Loss: 0.034325\n",
      "Checkpoint saved for fold 7, session 1, epoch 11\n",
      "Session 1 Epoch 12 - Train Loss: 0.033280\n",
      "Checkpoint saved for fold 7, session 1, epoch 12\n",
      "Session 1 Epoch 13 - Train Loss: 0.031573\n",
      "Checkpoint saved for fold 7, session 1, epoch 13\n",
      "Session 1 Epoch 14 - Train Loss: 0.030545\n",
      "Checkpoint saved for fold 7, session 1, epoch 14\n",
      "Session 1 Epoch 15 - Train Loss: 0.029152\n",
      "Checkpoint saved for fold 7, session 1, epoch 15\n",
      "Session 1 Epoch 16 - Train Loss: 0.028555\n",
      "Checkpoint saved for fold 7, session 1, epoch 16\n",
      "Session 1 Epoch 17 - Train Loss: 0.027342\n",
      "Checkpoint saved for fold 7, session 1, epoch 17\n",
      "Session 1 Epoch 18 - Train Loss: 0.026251\n",
      "Checkpoint saved for fold 7, session 1, epoch 18\n",
      "Session 1 Epoch 19 - Train Loss: 0.024806\n",
      "Checkpoint saved for fold 7, session 1, epoch 19\n",
      "Session 1 Epoch 20 - Train Loss: 0.023405\n",
      "Checkpoint saved for fold 7, session 1, epoch 20\n",
      "Session 1 Epoch 21 - Train Loss: 0.022605\n",
      "Checkpoint saved for fold 7, session 1, epoch 21\n",
      "Session 1 Epoch 22 - Train Loss: 0.021844\n",
      "Checkpoint saved for fold 7, session 1, epoch 22\n",
      "Session 1 Epoch 23 - Train Loss: 0.021239\n",
      "Checkpoint saved for fold 7, session 1, epoch 23\n",
      "Session 1 Epoch 24 - Train Loss: 0.020918\n",
      "Checkpoint saved for fold 7, session 1, epoch 24\n",
      "Session 1 Epoch 25 - Train Loss: 0.020495\n",
      "Checkpoint saved for fold 7, session 1, epoch 25\n",
      "Session 1 Epoch 26 - Train Loss: 0.020503\n",
      "Session 1 Epoch 27 - Train Loss: 0.019990\n",
      "Checkpoint saved for fold 7, session 1, epoch 27\n",
      "Session 1 Epoch 28 - Train Loss: 0.020023\n",
      "Session 1 Epoch 29 - Train Loss: 0.019207\n",
      "Checkpoint saved for fold 7, session 1, epoch 29\n",
      "Session 1 Epoch 30 - Train Loss: 0.019439\n",
      "Session 1 Epoch 31 - Train Loss: 0.019163\n",
      "Session 1 Epoch 32 - Train Loss: 0.018644\n",
      "Checkpoint saved for fold 7, session 1, epoch 32\n",
      "Session 1 Epoch 33 - Train Loss: 0.018824\n",
      "Session 1 Epoch 34 - Train Loss: 0.018626\n",
      "Session 1 Epoch 35 - Train Loss: 0.018267\n",
      "Checkpoint saved for fold 7, session 1, epoch 35\n",
      "Session 1 Epoch 36 - Train Loss: 0.018620\n",
      "Session 1 Epoch 37 - Train Loss: 0.018096\n",
      "Checkpoint saved for fold 7, session 1, epoch 37\n",
      "Session 1 Epoch 38 - Train Loss: 0.018414\n",
      "Session 1 Epoch 39 - Train Loss: 0.017818\n",
      "Checkpoint saved for fold 7, session 1, epoch 39\n",
      "Session 1 Epoch 40 - Train Loss: 0.018148\n",
      "Session 1 Epoch 41 - Train Loss: 0.018046\n",
      "Session 1 Epoch 42 - Train Loss: 0.017882\n",
      "Session 1 Epoch 43 - Train Loss: 0.017696\n",
      "Checkpoint saved for fold 7, session 1, epoch 43\n",
      "Session 1 Epoch 44 - Train Loss: 0.017523\n",
      "Checkpoint saved for fold 7, session 1, epoch 44\n",
      "Session 1 Epoch 45 - Train Loss: 0.017788\n",
      "Session 1 Epoch 46 - Train Loss: 0.017709\n",
      "Session 1 Epoch 47 - Train Loss: 0.018099\n",
      "Session 1 Epoch 48 - Train Loss: 0.018355\n",
      "Session 1 Epoch 49 - Train Loss: 0.017564\n",
      "Session 1 Epoch 50 - Train Loss: 0.017907\n",
      "Session 1 Epoch 51 - Train Loss: 0.017620\n",
      "Session 1 Epoch 52 - Train Loss: 0.017164\n",
      "Checkpoint saved for fold 7, session 1, epoch 52\n",
      "Session 1 Epoch 53 - Train Loss: 0.017532\n",
      "Session 1 Epoch 54 - Train Loss: 0.017702\n",
      "Session 1 Epoch 55 - Train Loss: 0.017647\n",
      "Session 1 Epoch 56 - Train Loss: 0.017969\n",
      "Session 1 Epoch 57 - Train Loss: 0.017989\n",
      "Session 1 Epoch 58 - Train Loss: 0.017919\n",
      "Session 1 Epoch 59 - Train Loss: 0.017864\n",
      "Session 1 Epoch 60 - Train Loss: 0.017676\n",
      "Session 1 Epoch 61 - Train Loss: 0.017744\n",
      "Session 1 Epoch 62 - Train Loss: 0.017578\n",
      "Early stopping at epoch 62 for session 1\n",
      "Training on Session 3/9\n",
      "Session 2 Epoch 1 - Train Loss: 0.277539\n",
      "Checkpoint saved for fold 7, session 2, epoch 1\n",
      "Session 2 Epoch 2 - Train Loss: 0.274483\n",
      "Checkpoint saved for fold 7, session 2, epoch 2\n",
      "Session 2 Epoch 3 - Train Loss: 0.271953\n",
      "Checkpoint saved for fold 7, session 2, epoch 3\n",
      "Session 2 Epoch 4 - Train Loss: 0.271083\n",
      "Checkpoint saved for fold 7, session 2, epoch 4\n",
      "Session 2 Epoch 5 - Train Loss: 0.269718\n",
      "Checkpoint saved for fold 7, session 2, epoch 5\n",
      "Session 2 Epoch 6 - Train Loss: 0.267997\n",
      "Checkpoint saved for fold 7, session 2, epoch 6\n",
      "Session 2 Epoch 7 - Train Loss: 0.266934\n",
      "Checkpoint saved for fold 7, session 2, epoch 7\n",
      "Session 2 Epoch 8 - Train Loss: 0.265470\n",
      "Checkpoint saved for fold 7, session 2, epoch 8\n",
      "Session 2 Epoch 9 - Train Loss: 0.264277\n",
      "Checkpoint saved for fold 7, session 2, epoch 9\n",
      "Session 2 Epoch 10 - Train Loss: 0.263435\n",
      "Checkpoint saved for fold 7, session 2, epoch 10\n",
      "Session 2 Epoch 11 - Train Loss: 0.262975\n",
      "Checkpoint saved for fold 7, session 2, epoch 11\n",
      "Session 2 Epoch 12 - Train Loss: 0.262425\n",
      "Checkpoint saved for fold 7, session 2, epoch 12\n",
      "Session 2 Epoch 13 - Train Loss: 0.261643\n",
      "Checkpoint saved for fold 7, session 2, epoch 13\n",
      "Session 2 Epoch 14 - Train Loss: 0.261011\n",
      "Checkpoint saved for fold 7, session 2, epoch 14\n",
      "Session 2 Epoch 15 - Train Loss: 0.259943\n",
      "Checkpoint saved for fold 7, session 2, epoch 15\n",
      "Session 2 Epoch 16 - Train Loss: 0.259512\n",
      "Checkpoint saved for fold 7, session 2, epoch 16\n",
      "Session 2 Epoch 17 - Train Loss: 0.259309\n",
      "Checkpoint saved for fold 7, session 2, epoch 17\n",
      "Session 2 Epoch 18 - Train Loss: 0.258919\n",
      "Checkpoint saved for fold 7, session 2, epoch 18\n",
      "Session 2 Epoch 19 - Train Loss: 0.258274\n",
      "Checkpoint saved for fold 7, session 2, epoch 19\n",
      "Session 2 Epoch 20 - Train Loss: 0.258031\n",
      "Checkpoint saved for fold 7, session 2, epoch 20\n",
      "Session 2 Epoch 21 - Train Loss: 0.257965\n",
      "Session 2 Epoch 22 - Train Loss: 0.257356\n",
      "Checkpoint saved for fold 7, session 2, epoch 22\n",
      "Session 2 Epoch 23 - Train Loss: 0.257187\n",
      "Checkpoint saved for fold 7, session 2, epoch 23\n",
      "Session 2 Epoch 24 - Train Loss: 0.256874\n",
      "Checkpoint saved for fold 7, session 2, epoch 24\n",
      "Session 2 Epoch 25 - Train Loss: 0.256790\n",
      "Session 2 Epoch 26 - Train Loss: 0.256537\n",
      "Checkpoint saved for fold 7, session 2, epoch 26\n",
      "Session 2 Epoch 27 - Train Loss: 0.256018\n",
      "Checkpoint saved for fold 7, session 2, epoch 27\n",
      "Session 2 Epoch 28 - Train Loss: 0.255976\n",
      "Session 2 Epoch 29 - Train Loss: 0.256478\n",
      "Session 2 Epoch 30 - Train Loss: 0.255801\n",
      "Checkpoint saved for fold 7, session 2, epoch 30\n",
      "Session 2 Epoch 31 - Train Loss: 0.256016\n",
      "Session 2 Epoch 32 - Train Loss: 0.256370\n",
      "Session 2 Epoch 33 - Train Loss: 0.255584\n",
      "Checkpoint saved for fold 7, session 2, epoch 33\n",
      "Session 2 Epoch 34 - Train Loss: 0.255778\n",
      "Session 2 Epoch 35 - Train Loss: 0.255702\n",
      "Session 2 Epoch 36 - Train Loss: 0.255682\n",
      "Session 2 Epoch 37 - Train Loss: 0.255171\n",
      "Checkpoint saved for fold 7, session 2, epoch 37\n",
      "Session 2 Epoch 38 - Train Loss: 0.255633\n",
      "Session 2 Epoch 39 - Train Loss: 0.255361\n",
      "Session 2 Epoch 40 - Train Loss: 0.255733\n",
      "Session 2 Epoch 41 - Train Loss: 0.255434\n",
      "Session 2 Epoch 42 - Train Loss: 0.255293\n",
      "Session 2 Epoch 43 - Train Loss: 0.255248\n",
      "Session 2 Epoch 44 - Train Loss: 0.255174\n",
      "Session 2 Epoch 45 - Train Loss: 0.255048\n",
      "Checkpoint saved for fold 7, session 2, epoch 45\n",
      "Session 2 Epoch 46 - Train Loss: 0.255237\n",
      "Session 2 Epoch 47 - Train Loss: 0.255075\n",
      "Session 2 Epoch 48 - Train Loss: 0.255286\n",
      "Session 2 Epoch 49 - Train Loss: 0.255563\n",
      "Session 2 Epoch 50 - Train Loss: 0.254908\n",
      "Checkpoint saved for fold 7, session 2, epoch 50\n",
      "Session 2 Epoch 51 - Train Loss: 0.254920\n",
      "Session 2 Epoch 52 - Train Loss: 0.254672\n",
      "Checkpoint saved for fold 7, session 2, epoch 52\n",
      "Session 2 Epoch 53 - Train Loss: 0.254988\n",
      "Session 2 Epoch 54 - Train Loss: 0.254416\n",
      "Checkpoint saved for fold 7, session 2, epoch 54\n",
      "Session 2 Epoch 55 - Train Loss: 0.254611\n",
      "Session 2 Epoch 56 - Train Loss: 0.254315\n",
      "Checkpoint saved for fold 7, session 2, epoch 56\n",
      "Session 2 Epoch 57 - Train Loss: 0.254182\n",
      "Checkpoint saved for fold 7, session 2, epoch 57\n",
      "Session 2 Epoch 58 - Train Loss: 0.254186\n",
      "Session 2 Epoch 59 - Train Loss: 0.254185\n",
      "Session 2 Epoch 60 - Train Loss: 0.254493\n",
      "Session 2 Epoch 61 - Train Loss: 0.254308\n",
      "Session 2 Epoch 62 - Train Loss: 0.253979\n",
      "Checkpoint saved for fold 7, session 2, epoch 62\n",
      "Session 2 Epoch 63 - Train Loss: 0.254117\n",
      "Session 2 Epoch 64 - Train Loss: 0.253773\n",
      "Checkpoint saved for fold 7, session 2, epoch 64\n",
      "Session 2 Epoch 65 - Train Loss: 0.253858\n",
      "Session 2 Epoch 66 - Train Loss: 0.253637\n",
      "Checkpoint saved for fold 7, session 2, epoch 66\n",
      "Session 2 Epoch 67 - Train Loss: 0.253977\n",
      "Session 2 Epoch 68 - Train Loss: 0.254077\n",
      "Session 2 Epoch 69 - Train Loss: 0.253583\n",
      "Session 2 Epoch 70 - Train Loss: 0.253727\n",
      "Session 2 Epoch 71 - Train Loss: 0.253799\n",
      "Session 2 Epoch 72 - Train Loss: 0.253804\n",
      "Session 2 Epoch 73 - Train Loss: 0.253742\n",
      "Session 2 Epoch 74 - Train Loss: 0.253535\n",
      "Checkpoint saved for fold 7, session 2, epoch 74\n",
      "Session 2 Epoch 75 - Train Loss: 0.253671\n",
      "Session 2 Epoch 76 - Train Loss: 0.253171\n",
      "Checkpoint saved for fold 7, session 2, epoch 76\n",
      "Session 2 Epoch 77 - Train Loss: 0.253665\n",
      "Session 2 Epoch 78 - Train Loss: 0.253040\n",
      "Checkpoint saved for fold 7, session 2, epoch 78\n",
      "Session 2 Epoch 79 - Train Loss: 0.253381\n",
      "Session 2 Epoch 80 - Train Loss: 0.253212\n",
      "Training on Session 4/9\n",
      "Session 3 Epoch 1 - Train Loss: 0.160940\n",
      "Checkpoint saved for fold 7, session 3, epoch 1\n",
      "Session 3 Epoch 2 - Train Loss: 0.160665\n",
      "Checkpoint saved for fold 7, session 3, epoch 2\n",
      "Session 3 Epoch 3 - Train Loss: 0.160714\n",
      "Session 3 Epoch 4 - Train Loss: 0.160805\n",
      "Session 3 Epoch 5 - Train Loss: 0.160523\n",
      "Checkpoint saved for fold 7, session 3, epoch 5\n",
      "Session 3 Epoch 6 - Train Loss: 0.160718\n",
      "Session 3 Epoch 7 - Train Loss: 0.160879\n",
      "Session 3 Epoch 8 - Train Loss: 0.160247\n",
      "Checkpoint saved for fold 7, session 3, epoch 8\n",
      "Session 3 Epoch 9 - Train Loss: 0.160983\n",
      "Session 3 Epoch 10 - Train Loss: 0.159862\n",
      "Checkpoint saved for fold 7, session 3, epoch 10\n",
      "Session 3 Epoch 11 - Train Loss: 0.160273\n",
      "Session 3 Epoch 12 - Train Loss: 0.159927\n",
      "Session 3 Epoch 13 - Train Loss: 0.160248\n",
      "Session 3 Epoch 14 - Train Loss: 0.159997\n",
      "Session 3 Epoch 15 - Train Loss: 0.159551\n",
      "Checkpoint saved for fold 7, session 3, epoch 15\n",
      "Session 3 Epoch 16 - Train Loss: 0.160221\n",
      "Session 3 Epoch 17 - Train Loss: 0.159885\n",
      "Session 3 Epoch 18 - Train Loss: 0.160195\n",
      "Session 3 Epoch 19 - Train Loss: 0.159928\n",
      "Session 3 Epoch 20 - Train Loss: 0.159779\n",
      "Session 3 Epoch 21 - Train Loss: 0.159326\n",
      "Checkpoint saved for fold 7, session 3, epoch 21\n",
      "Session 3 Epoch 22 - Train Loss: 0.159356\n",
      "Session 3 Epoch 23 - Train Loss: 0.159822\n",
      "Session 3 Epoch 24 - Train Loss: 0.159661\n",
      "Session 3 Epoch 25 - Train Loss: 0.159762\n",
      "Session 3 Epoch 26 - Train Loss: 0.159244\n",
      "Session 3 Epoch 27 - Train Loss: 0.159305\n",
      "Session 3 Epoch 28 - Train Loss: 0.159178\n",
      "Checkpoint saved for fold 7, session 3, epoch 28\n",
      "Session 3 Epoch 29 - Train Loss: 0.159368\n",
      "Session 3 Epoch 30 - Train Loss: 0.159153\n",
      "Session 3 Epoch 31 - Train Loss: 0.159434\n",
      "Session 3 Epoch 32 - Train Loss: 0.158500\n",
      "Checkpoint saved for fold 7, session 3, epoch 32\n",
      "Session 3 Epoch 33 - Train Loss: 0.159101\n",
      "Session 3 Epoch 34 - Train Loss: 0.159176\n",
      "Session 3 Epoch 35 - Train Loss: 0.159027\n",
      "Session 3 Epoch 36 - Train Loss: 0.159668\n",
      "Session 3 Epoch 37 - Train Loss: 0.158382\n",
      "Checkpoint saved for fold 7, session 3, epoch 37\n",
      "Session 3 Epoch 38 - Train Loss: 0.158709\n",
      "Session 3 Epoch 39 - Train Loss: 0.158673\n",
      "Session 3 Epoch 40 - Train Loss: 0.158079\n",
      "Checkpoint saved for fold 7, session 3, epoch 40\n",
      "Session 3 Epoch 41 - Train Loss: 0.158243\n",
      "Session 3 Epoch 42 - Train Loss: 0.158777\n",
      "Session 3 Epoch 43 - Train Loss: 0.158241\n",
      "Session 3 Epoch 44 - Train Loss: 0.157901\n",
      "Checkpoint saved for fold 7, session 3, epoch 44\n",
      "Session 3 Epoch 45 - Train Loss: 0.157834\n",
      "Session 3 Epoch 46 - Train Loss: 0.158031\n",
      "Session 3 Epoch 47 - Train Loss: 0.158021\n",
      "Session 3 Epoch 48 - Train Loss: 0.158163\n",
      "Session 3 Epoch 49 - Train Loss: 0.158058\n",
      "Session 3 Epoch 50 - Train Loss: 0.158018\n",
      "Session 3 Epoch 51 - Train Loss: 0.157560\n",
      "Checkpoint saved for fold 7, session 3, epoch 51\n",
      "Session 3 Epoch 52 - Train Loss: 0.157427\n",
      "Checkpoint saved for fold 7, session 3, epoch 52\n",
      "Session 3 Epoch 53 - Train Loss: 0.158013\n",
      "Session 3 Epoch 54 - Train Loss: 0.157721\n",
      "Session 3 Epoch 55 - Train Loss: 0.157535\n",
      "Session 3 Epoch 56 - Train Loss: 0.157323\n",
      "Checkpoint saved for fold 7, session 3, epoch 56\n",
      "Session 3 Epoch 57 - Train Loss: 0.157261\n",
      "Session 3 Epoch 58 - Train Loss: 0.157548\n",
      "Session 3 Epoch 59 - Train Loss: 0.157436\n",
      "Session 3 Epoch 60 - Train Loss: 0.157430\n",
      "Session 3 Epoch 61 - Train Loss: 0.157398\n",
      "Session 3 Epoch 62 - Train Loss: 0.157172\n",
      "Checkpoint saved for fold 7, session 3, epoch 62\n",
      "Session 3 Epoch 63 - Train Loss: 0.157099\n",
      "Session 3 Epoch 64 - Train Loss: 0.156961\n",
      "Checkpoint saved for fold 7, session 3, epoch 64\n",
      "Session 3 Epoch 65 - Train Loss: 0.157212\n",
      "Session 3 Epoch 66 - Train Loss: 0.157115\n",
      "Session 3 Epoch 67 - Train Loss: 0.156919\n",
      "Session 3 Epoch 68 - Train Loss: 0.156391\n",
      "Checkpoint saved for fold 7, session 3, epoch 68\n",
      "Session 3 Epoch 69 - Train Loss: 0.156780\n",
      "Session 3 Epoch 70 - Train Loss: 0.156462\n",
      "Session 3 Epoch 71 - Train Loss: 0.156813\n",
      "Session 3 Epoch 72 - Train Loss: 0.156629\n",
      "Session 3 Epoch 73 - Train Loss: 0.156227\n",
      "Checkpoint saved for fold 7, session 3, epoch 73\n",
      "Session 3 Epoch 74 - Train Loss: 0.156462\n",
      "Session 3 Epoch 75 - Train Loss: 0.156012\n",
      "Checkpoint saved for fold 7, session 3, epoch 75\n",
      "Session 3 Epoch 76 - Train Loss: 0.156387\n",
      "Session 3 Epoch 77 - Train Loss: 0.155903\n",
      "Checkpoint saved for fold 7, session 3, epoch 77\n",
      "Session 3 Epoch 78 - Train Loss: 0.156362\n",
      "Session 3 Epoch 79 - Train Loss: 0.155961\n",
      "Session 3 Epoch 80 - Train Loss: 0.156285\n",
      "Training on Session 5/9\n",
      "Session 4 Epoch 1 - Train Loss: 0.055899\n",
      "Checkpoint saved for fold 7, session 4, epoch 1\n",
      "Session 4 Epoch 2 - Train Loss: 0.055808\n",
      "Session 4 Epoch 3 - Train Loss: 0.055906\n",
      "Session 4 Epoch 4 - Train Loss: 0.055573\n",
      "Checkpoint saved for fold 7, session 4, epoch 4\n",
      "Session 4 Epoch 5 - Train Loss: 0.055232\n",
      "Checkpoint saved for fold 7, session 4, epoch 5\n",
      "Session 4 Epoch 6 - Train Loss: 0.055729\n",
      "Session 4 Epoch 7 - Train Loss: 0.055253\n",
      "Session 4 Epoch 8 - Train Loss: 0.055847\n",
      "Session 4 Epoch 9 - Train Loss: 0.055422\n",
      "Session 4 Epoch 10 - Train Loss: 0.055417\n",
      "Session 4 Epoch 11 - Train Loss: 0.055650\n",
      "Session 4 Epoch 12 - Train Loss: 0.055330\n",
      "Session 4 Epoch 13 - Train Loss: 0.055851\n",
      "Session 4 Epoch 14 - Train Loss: 0.055342\n",
      "Session 4 Epoch 15 - Train Loss: 0.055631\n",
      "Early stopping at epoch 15 for session 4\n",
      "Training on Session 6/9\n",
      "Session 5 Epoch 1 - Train Loss: 0.229722\n",
      "Checkpoint saved for fold 7, session 5, epoch 1\n",
      "Session 5 Epoch 2 - Train Loss: 0.229770\n",
      "Session 5 Epoch 3 - Train Loss: 0.229935\n",
      "Session 5 Epoch 4 - Train Loss: 0.229803\n",
      "Session 5 Epoch 5 - Train Loss: 0.230120\n",
      "Session 5 Epoch 6 - Train Loss: 0.229764\n",
      "Session 5 Epoch 7 - Train Loss: 0.229415\n",
      "Checkpoint saved for fold 7, session 5, epoch 7\n",
      "Session 5 Epoch 8 - Train Loss: 0.229479\n",
      "Session 5 Epoch 9 - Train Loss: 0.229508\n",
      "Session 5 Epoch 10 - Train Loss: 0.229906\n",
      "Session 5 Epoch 11 - Train Loss: 0.229316\n",
      "Session 5 Epoch 12 - Train Loss: 0.229471\n",
      "Session 5 Epoch 13 - Train Loss: 0.228970\n",
      "Checkpoint saved for fold 7, session 5, epoch 13\n",
      "Session 5 Epoch 14 - Train Loss: 0.229234\n",
      "Session 5 Epoch 15 - Train Loss: 0.229338\n",
      "Session 5 Epoch 16 - Train Loss: 0.229209\n",
      "Session 5 Epoch 17 - Train Loss: 0.228932\n",
      "Session 5 Epoch 18 - Train Loss: 0.228928\n",
      "Session 5 Epoch 19 - Train Loss: 0.229155\n",
      "Session 5 Epoch 20 - Train Loss: 0.228736\n",
      "Checkpoint saved for fold 7, session 5, epoch 20\n",
      "Session 5 Epoch 21 - Train Loss: 0.229132\n",
      "Session 5 Epoch 22 - Train Loss: 0.229162\n",
      "Session 5 Epoch 23 - Train Loss: 0.229662\n",
      "Session 5 Epoch 24 - Train Loss: 0.228593\n",
      "Checkpoint saved for fold 7, session 5, epoch 24\n",
      "Session 5 Epoch 25 - Train Loss: 0.228977\n",
      "Session 5 Epoch 26 - Train Loss: 0.228895\n",
      "Session 5 Epoch 27 - Train Loss: 0.228657\n",
      "Session 5 Epoch 28 - Train Loss: 0.228523\n",
      "Session 5 Epoch 29 - Train Loss: 0.228883\n",
      "Session 5 Epoch 30 - Train Loss: 0.228858\n",
      "Session 5 Epoch 31 - Train Loss: 0.228617\n",
      "Session 5 Epoch 32 - Train Loss: 0.228426\n",
      "Checkpoint saved for fold 7, session 5, epoch 32\n",
      "Session 5 Epoch 33 - Train Loss: 0.228660\n",
      "Session 5 Epoch 34 - Train Loss: 0.228477\n",
      "Session 5 Epoch 35 - Train Loss: 0.228697\n",
      "Session 5 Epoch 36 - Train Loss: 0.228678\n",
      "Session 5 Epoch 37 - Train Loss: 0.228573\n",
      "Session 5 Epoch 38 - Train Loss: 0.228224\n",
      "Checkpoint saved for fold 7, session 5, epoch 38\n",
      "Session 5 Epoch 39 - Train Loss: 0.228308\n",
      "Session 5 Epoch 40 - Train Loss: 0.228421\n",
      "Session 5 Epoch 41 - Train Loss: 0.228149\n",
      "Session 5 Epoch 42 - Train Loss: 0.228107\n",
      "Checkpoint saved for fold 7, session 5, epoch 42\n",
      "Session 5 Epoch 43 - Train Loss: 0.228327\n",
      "Session 5 Epoch 44 - Train Loss: 0.228304\n",
      "Session 5 Epoch 45 - Train Loss: 0.227844\n",
      "Checkpoint saved for fold 7, session 5, epoch 45\n",
      "Session 5 Epoch 46 - Train Loss: 0.228288\n",
      "Session 5 Epoch 47 - Train Loss: 0.228024\n",
      "Session 5 Epoch 48 - Train Loss: 0.227678\n",
      "Checkpoint saved for fold 7, session 5, epoch 48\n",
      "Session 5 Epoch 49 - Train Loss: 0.228081\n",
      "Session 5 Epoch 50 - Train Loss: 0.227805\n",
      "Session 5 Epoch 51 - Train Loss: 0.227934\n",
      "Session 5 Epoch 52 - Train Loss: 0.227579\n",
      "Session 5 Epoch 53 - Train Loss: 0.227554\n",
      "Checkpoint saved for fold 7, session 5, epoch 53\n",
      "Session 5 Epoch 54 - Train Loss: 0.227803\n",
      "Session 5 Epoch 55 - Train Loss: 0.227713\n",
      "Session 5 Epoch 56 - Train Loss: 0.227843\n",
      "Session 5 Epoch 57 - Train Loss: 0.227949\n",
      "Session 5 Epoch 58 - Train Loss: 0.227320\n",
      "Checkpoint saved for fold 7, session 5, epoch 58\n",
      "Session 5 Epoch 59 - Train Loss: 0.227505\n",
      "Session 5 Epoch 60 - Train Loss: 0.227194\n",
      "Checkpoint saved for fold 7, session 5, epoch 60\n",
      "Session 5 Epoch 61 - Train Loss: 0.227366\n",
      "Session 5 Epoch 62 - Train Loss: 0.227481\n",
      "Session 5 Epoch 63 - Train Loss: 0.227023\n",
      "Checkpoint saved for fold 7, session 5, epoch 63\n",
      "Session 5 Epoch 64 - Train Loss: 0.227497\n",
      "Session 5 Epoch 65 - Train Loss: 0.227186\n",
      "Session 5 Epoch 66 - Train Loss: 0.227380\n",
      "Session 5 Epoch 67 - Train Loss: 0.227270\n",
      "Session 5 Epoch 68 - Train Loss: 0.227355\n",
      "Session 5 Epoch 69 - Train Loss: 0.227082\n",
      "Session 5 Epoch 70 - Train Loss: 0.226868\n",
      "Checkpoint saved for fold 7, session 5, epoch 70\n",
      "Session 5 Epoch 71 - Train Loss: 0.226961\n",
      "Session 5 Epoch 72 - Train Loss: 0.226888\n",
      "Session 5 Epoch 73 - Train Loss: 0.226898\n",
      "Session 5 Epoch 74 - Train Loss: 0.227023\n",
      "Session 5 Epoch 75 - Train Loss: 0.226716\n",
      "Checkpoint saved for fold 7, session 5, epoch 75\n",
      "Session 5 Epoch 76 - Train Loss: 0.226640\n",
      "Session 5 Epoch 77 - Train Loss: 0.226793\n",
      "Session 5 Epoch 78 - Train Loss: 0.227125\n",
      "Session 5 Epoch 79 - Train Loss: 0.227065\n",
      "Session 5 Epoch 80 - Train Loss: 0.226595\n",
      "Checkpoint saved for fold 7, session 5, epoch 80\n",
      "Training on Session 7/9\n",
      "Session 6 Epoch 1 - Train Loss: 0.167502\n",
      "Checkpoint saved for fold 7, session 6, epoch 1\n",
      "Session 6 Epoch 2 - Train Loss: 0.167753\n",
      "Session 6 Epoch 3 - Train Loss: 0.167697\n",
      "Session 6 Epoch 4 - Train Loss: 0.167263\n",
      "Checkpoint saved for fold 7, session 6, epoch 4\n",
      "Session 6 Epoch 5 - Train Loss: 0.167796\n",
      "Session 6 Epoch 6 - Train Loss: 0.166947\n",
      "Checkpoint saved for fold 7, session 6, epoch 6\n",
      "Session 6 Epoch 7 - Train Loss: 0.167177\n",
      "Session 6 Epoch 8 - Train Loss: 0.167482\n",
      "Session 6 Epoch 9 - Train Loss: 0.167648\n",
      "Session 6 Epoch 10 - Train Loss: 0.166243\n",
      "Checkpoint saved for fold 7, session 6, epoch 10\n",
      "Session 6 Epoch 11 - Train Loss: 0.167374\n",
      "Session 6 Epoch 12 - Train Loss: 0.167000\n",
      "Session 6 Epoch 13 - Train Loss: 0.167072\n",
      "Session 6 Epoch 14 - Train Loss: 0.166908\n",
      "Session 6 Epoch 15 - Train Loss: 0.166787\n",
      "Session 6 Epoch 16 - Train Loss: 0.166518\n",
      "Session 6 Epoch 17 - Train Loss: 0.166369\n",
      "Session 6 Epoch 18 - Train Loss: 0.167015\n",
      "Session 6 Epoch 19 - Train Loss: 0.166171\n",
      "Session 6 Epoch 20 - Train Loss: 0.166538\n",
      "Early stopping at epoch 20 for session 6\n",
      "Training on Session 8/9\n",
      "Session 7 Epoch 1 - Train Loss: 0.122160\n",
      "Checkpoint saved for fold 7, session 7, epoch 1\n",
      "Session 7 Epoch 2 - Train Loss: 0.121926\n",
      "Checkpoint saved for fold 7, session 7, epoch 2\n",
      "Session 7 Epoch 3 - Train Loss: 0.122419\n",
      "Session 7 Epoch 4 - Train Loss: 0.122088\n",
      "Session 7 Epoch 5 - Train Loss: 0.121841\n",
      "Session 7 Epoch 6 - Train Loss: 0.122449\n",
      "Session 7 Epoch 7 - Train Loss: 0.121980\n",
      "Session 7 Epoch 8 - Train Loss: 0.121884\n",
      "Session 7 Epoch 9 - Train Loss: 0.121369\n",
      "Checkpoint saved for fold 7, session 7, epoch 9\n",
      "Session 7 Epoch 10 - Train Loss: 0.122269\n",
      "Session 7 Epoch 11 - Train Loss: 0.121925\n",
      "Session 7 Epoch 12 - Train Loss: 0.121572\n",
      "Session 7 Epoch 13 - Train Loss: 0.121874\n",
      "Session 7 Epoch 14 - Train Loss: 0.121925\n",
      "Session 7 Epoch 15 - Train Loss: 0.121452\n",
      "Session 7 Epoch 16 - Train Loss: 0.121882\n",
      "Session 7 Epoch 17 - Train Loss: 0.122186\n",
      "Session 7 Epoch 18 - Train Loss: 0.121805\n",
      "Session 7 Epoch 19 - Train Loss: 0.121900\n",
      "Early stopping at epoch 19 for session 7\n",
      "Training on Session 9/9\n",
      "Session 8 Epoch 1 - Train Loss: 0.084043\n",
      "Checkpoint saved for fold 7, session 8, epoch 1\n",
      "Session 8 Epoch 2 - Train Loss: 0.083996\n",
      "Session 8 Epoch 3 - Train Loss: 0.083602\n",
      "Checkpoint saved for fold 7, session 8, epoch 3\n",
      "Session 8 Epoch 4 - Train Loss: 0.084053\n",
      "Session 8 Epoch 5 - Train Loss: 0.084131\n",
      "Session 8 Epoch 6 - Train Loss: 0.084429\n",
      "Session 8 Epoch 7 - Train Loss: 0.084177\n",
      "Session 8 Epoch 8 - Train Loss: 0.083750\n",
      "Session 8 Epoch 9 - Train Loss: 0.083980\n",
      "Session 8 Epoch 10 - Train Loss: 0.083576\n",
      "Session 8 Epoch 11 - Train Loss: 0.083922\n",
      "Session 8 Epoch 12 - Train Loss: 0.083647\n",
      "Session 8 Epoch 13 - Train Loss: 0.083521\n",
      "Early stopping at epoch 13 for session 8\n",
      "Fold 7 - Test Loss: 0.0641, R^2: -25866959645424.9609\n",
      "\n",
      "=== Fold 8 ===\n",
      "Training on Session 1/9\n",
      "Session 0 Epoch 1 - Train Loss: 0.054610\n",
      "Checkpoint saved for fold 8, session 0, epoch 1\n",
      "Session 0 Epoch 2 - Train Loss: 0.053117\n",
      "Checkpoint saved for fold 8, session 0, epoch 2\n",
      "Session 0 Epoch 3 - Train Loss: 0.051565\n",
      "Checkpoint saved for fold 8, session 0, epoch 3\n",
      "Session 0 Epoch 4 - Train Loss: 0.044365\n",
      "Checkpoint saved for fold 8, session 0, epoch 4\n",
      "Session 0 Epoch 5 - Train Loss: 0.037848\n",
      "Checkpoint saved for fold 8, session 0, epoch 5\n",
      "Session 0 Epoch 6 - Train Loss: 0.032959\n",
      "Checkpoint saved for fold 8, session 0, epoch 6\n",
      "Session 0 Epoch 7 - Train Loss: 0.030089\n",
      "Checkpoint saved for fold 8, session 0, epoch 7\n",
      "Session 0 Epoch 8 - Train Loss: 0.029508\n",
      "Checkpoint saved for fold 8, session 0, epoch 8\n",
      "Session 0 Epoch 9 - Train Loss: 0.028018\n",
      "Checkpoint saved for fold 8, session 0, epoch 9\n",
      "Session 0 Epoch 10 - Train Loss: 0.027467\n",
      "Checkpoint saved for fold 8, session 0, epoch 10\n",
      "Session 0 Epoch 11 - Train Loss: 0.026702\n",
      "Checkpoint saved for fold 8, session 0, epoch 11\n",
      "Session 0 Epoch 12 - Train Loss: 0.024855\n",
      "Checkpoint saved for fold 8, session 0, epoch 12\n",
      "Session 0 Epoch 13 - Train Loss: 0.025286\n",
      "Session 0 Epoch 14 - Train Loss: 0.025219\n",
      "Session 0 Epoch 15 - Train Loss: 0.022636\n",
      "Checkpoint saved for fold 8, session 0, epoch 15\n",
      "Session 0 Epoch 16 - Train Loss: 0.023030\n",
      "Session 0 Epoch 17 - Train Loss: 0.022943\n",
      "Session 0 Epoch 18 - Train Loss: 0.020829\n",
      "Checkpoint saved for fold 8, session 0, epoch 18\n",
      "Session 0 Epoch 19 - Train Loss: 0.020257\n",
      "Checkpoint saved for fold 8, session 0, epoch 19\n",
      "Session 0 Epoch 20 - Train Loss: 0.019232\n",
      "Checkpoint saved for fold 8, session 0, epoch 20\n",
      "Session 0 Epoch 21 - Train Loss: 0.019180\n",
      "Session 0 Epoch 22 - Train Loss: 0.017984\n",
      "Checkpoint saved for fold 8, session 0, epoch 22\n",
      "Session 0 Epoch 23 - Train Loss: 0.017912\n",
      "Session 0 Epoch 24 - Train Loss: 0.017628\n",
      "Checkpoint saved for fold 8, session 0, epoch 24\n",
      "Session 0 Epoch 25 - Train Loss: 0.017400\n",
      "Checkpoint saved for fold 8, session 0, epoch 25\n",
      "Session 0 Epoch 26 - Train Loss: 0.016013\n",
      "Checkpoint saved for fold 8, session 0, epoch 26\n",
      "Session 0 Epoch 27 - Train Loss: 0.016396\n",
      "Session 0 Epoch 28 - Train Loss: 0.015357\n",
      "Checkpoint saved for fold 8, session 0, epoch 28\n",
      "Session 0 Epoch 29 - Train Loss: 0.015621\n",
      "Session 0 Epoch 30 - Train Loss: 0.015923\n",
      "Session 0 Epoch 31 - Train Loss: 0.015290\n",
      "Session 0 Epoch 32 - Train Loss: 0.015514\n",
      "Session 0 Epoch 33 - Train Loss: 0.015654\n",
      "Session 0 Epoch 34 - Train Loss: 0.014247\n",
      "Checkpoint saved for fold 8, session 0, epoch 34\n",
      "Session 0 Epoch 35 - Train Loss: 0.014124\n",
      "Checkpoint saved for fold 8, session 0, epoch 35\n",
      "Session 0 Epoch 36 - Train Loss: 0.014020\n",
      "Checkpoint saved for fold 8, session 0, epoch 36\n",
      "Session 0 Epoch 37 - Train Loss: 0.014713\n",
      "Session 0 Epoch 38 - Train Loss: 0.013861\n",
      "Checkpoint saved for fold 8, session 0, epoch 38\n",
      "Session 0 Epoch 39 - Train Loss: 0.013589\n",
      "Checkpoint saved for fold 8, session 0, epoch 39\n",
      "Session 0 Epoch 40 - Train Loss: 0.013010\n",
      "Checkpoint saved for fold 8, session 0, epoch 40\n",
      "Session 0 Epoch 41 - Train Loss: 0.013434\n",
      "Session 0 Epoch 42 - Train Loss: 0.012603\n",
      "Checkpoint saved for fold 8, session 0, epoch 42\n",
      "Session 0 Epoch 43 - Train Loss: 0.011818\n",
      "Checkpoint saved for fold 8, session 0, epoch 43\n",
      "Session 0 Epoch 44 - Train Loss: 0.012176\n",
      "Session 0 Epoch 45 - Train Loss: 0.012589\n",
      "Session 0 Epoch 46 - Train Loss: 0.011269\n",
      "Checkpoint saved for fold 8, session 0, epoch 46\n",
      "Session 0 Epoch 47 - Train Loss: 0.011363\n",
      "Session 0 Epoch 48 - Train Loss: 0.011997\n",
      "Session 0 Epoch 49 - Train Loss: 0.011195\n",
      "Session 0 Epoch 50 - Train Loss: 0.010576\n",
      "Checkpoint saved for fold 8, session 0, epoch 50\n",
      "Session 0 Epoch 51 - Train Loss: 0.010622\n",
      "Session 0 Epoch 52 - Train Loss: 0.010014\n",
      "Checkpoint saved for fold 8, session 0, epoch 52\n",
      "Session 0 Epoch 53 - Train Loss: 0.010447\n",
      "Session 0 Epoch 54 - Train Loss: 0.009409\n",
      "Checkpoint saved for fold 8, session 0, epoch 54\n",
      "Session 0 Epoch 55 - Train Loss: 0.010191\n",
      "Session 0 Epoch 56 - Train Loss: 0.009812\n",
      "Session 0 Epoch 57 - Train Loss: 0.008957\n",
      "Checkpoint saved for fold 8, session 0, epoch 57\n",
      "Session 0 Epoch 58 - Train Loss: 0.009475\n",
      "Session 0 Epoch 59 - Train Loss: 0.008668\n",
      "Checkpoint saved for fold 8, session 0, epoch 59\n",
      "Session 0 Epoch 60 - Train Loss: 0.009138\n",
      "Session 0 Epoch 61 - Train Loss: 0.008873\n",
      "Session 0 Epoch 62 - Train Loss: 0.008733\n",
      "Session 0 Epoch 63 - Train Loss: 0.009447\n",
      "Session 0 Epoch 64 - Train Loss: 0.008874\n",
      "Session 0 Epoch 65 - Train Loss: 0.008158\n",
      "Checkpoint saved for fold 8, session 0, epoch 65\n",
      "Session 0 Epoch 66 - Train Loss: 0.008429\n",
      "Session 0 Epoch 67 - Train Loss: 0.008612\n",
      "Session 0 Epoch 68 - Train Loss: 0.007888\n",
      "Checkpoint saved for fold 8, session 0, epoch 68\n",
      "Session 0 Epoch 69 - Train Loss: 0.007888\n",
      "Session 0 Epoch 70 - Train Loss: 0.007435\n",
      "Checkpoint saved for fold 8, session 0, epoch 70\n",
      "Session 0 Epoch 71 - Train Loss: 0.007874\n",
      "Session 0 Epoch 72 - Train Loss: 0.008102\n",
      "Session 0 Epoch 73 - Train Loss: 0.008274\n",
      "Session 0 Epoch 74 - Train Loss: 0.007214\n",
      "Checkpoint saved for fold 8, session 0, epoch 74\n",
      "Session 0 Epoch 75 - Train Loss: 0.007043\n",
      "Checkpoint saved for fold 8, session 0, epoch 75\n",
      "Session 0 Epoch 76 - Train Loss: 0.007124\n",
      "Session 0 Epoch 77 - Train Loss: 0.007572\n",
      "Session 0 Epoch 78 - Train Loss: 0.007874\n",
      "Session 0 Epoch 79 - Train Loss: 0.007164\n",
      "Session 0 Epoch 80 - Train Loss: 0.006946\n",
      "Training on Session 2/9\n",
      "Session 1 Epoch 1 - Train Loss: 0.051072\n",
      "Checkpoint saved for fold 8, session 1, epoch 1\n",
      "Session 1 Epoch 2 - Train Loss: 0.050857\n",
      "Checkpoint saved for fold 8, session 1, epoch 2\n",
      "Session 1 Epoch 3 - Train Loss: 0.050737\n",
      "Checkpoint saved for fold 8, session 1, epoch 3\n",
      "Session 1 Epoch 4 - Train Loss: 0.050420\n",
      "Checkpoint saved for fold 8, session 1, epoch 4\n",
      "Session 1 Epoch 5 - Train Loss: 0.049869\n",
      "Checkpoint saved for fold 8, session 1, epoch 5\n",
      "Session 1 Epoch 6 - Train Loss: 0.048029\n",
      "Checkpoint saved for fold 8, session 1, epoch 6\n",
      "Session 1 Epoch 7 - Train Loss: 0.042507\n",
      "Checkpoint saved for fold 8, session 1, epoch 7\n",
      "Session 1 Epoch 8 - Train Loss: 0.036245\n",
      "Checkpoint saved for fold 8, session 1, epoch 8\n",
      "Session 1 Epoch 9 - Train Loss: 0.028953\n",
      "Checkpoint saved for fold 8, session 1, epoch 9\n",
      "Session 1 Epoch 10 - Train Loss: 0.022683\n",
      "Checkpoint saved for fold 8, session 1, epoch 10\n",
      "Session 1 Epoch 11 - Train Loss: 0.021790\n",
      "Checkpoint saved for fold 8, session 1, epoch 11\n",
      "Session 1 Epoch 12 - Train Loss: 0.019538\n",
      "Checkpoint saved for fold 8, session 1, epoch 12\n",
      "Session 1 Epoch 13 - Train Loss: 0.016479\n",
      "Checkpoint saved for fold 8, session 1, epoch 13\n",
      "Session 1 Epoch 14 - Train Loss: 0.015334\n",
      "Checkpoint saved for fold 8, session 1, epoch 14\n",
      "Session 1 Epoch 15 - Train Loss: 0.014258\n",
      "Checkpoint saved for fold 8, session 1, epoch 15\n",
      "Session 1 Epoch 16 - Train Loss: 0.014078\n",
      "Checkpoint saved for fold 8, session 1, epoch 16\n",
      "Session 1 Epoch 17 - Train Loss: 0.013572\n",
      "Checkpoint saved for fold 8, session 1, epoch 17\n",
      "Session 1 Epoch 18 - Train Loss: 0.013118\n",
      "Checkpoint saved for fold 8, session 1, epoch 18\n",
      "Session 1 Epoch 19 - Train Loss: 0.012354\n",
      "Checkpoint saved for fold 8, session 1, epoch 19\n",
      "Session 1 Epoch 20 - Train Loss: 0.012500\n",
      "Session 1 Epoch 21 - Train Loss: 0.012163\n",
      "Checkpoint saved for fold 8, session 1, epoch 21\n",
      "Session 1 Epoch 22 - Train Loss: 0.012148\n",
      "Session 1 Epoch 23 - Train Loss: 0.011478\n",
      "Checkpoint saved for fold 8, session 1, epoch 23\n",
      "Session 1 Epoch 24 - Train Loss: 0.011758\n",
      "Session 1 Epoch 25 - Train Loss: 0.011494\n",
      "Session 1 Epoch 26 - Train Loss: 0.011594\n",
      "Session 1 Epoch 27 - Train Loss: 0.011217\n",
      "Checkpoint saved for fold 8, session 1, epoch 27\n",
      "Session 1 Epoch 28 - Train Loss: 0.010933\n",
      "Checkpoint saved for fold 8, session 1, epoch 28\n",
      "Session 1 Epoch 29 - Train Loss: 0.010996\n",
      "Session 1 Epoch 30 - Train Loss: 0.010768\n",
      "Checkpoint saved for fold 8, session 1, epoch 30\n",
      "Session 1 Epoch 31 - Train Loss: 0.010587\n",
      "Checkpoint saved for fold 8, session 1, epoch 31\n",
      "Session 1 Epoch 32 - Train Loss: 0.010735\n",
      "Session 1 Epoch 33 - Train Loss: 0.010678\n",
      "Session 1 Epoch 34 - Train Loss: 0.010878\n",
      "Session 1 Epoch 35 - Train Loss: 0.010819\n",
      "Session 1 Epoch 36 - Train Loss: 0.010617\n",
      "Session 1 Epoch 37 - Train Loss: 0.010696\n",
      "Session 1 Epoch 38 - Train Loss: 0.010467\n",
      "Checkpoint saved for fold 8, session 1, epoch 38\n",
      "Session 1 Epoch 39 - Train Loss: 0.010688\n",
      "Session 1 Epoch 40 - Train Loss: 0.010608\n",
      "Session 1 Epoch 41 - Train Loss: 0.010374\n",
      "Session 1 Epoch 42 - Train Loss: 0.010365\n",
      "Checkpoint saved for fold 8, session 1, epoch 42\n",
      "Session 1 Epoch 43 - Train Loss: 0.010526\n",
      "Session 1 Epoch 44 - Train Loss: 0.010784\n",
      "Session 1 Epoch 45 - Train Loss: 0.010673\n",
      "Session 1 Epoch 46 - Train Loss: 0.010644\n",
      "Session 1 Epoch 47 - Train Loss: 0.010476\n",
      "Session 1 Epoch 48 - Train Loss: 0.010266\n",
      "Session 1 Epoch 49 - Train Loss: 0.010306\n",
      "Session 1 Epoch 50 - Train Loss: 0.010574\n",
      "Session 1 Epoch 51 - Train Loss: 0.010690\n",
      "Session 1 Epoch 52 - Train Loss: 0.010405\n",
      "Early stopping at epoch 52 for session 1\n",
      "Training on Session 3/9\n",
      "Session 2 Epoch 1 - Train Loss: 0.262323\n",
      "Checkpoint saved for fold 8, session 2, epoch 1\n",
      "Session 2 Epoch 2 - Train Loss: 0.240533\n",
      "Checkpoint saved for fold 8, session 2, epoch 2\n",
      "Session 2 Epoch 3 - Train Loss: 0.225004\n",
      "Checkpoint saved for fold 8, session 2, epoch 3\n",
      "Session 2 Epoch 4 - Train Loss: 0.212847\n",
      "Checkpoint saved for fold 8, session 2, epoch 4\n",
      "Session 2 Epoch 5 - Train Loss: 0.199349\n",
      "Checkpoint saved for fold 8, session 2, epoch 5\n",
      "Session 2 Epoch 6 - Train Loss: 0.183786\n",
      "Checkpoint saved for fold 8, session 2, epoch 6\n",
      "Session 2 Epoch 7 - Train Loss: 0.166754\n",
      "Checkpoint saved for fold 8, session 2, epoch 7\n",
      "Session 2 Epoch 8 - Train Loss: 0.148391\n",
      "Checkpoint saved for fold 8, session 2, epoch 8\n",
      "Session 2 Epoch 9 - Train Loss: 0.135021\n",
      "Checkpoint saved for fold 8, session 2, epoch 9\n",
      "Session 2 Epoch 10 - Train Loss: 0.126603\n",
      "Checkpoint saved for fold 8, session 2, epoch 10\n",
      "Session 2 Epoch 11 - Train Loss: 0.118778\n",
      "Checkpoint saved for fold 8, session 2, epoch 11\n",
      "Session 2 Epoch 12 - Train Loss: 0.111793\n",
      "Checkpoint saved for fold 8, session 2, epoch 12\n",
      "Session 2 Epoch 13 - Train Loss: 0.106023\n",
      "Checkpoint saved for fold 8, session 2, epoch 13\n",
      "Session 2 Epoch 14 - Train Loss: 0.101153\n",
      "Checkpoint saved for fold 8, session 2, epoch 14\n",
      "Session 2 Epoch 15 - Train Loss: 0.097884\n",
      "Checkpoint saved for fold 8, session 2, epoch 15\n",
      "Session 2 Epoch 16 - Train Loss: 0.096033\n",
      "Checkpoint saved for fold 8, session 2, epoch 16\n",
      "Session 2 Epoch 17 - Train Loss: 0.094316\n",
      "Checkpoint saved for fold 8, session 2, epoch 17\n",
      "Session 2 Epoch 18 - Train Loss: 0.092583\n",
      "Checkpoint saved for fold 8, session 2, epoch 18\n",
      "Session 2 Epoch 19 - Train Loss: 0.091054\n",
      "Checkpoint saved for fold 8, session 2, epoch 19\n",
      "Session 2 Epoch 20 - Train Loss: 0.089569\n",
      "Checkpoint saved for fold 8, session 2, epoch 20\n",
      "Session 2 Epoch 21 - Train Loss: 0.088945\n",
      "Checkpoint saved for fold 8, session 2, epoch 21\n",
      "Session 2 Epoch 22 - Train Loss: 0.088354\n",
      "Checkpoint saved for fold 8, session 2, epoch 22\n",
      "Session 2 Epoch 23 - Train Loss: 0.088149\n",
      "Checkpoint saved for fold 8, session 2, epoch 23\n",
      "Session 2 Epoch 24 - Train Loss: 0.087825\n",
      "Checkpoint saved for fold 8, session 2, epoch 24\n",
      "Session 2 Epoch 25 - Train Loss: 0.086882\n",
      "Checkpoint saved for fold 8, session 2, epoch 25\n",
      "Session 2 Epoch 26 - Train Loss: 0.086566\n",
      "Checkpoint saved for fold 8, session 2, epoch 26\n",
      "Session 2 Epoch 27 - Train Loss: 0.086322\n",
      "Checkpoint saved for fold 8, session 2, epoch 27\n",
      "Session 2 Epoch 28 - Train Loss: 0.085971\n",
      "Checkpoint saved for fold 8, session 2, epoch 28\n",
      "Session 2 Epoch 29 - Train Loss: 0.085860\n",
      "Checkpoint saved for fold 8, session 2, epoch 29\n",
      "Session 2 Epoch 30 - Train Loss: 0.085715\n",
      "Checkpoint saved for fold 8, session 2, epoch 30\n",
      "Session 2 Epoch 31 - Train Loss: 0.085208\n",
      "Checkpoint saved for fold 8, session 2, epoch 31\n",
      "Session 2 Epoch 32 - Train Loss: 0.084911\n",
      "Checkpoint saved for fold 8, session 2, epoch 32\n",
      "Session 2 Epoch 33 - Train Loss: 0.085256\n",
      "Session 2 Epoch 34 - Train Loss: 0.084981\n",
      "Session 2 Epoch 35 - Train Loss: 0.084785\n",
      "Checkpoint saved for fold 8, session 2, epoch 35\n",
      "Session 2 Epoch 36 - Train Loss: 0.084906\n",
      "Session 2 Epoch 37 - Train Loss: 0.084655\n",
      "Checkpoint saved for fold 8, session 2, epoch 37\n",
      "Session 2 Epoch 38 - Train Loss: 0.084748\n",
      "Session 2 Epoch 39 - Train Loss: 0.084498\n",
      "Checkpoint saved for fold 8, session 2, epoch 39\n",
      "Session 2 Epoch 40 - Train Loss: 0.084379\n",
      "Checkpoint saved for fold 8, session 2, epoch 40\n",
      "Session 2 Epoch 41 - Train Loss: 0.084528\n",
      "Session 2 Epoch 42 - Train Loss: 0.084332\n",
      "Session 2 Epoch 43 - Train Loss: 0.084280\n",
      "Session 2 Epoch 44 - Train Loss: 0.084333\n",
      "Session 2 Epoch 45 - Train Loss: 0.084336\n",
      "Session 2 Epoch 46 - Train Loss: 0.084107\n",
      "Checkpoint saved for fold 8, session 2, epoch 46\n",
      "Session 2 Epoch 47 - Train Loss: 0.084216\n",
      "Session 2 Epoch 48 - Train Loss: 0.083962\n",
      "Checkpoint saved for fold 8, session 2, epoch 48\n",
      "Session 2 Epoch 49 - Train Loss: 0.084257\n",
      "Session 2 Epoch 50 - Train Loss: 0.084107\n",
      "Session 2 Epoch 51 - Train Loss: 0.084367\n",
      "Session 2 Epoch 52 - Train Loss: 0.084300\n",
      "Session 2 Epoch 53 - Train Loss: 0.084024\n",
      "Session 2 Epoch 54 - Train Loss: 0.083969\n",
      "Session 2 Epoch 55 - Train Loss: 0.084065\n",
      "Session 2 Epoch 56 - Train Loss: 0.084268\n",
      "Session 2 Epoch 57 - Train Loss: 0.084185\n",
      "Session 2 Epoch 58 - Train Loss: 0.084094\n",
      "Early stopping at epoch 58 for session 2\n",
      "Training on Session 4/9\n",
      "Session 3 Epoch 1 - Train Loss: 0.492771\n",
      "Checkpoint saved for fold 8, session 3, epoch 1\n",
      "Session 3 Epoch 2 - Train Loss: 0.491889\n",
      "Checkpoint saved for fold 8, session 3, epoch 2\n",
      "Session 3 Epoch 3 - Train Loss: 0.490542\n",
      "Checkpoint saved for fold 8, session 3, epoch 3\n",
      "Session 3 Epoch 4 - Train Loss: 0.491015\n",
      "Session 3 Epoch 5 - Train Loss: 0.489439\n",
      "Checkpoint saved for fold 8, session 3, epoch 5\n",
      "Session 3 Epoch 6 - Train Loss: 0.489143\n",
      "Checkpoint saved for fold 8, session 3, epoch 6\n",
      "Session 3 Epoch 7 - Train Loss: 0.488788\n",
      "Checkpoint saved for fold 8, session 3, epoch 7\n",
      "Session 3 Epoch 8 - Train Loss: 0.488033\n",
      "Checkpoint saved for fold 8, session 3, epoch 8\n",
      "Session 3 Epoch 9 - Train Loss: 0.487102\n",
      "Checkpoint saved for fold 8, session 3, epoch 9\n",
      "Session 3 Epoch 10 - Train Loss: 0.486857\n",
      "Checkpoint saved for fold 8, session 3, epoch 10\n",
      "Session 3 Epoch 11 - Train Loss: 0.486242\n",
      "Checkpoint saved for fold 8, session 3, epoch 11\n",
      "Session 3 Epoch 12 - Train Loss: 0.485363\n",
      "Checkpoint saved for fold 8, session 3, epoch 12\n",
      "Session 3 Epoch 13 - Train Loss: 0.484281\n",
      "Checkpoint saved for fold 8, session 3, epoch 13\n",
      "Session 3 Epoch 14 - Train Loss: 0.483976\n",
      "Checkpoint saved for fold 8, session 3, epoch 14\n",
      "Session 3 Epoch 15 - Train Loss: 0.483369\n",
      "Checkpoint saved for fold 8, session 3, epoch 15\n",
      "Session 3 Epoch 16 - Train Loss: 0.482755\n",
      "Checkpoint saved for fold 8, session 3, epoch 16\n",
      "Session 3 Epoch 17 - Train Loss: 0.482289\n",
      "Checkpoint saved for fold 8, session 3, epoch 17\n",
      "Session 3 Epoch 18 - Train Loss: 0.481736\n",
      "Checkpoint saved for fold 8, session 3, epoch 18\n",
      "Session 3 Epoch 19 - Train Loss: 0.480343\n",
      "Checkpoint saved for fold 8, session 3, epoch 19\n",
      "Session 3 Epoch 20 - Train Loss: 0.480168\n",
      "Checkpoint saved for fold 8, session 3, epoch 20\n",
      "Session 3 Epoch 21 - Train Loss: 0.478895\n",
      "Checkpoint saved for fold 8, session 3, epoch 21\n",
      "Session 3 Epoch 22 - Train Loss: 0.478979\n",
      "Session 3 Epoch 23 - Train Loss: 0.478355\n",
      "Checkpoint saved for fold 8, session 3, epoch 23\n",
      "Session 3 Epoch 24 - Train Loss: 0.477381\n",
      "Checkpoint saved for fold 8, session 3, epoch 24\n",
      "Session 3 Epoch 25 - Train Loss: 0.477291\n",
      "Session 3 Epoch 26 - Train Loss: 0.476170\n",
      "Checkpoint saved for fold 8, session 3, epoch 26\n",
      "Session 3 Epoch 27 - Train Loss: 0.475334\n",
      "Checkpoint saved for fold 8, session 3, epoch 27\n",
      "Session 3 Epoch 28 - Train Loss: 0.475523\n",
      "Session 3 Epoch 29 - Train Loss: 0.474956\n",
      "Checkpoint saved for fold 8, session 3, epoch 29\n",
      "Session 3 Epoch 30 - Train Loss: 0.474137\n",
      "Checkpoint saved for fold 8, session 3, epoch 30\n",
      "Session 3 Epoch 31 - Train Loss: 0.473263\n",
      "Checkpoint saved for fold 8, session 3, epoch 31\n",
      "Session 3 Epoch 32 - Train Loss: 0.472506\n",
      "Checkpoint saved for fold 8, session 3, epoch 32\n",
      "Session 3 Epoch 33 - Train Loss: 0.471949\n",
      "Checkpoint saved for fold 8, session 3, epoch 33\n",
      "Session 3 Epoch 34 - Train Loss: 0.471173\n",
      "Checkpoint saved for fold 8, session 3, epoch 34\n",
      "Session 3 Epoch 35 - Train Loss: 0.471063\n",
      "Checkpoint saved for fold 8, session 3, epoch 35\n",
      "Session 3 Epoch 36 - Train Loss: 0.470035\n",
      "Checkpoint saved for fold 8, session 3, epoch 36\n",
      "Session 3 Epoch 37 - Train Loss: 0.469944\n",
      "Session 3 Epoch 38 - Train Loss: 0.468838\n",
      "Checkpoint saved for fold 8, session 3, epoch 38\n",
      "Session 3 Epoch 39 - Train Loss: 0.468814\n",
      "Session 3 Epoch 40 - Train Loss: 0.467949\n",
      "Checkpoint saved for fold 8, session 3, epoch 40\n",
      "Session 3 Epoch 41 - Train Loss: 0.467503\n",
      "Checkpoint saved for fold 8, session 3, epoch 41\n",
      "Session 3 Epoch 42 - Train Loss: 0.466573\n",
      "Checkpoint saved for fold 8, session 3, epoch 42\n",
      "Session 3 Epoch 43 - Train Loss: 0.466327\n",
      "Checkpoint saved for fold 8, session 3, epoch 43\n",
      "Session 3 Epoch 44 - Train Loss: 0.464856\n",
      "Checkpoint saved for fold 8, session 3, epoch 44\n",
      "Session 3 Epoch 45 - Train Loss: 0.464676\n",
      "Checkpoint saved for fold 8, session 3, epoch 45\n",
      "Session 3 Epoch 46 - Train Loss: 0.464341\n",
      "Checkpoint saved for fold 8, session 3, epoch 46\n",
      "Session 3 Epoch 47 - Train Loss: 0.463776\n",
      "Checkpoint saved for fold 8, session 3, epoch 47\n",
      "Session 3 Epoch 48 - Train Loss: 0.463067\n",
      "Checkpoint saved for fold 8, session 3, epoch 48\n",
      "Session 3 Epoch 49 - Train Loss: 0.462690\n",
      "Checkpoint saved for fold 8, session 3, epoch 49\n",
      "Session 3 Epoch 50 - Train Loss: 0.461912\n",
      "Checkpoint saved for fold 8, session 3, epoch 50\n",
      "Session 3 Epoch 51 - Train Loss: 0.460876\n",
      "Checkpoint saved for fold 8, session 3, epoch 51\n",
      "Session 3 Epoch 52 - Train Loss: 0.460960\n",
      "Session 3 Epoch 53 - Train Loss: 0.460448\n",
      "Checkpoint saved for fold 8, session 3, epoch 53\n",
      "Session 3 Epoch 54 - Train Loss: 0.460085\n",
      "Checkpoint saved for fold 8, session 3, epoch 54\n",
      "Session 3 Epoch 55 - Train Loss: 0.459199\n",
      "Checkpoint saved for fold 8, session 3, epoch 55\n",
      "Session 3 Epoch 56 - Train Loss: 0.458659\n",
      "Checkpoint saved for fold 8, session 3, epoch 56\n",
      "Session 3 Epoch 57 - Train Loss: 0.457362\n",
      "Checkpoint saved for fold 8, session 3, epoch 57\n",
      "Session 3 Epoch 58 - Train Loss: 0.457354\n",
      "Session 3 Epoch 59 - Train Loss: 0.456504\n",
      "Checkpoint saved for fold 8, session 3, epoch 59\n",
      "Session 3 Epoch 60 - Train Loss: 0.455776\n",
      "Checkpoint saved for fold 8, session 3, epoch 60\n",
      "Session 3 Epoch 61 - Train Loss: 0.455603\n",
      "Checkpoint saved for fold 8, session 3, epoch 61\n",
      "Session 3 Epoch 62 - Train Loss: 0.454656\n",
      "Checkpoint saved for fold 8, session 3, epoch 62\n",
      "Session 3 Epoch 63 - Train Loss: 0.454267\n",
      "Checkpoint saved for fold 8, session 3, epoch 63\n",
      "Session 3 Epoch 64 - Train Loss: 0.453722\n",
      "Checkpoint saved for fold 8, session 3, epoch 64\n",
      "Session 3 Epoch 65 - Train Loss: 0.453092\n",
      "Checkpoint saved for fold 8, session 3, epoch 65\n",
      "Session 3 Epoch 66 - Train Loss: 0.452786\n",
      "Checkpoint saved for fold 8, session 3, epoch 66\n",
      "Session 3 Epoch 67 - Train Loss: 0.451771\n",
      "Checkpoint saved for fold 8, session 3, epoch 67\n",
      "Session 3 Epoch 68 - Train Loss: 0.451407\n",
      "Checkpoint saved for fold 8, session 3, epoch 68\n",
      "Session 3 Epoch 69 - Train Loss: 0.450662\n",
      "Checkpoint saved for fold 8, session 3, epoch 69\n",
      "Session 3 Epoch 70 - Train Loss: 0.450359\n",
      "Checkpoint saved for fold 8, session 3, epoch 70\n",
      "Session 3 Epoch 71 - Train Loss: 0.449651\n",
      "Checkpoint saved for fold 8, session 3, epoch 71\n",
      "Session 3 Epoch 72 - Train Loss: 0.448887\n",
      "Checkpoint saved for fold 8, session 3, epoch 72\n",
      "Session 3 Epoch 73 - Train Loss: 0.448566\n",
      "Checkpoint saved for fold 8, session 3, epoch 73\n",
      "Session 3 Epoch 74 - Train Loss: 0.448268\n",
      "Checkpoint saved for fold 8, session 3, epoch 74\n",
      "Session 3 Epoch 75 - Train Loss: 0.447168\n",
      "Checkpoint saved for fold 8, session 3, epoch 75\n",
      "Session 3 Epoch 76 - Train Loss: 0.446938\n",
      "Checkpoint saved for fold 8, session 3, epoch 76\n",
      "Session 3 Epoch 77 - Train Loss: 0.446497\n",
      "Checkpoint saved for fold 8, session 3, epoch 77\n",
      "Session 3 Epoch 78 - Train Loss: 0.445068\n",
      "Checkpoint saved for fold 8, session 3, epoch 78\n",
      "Session 3 Epoch 79 - Train Loss: 0.445769\n",
      "Session 3 Epoch 80 - Train Loss: 0.444850\n",
      "Checkpoint saved for fold 8, session 3, epoch 80\n",
      "Training on Session 5/9\n",
      "Session 4 Epoch 1 - Train Loss: 0.187249\n",
      "Checkpoint saved for fold 8, session 4, epoch 1\n",
      "Session 4 Epoch 2 - Train Loss: 0.186994\n",
      "Checkpoint saved for fold 8, session 4, epoch 2\n",
      "Session 4 Epoch 3 - Train Loss: 0.186821\n",
      "Checkpoint saved for fold 8, session 4, epoch 3\n",
      "Session 4 Epoch 4 - Train Loss: 0.186739\n",
      "Session 4 Epoch 5 - Train Loss: 0.186318\n",
      "Checkpoint saved for fold 8, session 4, epoch 5\n",
      "Session 4 Epoch 6 - Train Loss: 0.185839\n",
      "Checkpoint saved for fold 8, session 4, epoch 6\n",
      "Session 4 Epoch 7 - Train Loss: 0.185407\n",
      "Checkpoint saved for fold 8, session 4, epoch 7\n",
      "Session 4 Epoch 8 - Train Loss: 0.185388\n",
      "Session 4 Epoch 9 - Train Loss: 0.185077\n",
      "Checkpoint saved for fold 8, session 4, epoch 9\n",
      "Session 4 Epoch 10 - Train Loss: 0.184324\n",
      "Checkpoint saved for fold 8, session 4, epoch 10\n",
      "Session 4 Epoch 11 - Train Loss: 0.184400\n",
      "Session 4 Epoch 12 - Train Loss: 0.184101\n",
      "Checkpoint saved for fold 8, session 4, epoch 12\n",
      "Session 4 Epoch 13 - Train Loss: 0.183749\n",
      "Checkpoint saved for fold 8, session 4, epoch 13\n",
      "Session 4 Epoch 14 - Train Loss: 0.183215\n",
      "Checkpoint saved for fold 8, session 4, epoch 14\n",
      "Session 4 Epoch 15 - Train Loss: 0.182958\n",
      "Checkpoint saved for fold 8, session 4, epoch 15\n",
      "Session 4 Epoch 16 - Train Loss: 0.182748\n",
      "Checkpoint saved for fold 8, session 4, epoch 16\n",
      "Session 4 Epoch 17 - Train Loss: 0.182334\n",
      "Checkpoint saved for fold 8, session 4, epoch 17\n",
      "Session 4 Epoch 18 - Train Loss: 0.181999\n",
      "Checkpoint saved for fold 8, session 4, epoch 18\n",
      "Session 4 Epoch 19 - Train Loss: 0.181985\n",
      "Session 4 Epoch 20 - Train Loss: 0.181221\n",
      "Checkpoint saved for fold 8, session 4, epoch 20\n",
      "Session 4 Epoch 21 - Train Loss: 0.180789\n",
      "Checkpoint saved for fold 8, session 4, epoch 21\n",
      "Session 4 Epoch 22 - Train Loss: 0.180981\n",
      "Session 4 Epoch 23 - Train Loss: 0.180516\n",
      "Checkpoint saved for fold 8, session 4, epoch 23\n",
      "Session 4 Epoch 24 - Train Loss: 0.180294\n",
      "Checkpoint saved for fold 8, session 4, epoch 24\n",
      "Session 4 Epoch 25 - Train Loss: 0.179654\n",
      "Checkpoint saved for fold 8, session 4, epoch 25\n",
      "Session 4 Epoch 26 - Train Loss: 0.179563\n",
      "Session 4 Epoch 27 - Train Loss: 0.179182\n",
      "Checkpoint saved for fold 8, session 4, epoch 27\n",
      "Session 4 Epoch 28 - Train Loss: 0.178789\n",
      "Checkpoint saved for fold 8, session 4, epoch 28\n",
      "Session 4 Epoch 29 - Train Loss: 0.178622\n",
      "Checkpoint saved for fold 8, session 4, epoch 29\n",
      "Session 4 Epoch 30 - Train Loss: 0.178047\n",
      "Checkpoint saved for fold 8, session 4, epoch 30\n",
      "Session 4 Epoch 31 - Train Loss: 0.177974\n",
      "Session 4 Epoch 32 - Train Loss: 0.177662\n",
      "Checkpoint saved for fold 8, session 4, epoch 32\n",
      "Session 4 Epoch 33 - Train Loss: 0.177312\n",
      "Checkpoint saved for fold 8, session 4, epoch 33\n",
      "Session 4 Epoch 34 - Train Loss: 0.176959\n",
      "Checkpoint saved for fold 8, session 4, epoch 34\n",
      "Session 4 Epoch 35 - Train Loss: 0.176506\n",
      "Checkpoint saved for fold 8, session 4, epoch 35\n",
      "Session 4 Epoch 36 - Train Loss: 0.176246\n",
      "Checkpoint saved for fold 8, session 4, epoch 36\n",
      "Session 4 Epoch 37 - Train Loss: 0.176097\n",
      "Checkpoint saved for fold 8, session 4, epoch 37\n",
      "Session 4 Epoch 38 - Train Loss: 0.175839\n",
      "Checkpoint saved for fold 8, session 4, epoch 38\n",
      "Session 4 Epoch 39 - Train Loss: 0.175578\n",
      "Checkpoint saved for fold 8, session 4, epoch 39\n",
      "Session 4 Epoch 40 - Train Loss: 0.174981\n",
      "Checkpoint saved for fold 8, session 4, epoch 40\n",
      "Session 4 Epoch 41 - Train Loss: 0.174617\n",
      "Checkpoint saved for fold 8, session 4, epoch 41\n",
      "Session 4 Epoch 42 - Train Loss: 0.174388\n",
      "Checkpoint saved for fold 8, session 4, epoch 42\n",
      "Session 4 Epoch 43 - Train Loss: 0.174054\n",
      "Checkpoint saved for fold 8, session 4, epoch 43\n",
      "Session 4 Epoch 44 - Train Loss: 0.173968\n",
      "Session 4 Epoch 45 - Train Loss: 0.173671\n",
      "Checkpoint saved for fold 8, session 4, epoch 45\n",
      "Session 4 Epoch 46 - Train Loss: 0.173092\n",
      "Checkpoint saved for fold 8, session 4, epoch 46\n",
      "Session 4 Epoch 47 - Train Loss: 0.172971\n",
      "Checkpoint saved for fold 8, session 4, epoch 47\n",
      "Session 4 Epoch 48 - Train Loss: 0.172532\n",
      "Checkpoint saved for fold 8, session 4, epoch 48\n",
      "Session 4 Epoch 49 - Train Loss: 0.172213\n",
      "Checkpoint saved for fold 8, session 4, epoch 49\n",
      "Session 4 Epoch 50 - Train Loss: 0.172069\n",
      "Checkpoint saved for fold 8, session 4, epoch 50\n",
      "Session 4 Epoch 51 - Train Loss: 0.171893\n",
      "Checkpoint saved for fold 8, session 4, epoch 51\n",
      "Session 4 Epoch 52 - Train Loss: 0.171426\n",
      "Checkpoint saved for fold 8, session 4, epoch 52\n",
      "Session 4 Epoch 53 - Train Loss: 0.171233\n",
      "Checkpoint saved for fold 8, session 4, epoch 53\n",
      "Session 4 Epoch 54 - Train Loss: 0.170982\n",
      "Checkpoint saved for fold 8, session 4, epoch 54\n",
      "Session 4 Epoch 55 - Train Loss: 0.170845\n",
      "Checkpoint saved for fold 8, session 4, epoch 55\n",
      "Session 4 Epoch 56 - Train Loss: 0.170414\n",
      "Checkpoint saved for fold 8, session 4, epoch 56\n",
      "Session 4 Epoch 57 - Train Loss: 0.170176\n",
      "Checkpoint saved for fold 8, session 4, epoch 57\n",
      "Session 4 Epoch 58 - Train Loss: 0.169937\n",
      "Checkpoint saved for fold 8, session 4, epoch 58\n",
      "Session 4 Epoch 59 - Train Loss: 0.169195\n",
      "Checkpoint saved for fold 8, session 4, epoch 59\n",
      "Session 4 Epoch 60 - Train Loss: 0.169141\n",
      "Session 4 Epoch 61 - Train Loss: 0.169044\n",
      "Checkpoint saved for fold 8, session 4, epoch 61\n",
      "Session 4 Epoch 62 - Train Loss: 0.168661\n",
      "Checkpoint saved for fold 8, session 4, epoch 62\n",
      "Session 4 Epoch 63 - Train Loss: 0.168254\n",
      "Checkpoint saved for fold 8, session 4, epoch 63\n",
      "Session 4 Epoch 64 - Train Loss: 0.167989\n",
      "Checkpoint saved for fold 8, session 4, epoch 64\n",
      "Session 4 Epoch 65 - Train Loss: 0.167791\n",
      "Checkpoint saved for fold 8, session 4, epoch 65\n",
      "Session 4 Epoch 66 - Train Loss: 0.167188\n",
      "Checkpoint saved for fold 8, session 4, epoch 66\n",
      "Session 4 Epoch 67 - Train Loss: 0.167114\n",
      "Session 4 Epoch 68 - Train Loss: 0.166757\n",
      "Checkpoint saved for fold 8, session 4, epoch 68\n",
      "Session 4 Epoch 69 - Train Loss: 0.166463\n",
      "Checkpoint saved for fold 8, session 4, epoch 69\n",
      "Session 4 Epoch 70 - Train Loss: 0.166218\n",
      "Checkpoint saved for fold 8, session 4, epoch 70\n",
      "Session 4 Epoch 71 - Train Loss: 0.165856\n",
      "Checkpoint saved for fold 8, session 4, epoch 71\n",
      "Session 4 Epoch 72 - Train Loss: 0.165614\n",
      "Checkpoint saved for fold 8, session 4, epoch 72\n",
      "Session 4 Epoch 73 - Train Loss: 0.165315\n",
      "Checkpoint saved for fold 8, session 4, epoch 73\n",
      "Session 4 Epoch 74 - Train Loss: 0.165046\n",
      "Checkpoint saved for fold 8, session 4, epoch 74\n",
      "Session 4 Epoch 75 - Train Loss: 0.164618\n",
      "Checkpoint saved for fold 8, session 4, epoch 75\n",
      "Session 4 Epoch 76 - Train Loss: 0.164807\n",
      "Session 4 Epoch 77 - Train Loss: 0.164023\n",
      "Checkpoint saved for fold 8, session 4, epoch 77\n",
      "Session 4 Epoch 78 - Train Loss: 0.163982\n",
      "Session 4 Epoch 79 - Train Loss: 0.163633\n",
      "Checkpoint saved for fold 8, session 4, epoch 79\n",
      "Session 4 Epoch 80 - Train Loss: 0.163765\n",
      "Training on Session 6/9\n",
      "Session 5 Epoch 1 - Train Loss: 0.536486\n",
      "Checkpoint saved for fold 8, session 5, epoch 1\n",
      "Session 5 Epoch 2 - Train Loss: 0.535524\n",
      "Checkpoint saved for fold 8, session 5, epoch 2\n",
      "Session 5 Epoch 3 - Train Loss: 0.534455\n",
      "Checkpoint saved for fold 8, session 5, epoch 3\n",
      "Session 5 Epoch 4 - Train Loss: 0.534176\n",
      "Checkpoint saved for fold 8, session 5, epoch 4\n",
      "Session 5 Epoch 5 - Train Loss: 0.533711\n",
      "Checkpoint saved for fold 8, session 5, epoch 5\n",
      "Session 5 Epoch 6 - Train Loss: 0.532341\n",
      "Checkpoint saved for fold 8, session 5, epoch 6\n",
      "Session 5 Epoch 7 - Train Loss: 0.532383\n",
      "Session 5 Epoch 8 - Train Loss: 0.531766\n",
      "Checkpoint saved for fold 8, session 5, epoch 8\n",
      "Session 5 Epoch 9 - Train Loss: 0.531504\n",
      "Checkpoint saved for fold 8, session 5, epoch 9\n",
      "Session 5 Epoch 10 - Train Loss: 0.530387\n",
      "Checkpoint saved for fold 8, session 5, epoch 10\n",
      "Session 5 Epoch 11 - Train Loss: 0.529212\n",
      "Checkpoint saved for fold 8, session 5, epoch 11\n",
      "Session 5 Epoch 12 - Train Loss: 0.529348\n",
      "Session 5 Epoch 13 - Train Loss: 0.529310\n",
      "Session 5 Epoch 14 - Train Loss: 0.527767\n",
      "Checkpoint saved for fold 8, session 5, epoch 14\n",
      "Session 5 Epoch 15 - Train Loss: 0.527818\n",
      "Session 5 Epoch 16 - Train Loss: 0.527114\n",
      "Checkpoint saved for fold 8, session 5, epoch 16\n",
      "Session 5 Epoch 17 - Train Loss: 0.526194\n",
      "Checkpoint saved for fold 8, session 5, epoch 17\n",
      "Session 5 Epoch 18 - Train Loss: 0.525824\n",
      "Checkpoint saved for fold 8, session 5, epoch 18\n",
      "Session 5 Epoch 19 - Train Loss: 0.525481\n",
      "Checkpoint saved for fold 8, session 5, epoch 19\n",
      "Session 5 Epoch 20 - Train Loss: 0.525229\n",
      "Checkpoint saved for fold 8, session 5, epoch 20\n",
      "Session 5 Epoch 21 - Train Loss: 0.524040\n",
      "Checkpoint saved for fold 8, session 5, epoch 21\n",
      "Session 5 Epoch 22 - Train Loss: 0.523893\n",
      "Checkpoint saved for fold 8, session 5, epoch 22\n",
      "Session 5 Epoch 23 - Train Loss: 0.523117\n",
      "Checkpoint saved for fold 8, session 5, epoch 23\n",
      "Session 5 Epoch 24 - Train Loss: 0.521801\n",
      "Checkpoint saved for fold 8, session 5, epoch 24\n",
      "Session 5 Epoch 25 - Train Loss: 0.521161\n",
      "Checkpoint saved for fold 8, session 5, epoch 25\n",
      "Session 5 Epoch 26 - Train Loss: 0.521590\n",
      "Session 5 Epoch 27 - Train Loss: 0.520062\n",
      "Checkpoint saved for fold 8, session 5, epoch 27\n",
      "Session 5 Epoch 28 - Train Loss: 0.519664\n",
      "Checkpoint saved for fold 8, session 5, epoch 28\n",
      "Session 5 Epoch 29 - Train Loss: 0.519409\n",
      "Checkpoint saved for fold 8, session 5, epoch 29\n",
      "Session 5 Epoch 30 - Train Loss: 0.518285\n",
      "Checkpoint saved for fold 8, session 5, epoch 30\n",
      "Session 5 Epoch 31 - Train Loss: 0.518314\n",
      "Session 5 Epoch 32 - Train Loss: 0.517954\n",
      "Checkpoint saved for fold 8, session 5, epoch 32\n",
      "Session 5 Epoch 33 - Train Loss: 0.517248\n",
      "Checkpoint saved for fold 8, session 5, epoch 33\n",
      "Session 5 Epoch 34 - Train Loss: 0.516023\n",
      "Checkpoint saved for fold 8, session 5, epoch 34\n",
      "Session 5 Epoch 35 - Train Loss: 0.515679\n",
      "Checkpoint saved for fold 8, session 5, epoch 35\n",
      "Session 5 Epoch 36 - Train Loss: 0.515160\n",
      "Checkpoint saved for fold 8, session 5, epoch 36\n",
      "Session 5 Epoch 37 - Train Loss: 0.515105\n",
      "Session 5 Epoch 38 - Train Loss: 0.513935\n",
      "Checkpoint saved for fold 8, session 5, epoch 38\n",
      "Session 5 Epoch 39 - Train Loss: 0.513658\n",
      "Checkpoint saved for fold 8, session 5, epoch 39\n",
      "Session 5 Epoch 40 - Train Loss: 0.513009\n",
      "Checkpoint saved for fold 8, session 5, epoch 40\n",
      "Session 5 Epoch 41 - Train Loss: 0.512750\n",
      "Checkpoint saved for fold 8, session 5, epoch 41\n",
      "Session 5 Epoch 42 - Train Loss: 0.511635\n",
      "Checkpoint saved for fold 8, session 5, epoch 42\n",
      "Session 5 Epoch 43 - Train Loss: 0.511588\n",
      "Session 5 Epoch 44 - Train Loss: 0.510811\n",
      "Checkpoint saved for fold 8, session 5, epoch 44\n",
      "Session 5 Epoch 45 - Train Loss: 0.510365\n",
      "Checkpoint saved for fold 8, session 5, epoch 45\n",
      "Session 5 Epoch 46 - Train Loss: 0.509466\n",
      "Checkpoint saved for fold 8, session 5, epoch 46\n",
      "Session 5 Epoch 47 - Train Loss: 0.509285\n",
      "Checkpoint saved for fold 8, session 5, epoch 47\n",
      "Session 5 Epoch 48 - Train Loss: 0.509252\n",
      "Session 5 Epoch 49 - Train Loss: 0.508755\n",
      "Checkpoint saved for fold 8, session 5, epoch 49\n",
      "Session 5 Epoch 50 - Train Loss: 0.507581\n",
      "Checkpoint saved for fold 8, session 5, epoch 50\n",
      "Session 5 Epoch 51 - Train Loss: 0.506640\n",
      "Checkpoint saved for fold 8, session 5, epoch 51\n",
      "Session 5 Epoch 52 - Train Loss: 0.506703\n",
      "Session 5 Epoch 53 - Train Loss: 0.506820\n",
      "Session 5 Epoch 54 - Train Loss: 0.505302\n",
      "Checkpoint saved for fold 8, session 5, epoch 54\n",
      "Session 5 Epoch 55 - Train Loss: 0.504753\n",
      "Checkpoint saved for fold 8, session 5, epoch 55\n",
      "Session 5 Epoch 56 - Train Loss: 0.504680\n",
      "Session 5 Epoch 57 - Train Loss: 0.503671\n",
      "Checkpoint saved for fold 8, session 5, epoch 57\n",
      "Session 5 Epoch 58 - Train Loss: 0.503151\n",
      "Checkpoint saved for fold 8, session 5, epoch 58\n",
      "Session 5 Epoch 59 - Train Loss: 0.502664\n",
      "Checkpoint saved for fold 8, session 5, epoch 59\n",
      "Session 5 Epoch 60 - Train Loss: 0.502582\n",
      "Session 5 Epoch 61 - Train Loss: 0.501670\n",
      "Checkpoint saved for fold 8, session 5, epoch 61\n",
      "Session 5 Epoch 62 - Train Loss: 0.501364\n",
      "Checkpoint saved for fold 8, session 5, epoch 62\n",
      "Session 5 Epoch 63 - Train Loss: 0.500751\n",
      "Checkpoint saved for fold 8, session 5, epoch 63\n",
      "Session 5 Epoch 64 - Train Loss: 0.500081\n",
      "Checkpoint saved for fold 8, session 5, epoch 64\n",
      "Session 5 Epoch 65 - Train Loss: 0.499552\n",
      "Checkpoint saved for fold 8, session 5, epoch 65\n",
      "Session 5 Epoch 66 - Train Loss: 0.498800\n",
      "Checkpoint saved for fold 8, session 5, epoch 66\n",
      "Session 5 Epoch 67 - Train Loss: 0.498197\n",
      "Checkpoint saved for fold 8, session 5, epoch 67\n",
      "Session 5 Epoch 68 - Train Loss: 0.498271\n",
      "Session 5 Epoch 69 - Train Loss: 0.497349\n",
      "Checkpoint saved for fold 8, session 5, epoch 69\n",
      "Session 5 Epoch 70 - Train Loss: 0.497209\n",
      "Checkpoint saved for fold 8, session 5, epoch 70\n",
      "Session 5 Epoch 71 - Train Loss: 0.496424\n",
      "Checkpoint saved for fold 8, session 5, epoch 71\n",
      "Session 5 Epoch 72 - Train Loss: 0.495028\n",
      "Checkpoint saved for fold 8, session 5, epoch 72\n",
      "Session 5 Epoch 73 - Train Loss: 0.494975\n",
      "Session 5 Epoch 74 - Train Loss: 0.494851\n",
      "Checkpoint saved for fold 8, session 5, epoch 74\n",
      "Session 5 Epoch 75 - Train Loss: 0.494082\n",
      "Checkpoint saved for fold 8, session 5, epoch 75\n",
      "Session 5 Epoch 76 - Train Loss: 0.493017\n",
      "Checkpoint saved for fold 8, session 5, epoch 76\n",
      "Session 5 Epoch 77 - Train Loss: 0.493029\n",
      "Session 5 Epoch 78 - Train Loss: 0.491844\n",
      "Checkpoint saved for fold 8, session 5, epoch 78\n",
      "Session 5 Epoch 79 - Train Loss: 0.491983\n",
      "Session 5 Epoch 80 - Train Loss: 0.491613\n",
      "Checkpoint saved for fold 8, session 5, epoch 80\n",
      "Training on Session 7/9\n",
      "Session 6 Epoch 1 - Train Loss: 0.385015\n",
      "Checkpoint saved for fold 8, session 6, epoch 1\n",
      "Session 6 Epoch 2 - Train Loss: 0.384895\n",
      "Checkpoint saved for fold 8, session 6, epoch 2\n",
      "Session 6 Epoch 3 - Train Loss: 0.384415\n",
      "Checkpoint saved for fold 8, session 6, epoch 3\n",
      "Session 6 Epoch 4 - Train Loss: 0.383821\n",
      "Checkpoint saved for fold 8, session 6, epoch 4\n",
      "Session 6 Epoch 5 - Train Loss: 0.383606\n",
      "Checkpoint saved for fold 8, session 6, epoch 5\n",
      "Session 6 Epoch 6 - Train Loss: 0.383248\n",
      "Checkpoint saved for fold 8, session 6, epoch 6\n",
      "Session 6 Epoch 7 - Train Loss: 0.382476\n",
      "Checkpoint saved for fold 8, session 6, epoch 7\n",
      "Session 6 Epoch 8 - Train Loss: 0.382196\n",
      "Checkpoint saved for fold 8, session 6, epoch 8\n",
      "Session 6 Epoch 9 - Train Loss: 0.381895\n",
      "Checkpoint saved for fold 8, session 6, epoch 9\n",
      "Session 6 Epoch 10 - Train Loss: 0.381342\n",
      "Checkpoint saved for fold 8, session 6, epoch 10\n",
      "Session 6 Epoch 11 - Train Loss: 0.380606\n",
      "Checkpoint saved for fold 8, session 6, epoch 11\n",
      "Session 6 Epoch 12 - Train Loss: 0.380521\n",
      "Session 6 Epoch 13 - Train Loss: 0.380416\n",
      "Checkpoint saved for fold 8, session 6, epoch 13\n",
      "Session 6 Epoch 14 - Train Loss: 0.379515\n",
      "Checkpoint saved for fold 8, session 6, epoch 14\n",
      "Session 6 Epoch 15 - Train Loss: 0.379531\n",
      "Session 6 Epoch 16 - Train Loss: 0.378517\n",
      "Checkpoint saved for fold 8, session 6, epoch 16\n",
      "Session 6 Epoch 17 - Train Loss: 0.378178\n",
      "Checkpoint saved for fold 8, session 6, epoch 17\n",
      "Session 6 Epoch 18 - Train Loss: 0.378205\n",
      "Session 6 Epoch 19 - Train Loss: 0.377823\n",
      "Checkpoint saved for fold 8, session 6, epoch 19\n",
      "Session 6 Epoch 20 - Train Loss: 0.377224\n",
      "Checkpoint saved for fold 8, session 6, epoch 20\n",
      "Session 6 Epoch 21 - Train Loss: 0.376879\n",
      "Checkpoint saved for fold 8, session 6, epoch 21\n",
      "Session 6 Epoch 22 - Train Loss: 0.376540\n",
      "Checkpoint saved for fold 8, session 6, epoch 22\n",
      "Session 6 Epoch 23 - Train Loss: 0.375957\n",
      "Checkpoint saved for fold 8, session 6, epoch 23\n",
      "Session 6 Epoch 24 - Train Loss: 0.375575\n",
      "Checkpoint saved for fold 8, session 6, epoch 24\n",
      "Session 6 Epoch 25 - Train Loss: 0.375424\n",
      "Checkpoint saved for fold 8, session 6, epoch 25\n",
      "Session 6 Epoch 26 - Train Loss: 0.374757\n",
      "Checkpoint saved for fold 8, session 6, epoch 26\n",
      "Session 6 Epoch 27 - Train Loss: 0.374341\n",
      "Checkpoint saved for fold 8, session 6, epoch 27\n",
      "Session 6 Epoch 28 - Train Loss: 0.373853\n",
      "Checkpoint saved for fold 8, session 6, epoch 28\n",
      "Session 6 Epoch 29 - Train Loss: 0.373276\n",
      "Checkpoint saved for fold 8, session 6, epoch 29\n",
      "Session 6 Epoch 30 - Train Loss: 0.372977\n",
      "Checkpoint saved for fold 8, session 6, epoch 30\n",
      "Session 6 Epoch 31 - Train Loss: 0.372728\n",
      "Checkpoint saved for fold 8, session 6, epoch 31\n",
      "Session 6 Epoch 32 - Train Loss: 0.371829\n",
      "Checkpoint saved for fold 8, session 6, epoch 32\n",
      "Session 6 Epoch 33 - Train Loss: 0.371728\n",
      "Checkpoint saved for fold 8, session 6, epoch 33\n",
      "Session 6 Epoch 34 - Train Loss: 0.371178\n",
      "Checkpoint saved for fold 8, session 6, epoch 34\n",
      "Session 6 Epoch 35 - Train Loss: 0.371095\n",
      "Session 6 Epoch 36 - Train Loss: 0.370372\n",
      "Checkpoint saved for fold 8, session 6, epoch 36\n",
      "Session 6 Epoch 37 - Train Loss: 0.370362\n",
      "Session 6 Epoch 38 - Train Loss: 0.369641\n",
      "Checkpoint saved for fold 8, session 6, epoch 38\n",
      "Session 6 Epoch 39 - Train Loss: 0.369043\n",
      "Checkpoint saved for fold 8, session 6, epoch 39\n",
      "Session 6 Epoch 40 - Train Loss: 0.368934\n",
      "Checkpoint saved for fold 8, session 6, epoch 40\n",
      "Session 6 Epoch 41 - Train Loss: 0.368501\n",
      "Checkpoint saved for fold 8, session 6, epoch 41\n",
      "Session 6 Epoch 42 - Train Loss: 0.368450\n",
      "Session 6 Epoch 43 - Train Loss: 0.367986\n",
      "Checkpoint saved for fold 8, session 6, epoch 43\n",
      "Session 6 Epoch 44 - Train Loss: 0.367721\n",
      "Checkpoint saved for fold 8, session 6, epoch 44\n",
      "Session 6 Epoch 45 - Train Loss: 0.367017\n",
      "Checkpoint saved for fold 8, session 6, epoch 45\n",
      "Session 6 Epoch 46 - Train Loss: 0.366503\n",
      "Checkpoint saved for fold 8, session 6, epoch 46\n",
      "Session 6 Epoch 47 - Train Loss: 0.366095\n",
      "Checkpoint saved for fold 8, session 6, epoch 47\n",
      "Session 6 Epoch 48 - Train Loss: 0.365573\n",
      "Checkpoint saved for fold 8, session 6, epoch 48\n",
      "Session 6 Epoch 49 - Train Loss: 0.365375\n",
      "Checkpoint saved for fold 8, session 6, epoch 49\n",
      "Session 6 Epoch 50 - Train Loss: 0.365396\n",
      "Session 6 Epoch 51 - Train Loss: 0.364585\n",
      "Checkpoint saved for fold 8, session 6, epoch 51\n",
      "Session 6 Epoch 52 - Train Loss: 0.364234\n",
      "Checkpoint saved for fold 8, session 6, epoch 52\n",
      "Session 6 Epoch 53 - Train Loss: 0.363813\n",
      "Checkpoint saved for fold 8, session 6, epoch 53\n",
      "Session 6 Epoch 54 - Train Loss: 0.363048\n",
      "Checkpoint saved for fold 8, session 6, epoch 54\n",
      "Session 6 Epoch 55 - Train Loss: 0.362871\n",
      "Checkpoint saved for fold 8, session 6, epoch 55\n",
      "Session 6 Epoch 56 - Train Loss: 0.362633\n",
      "Checkpoint saved for fold 8, session 6, epoch 56\n",
      "Session 6 Epoch 57 - Train Loss: 0.361814\n",
      "Checkpoint saved for fold 8, session 6, epoch 57\n",
      "Session 6 Epoch 58 - Train Loss: 0.361945\n",
      "Session 6 Epoch 59 - Train Loss: 0.361727\n",
      "Session 6 Epoch 60 - Train Loss: 0.360801\n",
      "Checkpoint saved for fold 8, session 6, epoch 60\n",
      "Session 6 Epoch 61 - Train Loss: 0.360148\n",
      "Checkpoint saved for fold 8, session 6, epoch 61\n",
      "Session 6 Epoch 62 - Train Loss: 0.360075\n",
      "Session 6 Epoch 63 - Train Loss: 0.359708\n",
      "Checkpoint saved for fold 8, session 6, epoch 63\n",
      "Session 6 Epoch 64 - Train Loss: 0.359196\n",
      "Checkpoint saved for fold 8, session 6, epoch 64\n",
      "Session 6 Epoch 65 - Train Loss: 0.358963\n",
      "Checkpoint saved for fold 8, session 6, epoch 65\n",
      "Session 6 Epoch 66 - Train Loss: 0.358588\n",
      "Checkpoint saved for fold 8, session 6, epoch 66\n",
      "Session 6 Epoch 67 - Train Loss: 0.358117\n",
      "Checkpoint saved for fold 8, session 6, epoch 67\n",
      "Session 6 Epoch 68 - Train Loss: 0.358043\n",
      "Session 6 Epoch 69 - Train Loss: 0.357241\n",
      "Checkpoint saved for fold 8, session 6, epoch 69\n",
      "Session 6 Epoch 70 - Train Loss: 0.357313\n",
      "Session 6 Epoch 71 - Train Loss: 0.356780\n",
      "Checkpoint saved for fold 8, session 6, epoch 71\n",
      "Session 6 Epoch 72 - Train Loss: 0.356333\n",
      "Checkpoint saved for fold 8, session 6, epoch 72\n",
      "Session 6 Epoch 73 - Train Loss: 0.355689\n",
      "Checkpoint saved for fold 8, session 6, epoch 73\n",
      "Session 6 Epoch 74 - Train Loss: 0.356048\n",
      "Session 6 Epoch 75 - Train Loss: 0.355204\n",
      "Checkpoint saved for fold 8, session 6, epoch 75\n",
      "Session 6 Epoch 76 - Train Loss: 0.354994\n",
      "Checkpoint saved for fold 8, session 6, epoch 76\n",
      "Session 6 Epoch 77 - Train Loss: 0.354205\n",
      "Checkpoint saved for fold 8, session 6, epoch 77\n",
      "Session 6 Epoch 78 - Train Loss: 0.354041\n",
      "Checkpoint saved for fold 8, session 6, epoch 78\n",
      "Session 6 Epoch 79 - Train Loss: 0.353701\n",
      "Checkpoint saved for fold 8, session 6, epoch 79\n",
      "Session 6 Epoch 80 - Train Loss: 0.353408\n",
      "Checkpoint saved for fold 8, session 6, epoch 80\n",
      "Training on Session 8/9\n",
      "Session 7 Epoch 1 - Train Loss: 0.141005\n",
      "Checkpoint saved for fold 8, session 7, epoch 1\n",
      "Session 7 Epoch 2 - Train Loss: 0.140975\n",
      "Session 7 Epoch 3 - Train Loss: 0.140868\n",
      "Checkpoint saved for fold 8, session 7, epoch 3\n",
      "Session 7 Epoch 4 - Train Loss: 0.140566\n",
      "Checkpoint saved for fold 8, session 7, epoch 4\n",
      "Session 7 Epoch 5 - Train Loss: 0.140324\n",
      "Checkpoint saved for fold 8, session 7, epoch 5\n",
      "Session 7 Epoch 6 - Train Loss: 0.140048\n",
      "Checkpoint saved for fold 8, session 7, epoch 6\n",
      "Session 7 Epoch 7 - Train Loss: 0.140000\n",
      "Session 7 Epoch 8 - Train Loss: 0.139448\n",
      "Checkpoint saved for fold 8, session 7, epoch 8\n",
      "Session 7 Epoch 9 - Train Loss: 0.139397\n",
      "Session 7 Epoch 10 - Train Loss: 0.139148\n",
      "Checkpoint saved for fold 8, session 7, epoch 10\n",
      "Session 7 Epoch 11 - Train Loss: 0.138999\n",
      "Checkpoint saved for fold 8, session 7, epoch 11\n",
      "Session 7 Epoch 12 - Train Loss: 0.138868\n",
      "Checkpoint saved for fold 8, session 7, epoch 12\n",
      "Session 7 Epoch 13 - Train Loss: 0.138562\n",
      "Checkpoint saved for fold 8, session 7, epoch 13\n",
      "Session 7 Epoch 14 - Train Loss: 0.138282\n",
      "Checkpoint saved for fold 8, session 7, epoch 14\n",
      "Session 7 Epoch 15 - Train Loss: 0.138030\n",
      "Checkpoint saved for fold 8, session 7, epoch 15\n",
      "Session 7 Epoch 16 - Train Loss: 0.137859\n",
      "Checkpoint saved for fold 8, session 7, epoch 16\n",
      "Session 7 Epoch 17 - Train Loss: 0.137842\n",
      "Session 7 Epoch 18 - Train Loss: 0.137398\n",
      "Checkpoint saved for fold 8, session 7, epoch 18\n",
      "Session 7 Epoch 19 - Train Loss: 0.137122\n",
      "Checkpoint saved for fold 8, session 7, epoch 19\n",
      "Session 7 Epoch 20 - Train Loss: 0.136872\n",
      "Checkpoint saved for fold 8, session 7, epoch 20\n",
      "Session 7 Epoch 21 - Train Loss: 0.136954\n",
      "Session 7 Epoch 22 - Train Loss: 0.136776\n",
      "Session 7 Epoch 23 - Train Loss: 0.136479\n",
      "Checkpoint saved for fold 8, session 7, epoch 23\n",
      "Session 7 Epoch 24 - Train Loss: 0.136119\n",
      "Checkpoint saved for fold 8, session 7, epoch 24\n",
      "Session 7 Epoch 25 - Train Loss: 0.136037\n",
      "Session 7 Epoch 26 - Train Loss: 0.135736\n",
      "Checkpoint saved for fold 8, session 7, epoch 26\n",
      "Session 7 Epoch 27 - Train Loss: 0.135685\n",
      "Session 7 Epoch 28 - Train Loss: 0.135302\n",
      "Checkpoint saved for fold 8, session 7, epoch 28\n",
      "Session 7 Epoch 29 - Train Loss: 0.135079\n",
      "Checkpoint saved for fold 8, session 7, epoch 29\n",
      "Session 7 Epoch 30 - Train Loss: 0.134842\n",
      "Checkpoint saved for fold 8, session 7, epoch 30\n",
      "Session 7 Epoch 31 - Train Loss: 0.134746\n",
      "Session 7 Epoch 32 - Train Loss: 0.134483\n",
      "Checkpoint saved for fold 8, session 7, epoch 32\n",
      "Session 7 Epoch 33 - Train Loss: 0.134393\n",
      "Session 7 Epoch 34 - Train Loss: 0.134274\n",
      "Checkpoint saved for fold 8, session 7, epoch 34\n",
      "Session 7 Epoch 35 - Train Loss: 0.133992\n",
      "Checkpoint saved for fold 8, session 7, epoch 35\n",
      "Session 7 Epoch 36 - Train Loss: 0.133821\n",
      "Checkpoint saved for fold 8, session 7, epoch 36\n",
      "Session 7 Epoch 37 - Train Loss: 0.133543\n",
      "Checkpoint saved for fold 8, session 7, epoch 37\n",
      "Session 7 Epoch 38 - Train Loss: 0.133548\n",
      "Session 7 Epoch 39 - Train Loss: 0.133076\n",
      "Checkpoint saved for fold 8, session 7, epoch 39\n",
      "Session 7 Epoch 40 - Train Loss: 0.132922\n",
      "Checkpoint saved for fold 8, session 7, epoch 40\n",
      "Session 7 Epoch 41 - Train Loss: 0.132805\n",
      "Checkpoint saved for fold 8, session 7, epoch 41\n",
      "Session 7 Epoch 42 - Train Loss: 0.132960\n",
      "Session 7 Epoch 43 - Train Loss: 0.132443\n",
      "Checkpoint saved for fold 8, session 7, epoch 43\n",
      "Session 7 Epoch 44 - Train Loss: 0.132389\n",
      "Session 7 Epoch 45 - Train Loss: 0.132131\n",
      "Checkpoint saved for fold 8, session 7, epoch 45\n",
      "Session 7 Epoch 46 - Train Loss: 0.131624\n",
      "Checkpoint saved for fold 8, session 7, epoch 46\n",
      "Session 7 Epoch 47 - Train Loss: 0.131683\n",
      "Session 7 Epoch 48 - Train Loss: 0.131608\n",
      "Session 7 Epoch 49 - Train Loss: 0.131301\n",
      "Checkpoint saved for fold 8, session 7, epoch 49\n",
      "Session 7 Epoch 50 - Train Loss: 0.131057\n",
      "Checkpoint saved for fold 8, session 7, epoch 50\n",
      "Session 7 Epoch 51 - Train Loss: 0.130961\n",
      "Session 7 Epoch 52 - Train Loss: 0.130820\n",
      "Checkpoint saved for fold 8, session 7, epoch 52\n",
      "Session 7 Epoch 53 - Train Loss: 0.130566\n",
      "Checkpoint saved for fold 8, session 7, epoch 53\n",
      "Session 7 Epoch 54 - Train Loss: 0.130242\n",
      "Checkpoint saved for fold 8, session 7, epoch 54\n",
      "Session 7 Epoch 55 - Train Loss: 0.129963\n",
      "Checkpoint saved for fold 8, session 7, epoch 55\n",
      "Session 7 Epoch 56 - Train Loss: 0.129858\n",
      "Checkpoint saved for fold 8, session 7, epoch 56\n",
      "Session 7 Epoch 57 - Train Loss: 0.129782\n",
      "Session 7 Epoch 58 - Train Loss: 0.129448\n",
      "Checkpoint saved for fold 8, session 7, epoch 58\n",
      "Session 7 Epoch 59 - Train Loss: 0.129290\n",
      "Checkpoint saved for fold 8, session 7, epoch 59\n",
      "Session 7 Epoch 60 - Train Loss: 0.129152\n",
      "Checkpoint saved for fold 8, session 7, epoch 60\n",
      "Session 7 Epoch 61 - Train Loss: 0.128932\n",
      "Checkpoint saved for fold 8, session 7, epoch 61\n",
      "Session 7 Epoch 62 - Train Loss: 0.128654\n",
      "Checkpoint saved for fold 8, session 7, epoch 62\n",
      "Session 7 Epoch 63 - Train Loss: 0.128583\n",
      "Session 7 Epoch 64 - Train Loss: 0.128393\n",
      "Checkpoint saved for fold 8, session 7, epoch 64\n",
      "Session 7 Epoch 65 - Train Loss: 0.128096\n",
      "Checkpoint saved for fold 8, session 7, epoch 65\n",
      "Session 7 Epoch 66 - Train Loss: 0.128044\n",
      "Session 7 Epoch 67 - Train Loss: 0.127846\n",
      "Checkpoint saved for fold 8, session 7, epoch 67\n",
      "Session 7 Epoch 68 - Train Loss: 0.127448\n",
      "Checkpoint saved for fold 8, session 7, epoch 68\n",
      "Session 7 Epoch 69 - Train Loss: 0.127387\n",
      "Session 7 Epoch 70 - Train Loss: 0.127207\n",
      "Checkpoint saved for fold 8, session 7, epoch 70\n",
      "Session 7 Epoch 71 - Train Loss: 0.127140\n",
      "Session 7 Epoch 72 - Train Loss: 0.127098\n",
      "Checkpoint saved for fold 8, session 7, epoch 72\n",
      "Session 7 Epoch 73 - Train Loss: 0.126844\n",
      "Checkpoint saved for fold 8, session 7, epoch 73\n",
      "Session 7 Epoch 74 - Train Loss: 0.126636\n",
      "Checkpoint saved for fold 8, session 7, epoch 74\n",
      "Session 7 Epoch 75 - Train Loss: 0.126324\n",
      "Checkpoint saved for fold 8, session 7, epoch 75\n",
      "Session 7 Epoch 76 - Train Loss: 0.126048\n",
      "Checkpoint saved for fold 8, session 7, epoch 76\n",
      "Session 7 Epoch 77 - Train Loss: 0.125927\n",
      "Checkpoint saved for fold 8, session 7, epoch 77\n",
      "Session 7 Epoch 78 - Train Loss: 0.125720\n",
      "Checkpoint saved for fold 8, session 7, epoch 78\n",
      "Session 7 Epoch 79 - Train Loss: 0.125678\n",
      "Session 7 Epoch 80 - Train Loss: 0.125406\n",
      "Checkpoint saved for fold 8, session 7, epoch 80\n",
      "Training on Session 9/9\n",
      "Session 8 Epoch 1 - Train Loss: 0.188381\n",
      "Checkpoint saved for fold 8, session 8, epoch 1\n",
      "Session 8 Epoch 2 - Train Loss: 0.188026\n",
      "Checkpoint saved for fold 8, session 8, epoch 2\n",
      "Session 8 Epoch 3 - Train Loss: 0.188151\n",
      "Session 8 Epoch 4 - Train Loss: 0.187676\n",
      "Checkpoint saved for fold 8, session 8, epoch 4\n",
      "Session 8 Epoch 5 - Train Loss: 0.187610\n",
      "Session 8 Epoch 6 - Train Loss: 0.187411\n",
      "Checkpoint saved for fold 8, session 8, epoch 6\n",
      "Session 8 Epoch 7 - Train Loss: 0.187147\n",
      "Checkpoint saved for fold 8, session 8, epoch 7\n",
      "Session 8 Epoch 8 - Train Loss: 0.186637\n",
      "Checkpoint saved for fold 8, session 8, epoch 8\n",
      "Session 8 Epoch 9 - Train Loss: 0.186881\n",
      "Session 8 Epoch 10 - Train Loss: 0.186297\n",
      "Checkpoint saved for fold 8, session 8, epoch 10\n",
      "Session 8 Epoch 11 - Train Loss: 0.186123\n",
      "Checkpoint saved for fold 8, session 8, epoch 11\n",
      "Session 8 Epoch 12 - Train Loss: 0.185871\n",
      "Checkpoint saved for fold 8, session 8, epoch 12\n",
      "Session 8 Epoch 13 - Train Loss: 0.185868\n",
      "Session 8 Epoch 14 - Train Loss: 0.185599\n",
      "Checkpoint saved for fold 8, session 8, epoch 14\n",
      "Session 8 Epoch 15 - Train Loss: 0.185202\n",
      "Checkpoint saved for fold 8, session 8, epoch 15\n",
      "Session 8 Epoch 16 - Train Loss: 0.185175\n",
      "Session 8 Epoch 17 - Train Loss: 0.184865\n",
      "Checkpoint saved for fold 8, session 8, epoch 17\n",
      "Session 8 Epoch 18 - Train Loss: 0.184354\n",
      "Checkpoint saved for fold 8, session 8, epoch 18\n",
      "Session 8 Epoch 19 - Train Loss: 0.184343\n",
      "Session 8 Epoch 20 - Train Loss: 0.184221\n",
      "Checkpoint saved for fold 8, session 8, epoch 20\n",
      "Session 8 Epoch 21 - Train Loss: 0.183733\n",
      "Checkpoint saved for fold 8, session 8, epoch 21\n",
      "Session 8 Epoch 22 - Train Loss: 0.183718\n",
      "Session 8 Epoch 23 - Train Loss: 0.183410\n",
      "Checkpoint saved for fold 8, session 8, epoch 23\n",
      "Session 8 Epoch 24 - Train Loss: 0.183091\n",
      "Checkpoint saved for fold 8, session 8, epoch 24\n",
      "Session 8 Epoch 25 - Train Loss: 0.183088\n",
      "Session 8 Epoch 26 - Train Loss: 0.182706\n",
      "Checkpoint saved for fold 8, session 8, epoch 26\n",
      "Session 8 Epoch 27 - Train Loss: 0.182486\n",
      "Checkpoint saved for fold 8, session 8, epoch 27\n",
      "Session 8 Epoch 28 - Train Loss: 0.182214\n",
      "Checkpoint saved for fold 8, session 8, epoch 28\n",
      "Session 8 Epoch 29 - Train Loss: 0.182146\n",
      "Session 8 Epoch 30 - Train Loss: 0.181941\n",
      "Checkpoint saved for fold 8, session 8, epoch 30\n",
      "Session 8 Epoch 31 - Train Loss: 0.181424\n",
      "Checkpoint saved for fold 8, session 8, epoch 31\n",
      "Session 8 Epoch 32 - Train Loss: 0.181367\n",
      "Session 8 Epoch 33 - Train Loss: 0.181264\n",
      "Checkpoint saved for fold 8, session 8, epoch 33\n",
      "Session 8 Epoch 34 - Train Loss: 0.181229\n",
      "Session 8 Epoch 35 - Train Loss: 0.180950\n",
      "Checkpoint saved for fold 8, session 8, epoch 35\n",
      "Session 8 Epoch 36 - Train Loss: 0.180522\n",
      "Checkpoint saved for fold 8, session 8, epoch 36\n",
      "Session 8 Epoch 37 - Train Loss: 0.180404\n",
      "Checkpoint saved for fold 8, session 8, epoch 37\n",
      "Session 8 Epoch 38 - Train Loss: 0.180185\n",
      "Checkpoint saved for fold 8, session 8, epoch 38\n",
      "Session 8 Epoch 39 - Train Loss: 0.179935\n",
      "Checkpoint saved for fold 8, session 8, epoch 39\n",
      "Session 8 Epoch 40 - Train Loss: 0.179778\n",
      "Checkpoint saved for fold 8, session 8, epoch 40\n",
      "Session 8 Epoch 41 - Train Loss: 0.179423\n",
      "Checkpoint saved for fold 8, session 8, epoch 41\n",
      "Session 8 Epoch 42 - Train Loss: 0.179412\n",
      "Session 8 Epoch 43 - Train Loss: 0.179142\n",
      "Checkpoint saved for fold 8, session 8, epoch 43\n",
      "Session 8 Epoch 44 - Train Loss: 0.178644\n",
      "Checkpoint saved for fold 8, session 8, epoch 44\n",
      "Session 8 Epoch 45 - Train Loss: 0.178766\n",
      "Session 8 Epoch 46 - Train Loss: 0.178134\n",
      "Checkpoint saved for fold 8, session 8, epoch 46\n",
      "Session 8 Epoch 47 - Train Loss: 0.178166\n",
      "Session 8 Epoch 48 - Train Loss: 0.177984\n",
      "Checkpoint saved for fold 8, session 8, epoch 48\n",
      "Session 8 Epoch 49 - Train Loss: 0.177872\n",
      "Checkpoint saved for fold 8, session 8, epoch 49\n",
      "Session 8 Epoch 50 - Train Loss: 0.177392\n",
      "Checkpoint saved for fold 8, session 8, epoch 50\n",
      "Session 8 Epoch 51 - Train Loss: 0.177511\n",
      "Session 8 Epoch 52 - Train Loss: 0.177140\n",
      "Checkpoint saved for fold 8, session 8, epoch 52\n",
      "Session 8 Epoch 53 - Train Loss: 0.176812\n",
      "Checkpoint saved for fold 8, session 8, epoch 53\n",
      "Session 8 Epoch 54 - Train Loss: 0.176521\n",
      "Checkpoint saved for fold 8, session 8, epoch 54\n",
      "Session 8 Epoch 55 - Train Loss: 0.176618\n",
      "Session 8 Epoch 56 - Train Loss: 0.176263\n",
      "Checkpoint saved for fold 8, session 8, epoch 56\n",
      "Session 8 Epoch 57 - Train Loss: 0.176236\n",
      "Session 8 Epoch 58 - Train Loss: 0.175937\n",
      "Checkpoint saved for fold 8, session 8, epoch 58\n",
      "Session 8 Epoch 59 - Train Loss: 0.175787\n",
      "Checkpoint saved for fold 8, session 8, epoch 59\n",
      "Session 8 Epoch 60 - Train Loss: 0.175323\n",
      "Checkpoint saved for fold 8, session 8, epoch 60\n",
      "Session 8 Epoch 61 - Train Loss: 0.175148\n",
      "Checkpoint saved for fold 8, session 8, epoch 61\n",
      "Session 8 Epoch 62 - Train Loss: 0.174941\n",
      "Checkpoint saved for fold 8, session 8, epoch 62\n",
      "Session 8 Epoch 63 - Train Loss: 0.175270\n",
      "Session 8 Epoch 64 - Train Loss: 0.174711\n",
      "Checkpoint saved for fold 8, session 8, epoch 64\n",
      "Session 8 Epoch 65 - Train Loss: 0.174481\n",
      "Checkpoint saved for fold 8, session 8, epoch 65\n",
      "Session 8 Epoch 66 - Train Loss: 0.174085\n",
      "Checkpoint saved for fold 8, session 8, epoch 66\n",
      "Session 8 Epoch 67 - Train Loss: 0.174066\n",
      "Session 8 Epoch 68 - Train Loss: 0.173899\n",
      "Checkpoint saved for fold 8, session 8, epoch 68\n",
      "Session 8 Epoch 69 - Train Loss: 0.173384\n",
      "Checkpoint saved for fold 8, session 8, epoch 69\n",
      "Session 8 Epoch 70 - Train Loss: 0.173432\n",
      "Session 8 Epoch 71 - Train Loss: 0.173191\n",
      "Checkpoint saved for fold 8, session 8, epoch 71\n",
      "Session 8 Epoch 72 - Train Loss: 0.173336\n",
      "Session 8 Epoch 73 - Train Loss: 0.173094\n",
      "Session 8 Epoch 74 - Train Loss: 0.172577\n",
      "Checkpoint saved for fold 8, session 8, epoch 74\n",
      "Session 8 Epoch 75 - Train Loss: 0.172429\n",
      "Checkpoint saved for fold 8, session 8, epoch 75\n",
      "Session 8 Epoch 76 - Train Loss: 0.172112\n",
      "Checkpoint saved for fold 8, session 8, epoch 76\n",
      "Session 8 Epoch 77 - Train Loss: 0.172170\n",
      "Session 8 Epoch 78 - Train Loss: 0.171698\n",
      "Checkpoint saved for fold 8, session 8, epoch 78\n",
      "Session 8 Epoch 79 - Train Loss: 0.171627\n",
      "Session 8 Epoch 80 - Train Loss: 0.171165\n",
      "Checkpoint saved for fold 8, session 8, epoch 80\n",
      "Fold 8 - Test Loss: 0.2071, R^2: -42277316069751.1328\n",
      "\n",
      "=== Fold 9 ===\n",
      "Training on Session 1/9\n",
      "Session 0 Epoch 1 - Train Loss: 0.054516\n",
      "Checkpoint saved for fold 9, session 0, epoch 1\n",
      "Session 0 Epoch 2 - Train Loss: 0.053260\n",
      "Checkpoint saved for fold 9, session 0, epoch 2\n",
      "Session 0 Epoch 3 - Train Loss: 0.050248\n",
      "Checkpoint saved for fold 9, session 0, epoch 3\n",
      "Session 0 Epoch 4 - Train Loss: 0.044631\n",
      "Checkpoint saved for fold 9, session 0, epoch 4\n",
      "Session 0 Epoch 5 - Train Loss: 0.036801\n",
      "Checkpoint saved for fold 9, session 0, epoch 5\n",
      "Session 0 Epoch 6 - Train Loss: 0.033605\n",
      "Checkpoint saved for fold 9, session 0, epoch 6\n",
      "Session 0 Epoch 7 - Train Loss: 0.030667\n",
      "Checkpoint saved for fold 9, session 0, epoch 7\n",
      "Session 0 Epoch 8 - Train Loss: 0.029979\n",
      "Checkpoint saved for fold 9, session 0, epoch 8\n",
      "Session 0 Epoch 9 - Train Loss: 0.027987\n",
      "Checkpoint saved for fold 9, session 0, epoch 9\n",
      "Session 0 Epoch 10 - Train Loss: 0.028601\n",
      "Session 0 Epoch 11 - Train Loss: 0.027388\n",
      "Checkpoint saved for fold 9, session 0, epoch 11\n",
      "Session 0 Epoch 12 - Train Loss: 0.025517\n",
      "Checkpoint saved for fold 9, session 0, epoch 12\n",
      "Session 0 Epoch 13 - Train Loss: 0.026413\n",
      "Session 0 Epoch 14 - Train Loss: 0.024683\n",
      "Checkpoint saved for fold 9, session 0, epoch 14\n",
      "Session 0 Epoch 15 - Train Loss: 0.024714\n",
      "Session 0 Epoch 16 - Train Loss: 0.023799\n",
      "Checkpoint saved for fold 9, session 0, epoch 16\n",
      "Session 0 Epoch 17 - Train Loss: 0.022394\n",
      "Checkpoint saved for fold 9, session 0, epoch 17\n",
      "Session 0 Epoch 18 - Train Loss: 0.022069\n",
      "Checkpoint saved for fold 9, session 0, epoch 18\n",
      "Session 0 Epoch 19 - Train Loss: 0.021860\n",
      "Checkpoint saved for fold 9, session 0, epoch 19\n",
      "Session 0 Epoch 20 - Train Loss: 0.021565\n",
      "Checkpoint saved for fold 9, session 0, epoch 20\n",
      "Session 0 Epoch 21 - Train Loss: 0.021117\n",
      "Checkpoint saved for fold 9, session 0, epoch 21\n",
      "Session 0 Epoch 22 - Train Loss: 0.019601\n",
      "Checkpoint saved for fold 9, session 0, epoch 22\n",
      "Session 0 Epoch 23 - Train Loss: 0.019953\n",
      "Session 0 Epoch 24 - Train Loss: 0.020007\n",
      "Session 0 Epoch 25 - Train Loss: 0.018972\n",
      "Checkpoint saved for fold 9, session 0, epoch 25\n",
      "Session 0 Epoch 26 - Train Loss: 0.017732\n",
      "Checkpoint saved for fold 9, session 0, epoch 26\n",
      "Session 0 Epoch 27 - Train Loss: 0.018663\n",
      "Session 0 Epoch 28 - Train Loss: 0.018361\n",
      "Session 0 Epoch 29 - Train Loss: 0.017725\n",
      "Session 0 Epoch 30 - Train Loss: 0.017603\n",
      "Checkpoint saved for fold 9, session 0, epoch 30\n",
      "Session 0 Epoch 31 - Train Loss: 0.017417\n",
      "Checkpoint saved for fold 9, session 0, epoch 31\n",
      "Session 0 Epoch 32 - Train Loss: 0.016432\n",
      "Checkpoint saved for fold 9, session 0, epoch 32\n",
      "Session 0 Epoch 33 - Train Loss: 0.016251\n",
      "Checkpoint saved for fold 9, session 0, epoch 33\n",
      "Session 0 Epoch 34 - Train Loss: 0.015398\n",
      "Checkpoint saved for fold 9, session 0, epoch 34\n",
      "Session 0 Epoch 35 - Train Loss: 0.015104\n",
      "Checkpoint saved for fold 9, session 0, epoch 35\n",
      "Session 0 Epoch 36 - Train Loss: 0.014656\n",
      "Checkpoint saved for fold 9, session 0, epoch 36\n",
      "Session 0 Epoch 37 - Train Loss: 0.014955\n",
      "Session 0 Epoch 38 - Train Loss: 0.014224\n",
      "Checkpoint saved for fold 9, session 0, epoch 38\n",
      "Session 0 Epoch 39 - Train Loss: 0.014271\n",
      "Session 0 Epoch 40 - Train Loss: 0.013534\n",
      "Checkpoint saved for fold 9, session 0, epoch 40\n",
      "Session 0 Epoch 41 - Train Loss: 0.013079\n",
      "Checkpoint saved for fold 9, session 0, epoch 41\n",
      "Session 0 Epoch 42 - Train Loss: 0.013339\n",
      "Session 0 Epoch 43 - Train Loss: 0.013650\n",
      "Session 0 Epoch 44 - Train Loss: 0.013523\n",
      "Session 0 Epoch 45 - Train Loss: 0.012161\n",
      "Checkpoint saved for fold 9, session 0, epoch 45\n",
      "Session 0 Epoch 46 - Train Loss: 0.012039\n",
      "Checkpoint saved for fold 9, session 0, epoch 46\n",
      "Session 0 Epoch 47 - Train Loss: 0.012233\n",
      "Session 0 Epoch 48 - Train Loss: 0.011624\n",
      "Checkpoint saved for fold 9, session 0, epoch 48\n",
      "Session 0 Epoch 49 - Train Loss: 0.011578\n",
      "Session 0 Epoch 50 - Train Loss: 0.011673\n",
      "Session 0 Epoch 51 - Train Loss: 0.012365\n",
      "Session 0 Epoch 52 - Train Loss: 0.010776\n",
      "Checkpoint saved for fold 9, session 0, epoch 52\n",
      "Session 0 Epoch 53 - Train Loss: 0.011754\n",
      "Session 0 Epoch 54 - Train Loss: 0.010990\n",
      "Session 0 Epoch 55 - Train Loss: 0.010879\n",
      "Session 0 Epoch 56 - Train Loss: 0.010060\n",
      "Checkpoint saved for fold 9, session 0, epoch 56\n",
      "Session 0 Epoch 57 - Train Loss: 0.011438\n",
      "Session 0 Epoch 58 - Train Loss: 0.009864\n",
      "Checkpoint saved for fold 9, session 0, epoch 58\n",
      "Session 0 Epoch 59 - Train Loss: 0.010152\n",
      "Session 0 Epoch 60 - Train Loss: 0.009289\n",
      "Checkpoint saved for fold 9, session 0, epoch 60\n",
      "Session 0 Epoch 61 - Train Loss: 0.009908\n",
      "Session 0 Epoch 62 - Train Loss: 0.009411\n",
      "Session 0 Epoch 63 - Train Loss: 0.009438\n",
      "Session 0 Epoch 64 - Train Loss: 0.009844\n",
      "Session 0 Epoch 65 - Train Loss: 0.009135\n",
      "Checkpoint saved for fold 9, session 0, epoch 65\n",
      "Session 0 Epoch 66 - Train Loss: 0.008734\n",
      "Checkpoint saved for fold 9, session 0, epoch 66\n",
      "Session 0 Epoch 67 - Train Loss: 0.009093\n",
      "Session 0 Epoch 68 - Train Loss: 0.008135\n",
      "Checkpoint saved for fold 9, session 0, epoch 68\n",
      "Session 0 Epoch 69 - Train Loss: 0.008556\n",
      "Session 0 Epoch 70 - Train Loss: 0.008966\n",
      "Session 0 Epoch 71 - Train Loss: 0.008429\n",
      "Session 0 Epoch 72 - Train Loss: 0.008185\n",
      "Session 0 Epoch 73 - Train Loss: 0.008056\n",
      "Session 0 Epoch 74 - Train Loss: 0.007928\n",
      "Checkpoint saved for fold 9, session 0, epoch 74\n",
      "Session 0 Epoch 75 - Train Loss: 0.007774\n",
      "Checkpoint saved for fold 9, session 0, epoch 75\n",
      "Session 0 Epoch 76 - Train Loss: 0.007648\n",
      "Checkpoint saved for fold 9, session 0, epoch 76\n",
      "Session 0 Epoch 77 - Train Loss: 0.007807\n",
      "Session 0 Epoch 78 - Train Loss: 0.007318\n",
      "Checkpoint saved for fold 9, session 0, epoch 78\n",
      "Session 0 Epoch 79 - Train Loss: 0.006857\n",
      "Checkpoint saved for fold 9, session 0, epoch 79\n",
      "Session 0 Epoch 80 - Train Loss: 0.006939\n",
      "Training on Session 2/9\n",
      "Session 1 Epoch 1 - Train Loss: 0.051187\n",
      "Checkpoint saved for fold 9, session 1, epoch 1\n",
      "Session 1 Epoch 2 - Train Loss: 0.050870\n",
      "Checkpoint saved for fold 9, session 1, epoch 2\n",
      "Session 1 Epoch 3 - Train Loss: 0.050855\n",
      "Session 1 Epoch 4 - Train Loss: 0.049780\n",
      "Checkpoint saved for fold 9, session 1, epoch 4\n",
      "Session 1 Epoch 5 - Train Loss: 0.047258\n",
      "Checkpoint saved for fold 9, session 1, epoch 5\n",
      "Session 1 Epoch 6 - Train Loss: 0.044683\n",
      "Checkpoint saved for fold 9, session 1, epoch 6\n",
      "Session 1 Epoch 7 - Train Loss: 0.042492\n",
      "Checkpoint saved for fold 9, session 1, epoch 7\n",
      "Session 1 Epoch 8 - Train Loss: 0.041392\n",
      "Checkpoint saved for fold 9, session 1, epoch 8\n",
      "Session 1 Epoch 9 - Train Loss: 0.039895\n",
      "Checkpoint saved for fold 9, session 1, epoch 9\n",
      "Session 1 Epoch 10 - Train Loss: 0.039052\n",
      "Checkpoint saved for fold 9, session 1, epoch 10\n",
      "Session 1 Epoch 11 - Train Loss: 0.038817\n",
      "Checkpoint saved for fold 9, session 1, epoch 11\n",
      "Session 1 Epoch 12 - Train Loss: 0.037949\n",
      "Checkpoint saved for fold 9, session 1, epoch 12\n",
      "Session 1 Epoch 13 - Train Loss: 0.037057\n",
      "Checkpoint saved for fold 9, session 1, epoch 13\n",
      "Session 1 Epoch 14 - Train Loss: 0.035475\n",
      "Checkpoint saved for fold 9, session 1, epoch 14\n",
      "Session 1 Epoch 15 - Train Loss: 0.032299\n",
      "Checkpoint saved for fold 9, session 1, epoch 15\n",
      "Session 1 Epoch 16 - Train Loss: 0.028780\n",
      "Checkpoint saved for fold 9, session 1, epoch 16\n",
      "Session 1 Epoch 17 - Train Loss: 0.025565\n",
      "Checkpoint saved for fold 9, session 1, epoch 17\n",
      "Session 1 Epoch 18 - Train Loss: 0.021930\n",
      "Checkpoint saved for fold 9, session 1, epoch 18\n",
      "Session 1 Epoch 19 - Train Loss: 0.020297\n",
      "Checkpoint saved for fold 9, session 1, epoch 19\n",
      "Session 1 Epoch 20 - Train Loss: 0.018172\n",
      "Checkpoint saved for fold 9, session 1, epoch 20\n",
      "Session 1 Epoch 21 - Train Loss: 0.017562\n",
      "Checkpoint saved for fold 9, session 1, epoch 21\n",
      "Session 1 Epoch 22 - Train Loss: 0.017464\n",
      "Session 1 Epoch 23 - Train Loss: 0.016223\n",
      "Checkpoint saved for fold 9, session 1, epoch 23\n",
      "Session 1 Epoch 24 - Train Loss: 0.015493\n",
      "Checkpoint saved for fold 9, session 1, epoch 24\n",
      "Session 1 Epoch 25 - Train Loss: 0.015214\n",
      "Checkpoint saved for fold 9, session 1, epoch 25\n",
      "Session 1 Epoch 26 - Train Loss: 0.014625\n",
      "Checkpoint saved for fold 9, session 1, epoch 26\n",
      "Session 1 Epoch 27 - Train Loss: 0.014929\n",
      "Session 1 Epoch 28 - Train Loss: 0.013946\n",
      "Checkpoint saved for fold 9, session 1, epoch 28\n",
      "Session 1 Epoch 29 - Train Loss: 0.013866\n",
      "Session 1 Epoch 30 - Train Loss: 0.013777\n",
      "Checkpoint saved for fold 9, session 1, epoch 30\n",
      "Session 1 Epoch 31 - Train Loss: 0.013257\n",
      "Checkpoint saved for fold 9, session 1, epoch 31\n",
      "Session 1 Epoch 32 - Train Loss: 0.013428\n",
      "Session 1 Epoch 33 - Train Loss: 0.013301\n",
      "Session 1 Epoch 34 - Train Loss: 0.013404\n",
      "Session 1 Epoch 35 - Train Loss: 0.013319\n",
      "Session 1 Epoch 36 - Train Loss: 0.012592\n",
      "Checkpoint saved for fold 9, session 1, epoch 36\n",
      "Session 1 Epoch 37 - Train Loss: 0.012792\n",
      "Session 1 Epoch 38 - Train Loss: 0.012842\n",
      "Session 1 Epoch 39 - Train Loss: 0.012359\n",
      "Checkpoint saved for fold 9, session 1, epoch 39\n",
      "Session 1 Epoch 40 - Train Loss: 0.012588\n",
      "Session 1 Epoch 41 - Train Loss: 0.012662\n",
      "Session 1 Epoch 42 - Train Loss: 0.012303\n",
      "Session 1 Epoch 43 - Train Loss: 0.012186\n",
      "Checkpoint saved for fold 9, session 1, epoch 43\n",
      "Session 1 Epoch 44 - Train Loss: 0.012118\n",
      "Session 1 Epoch 45 - Train Loss: 0.012419\n",
      "Session 1 Epoch 46 - Train Loss: 0.012363\n",
      "Session 1 Epoch 47 - Train Loss: 0.012169\n",
      "Session 1 Epoch 48 - Train Loss: 0.012531\n",
      "Session 1 Epoch 49 - Train Loss: 0.012237\n",
      "Session 1 Epoch 50 - Train Loss: 0.012090\n",
      "Session 1 Epoch 51 - Train Loss: 0.011714\n",
      "Checkpoint saved for fold 9, session 1, epoch 51\n",
      "Session 1 Epoch 52 - Train Loss: 0.011933\n",
      "Session 1 Epoch 53 - Train Loss: 0.011815\n",
      "Session 1 Epoch 54 - Train Loss: 0.012289\n",
      "Session 1 Epoch 55 - Train Loss: 0.012007\n",
      "Session 1 Epoch 56 - Train Loss: 0.012388\n",
      "Session 1 Epoch 57 - Train Loss: 0.011763\n",
      "Session 1 Epoch 58 - Train Loss: 0.012297\n",
      "Session 1 Epoch 59 - Train Loss: 0.011973\n",
      "Session 1 Epoch 60 - Train Loss: 0.012281\n",
      "Session 1 Epoch 61 - Train Loss: 0.011692\n",
      "Early stopping at epoch 61 for session 1\n",
      "Training on Session 3/9\n",
      "Session 2 Epoch 1 - Train Loss: 0.277277\n",
      "Checkpoint saved for fold 9, session 2, epoch 1\n",
      "Session 2 Epoch 2 - Train Loss: 0.271432\n",
      "Checkpoint saved for fold 9, session 2, epoch 2\n",
      "Session 2 Epoch 3 - Train Loss: 0.266378\n",
      "Checkpoint saved for fold 9, session 2, epoch 3\n",
      "Session 2 Epoch 4 - Train Loss: 0.262339\n",
      "Checkpoint saved for fold 9, session 2, epoch 4\n",
      "Session 2 Epoch 5 - Train Loss: 0.258781\n",
      "Checkpoint saved for fold 9, session 2, epoch 5\n",
      "Session 2 Epoch 6 - Train Loss: 0.255928\n",
      "Checkpoint saved for fold 9, session 2, epoch 6\n",
      "Session 2 Epoch 7 - Train Loss: 0.254023\n",
      "Checkpoint saved for fold 9, session 2, epoch 7\n",
      "Session 2 Epoch 8 - Train Loss: 0.251268\n",
      "Checkpoint saved for fold 9, session 2, epoch 8\n",
      "Session 2 Epoch 9 - Train Loss: 0.249759\n",
      "Checkpoint saved for fold 9, session 2, epoch 9\n",
      "Session 2 Epoch 10 - Train Loss: 0.247454\n",
      "Checkpoint saved for fold 9, session 2, epoch 10\n",
      "Session 2 Epoch 11 - Train Loss: 0.245648\n",
      "Checkpoint saved for fold 9, session 2, epoch 11\n",
      "Session 2 Epoch 12 - Train Loss: 0.244572\n",
      "Checkpoint saved for fold 9, session 2, epoch 12\n",
      "Session 2 Epoch 13 - Train Loss: 0.243408\n",
      "Checkpoint saved for fold 9, session 2, epoch 13\n",
      "Session 2 Epoch 14 - Train Loss: 0.241794\n",
      "Checkpoint saved for fold 9, session 2, epoch 14\n",
      "Session 2 Epoch 15 - Train Loss: 0.240808\n",
      "Checkpoint saved for fold 9, session 2, epoch 15\n",
      "Session 2 Epoch 16 - Train Loss: 0.239589\n",
      "Checkpoint saved for fold 9, session 2, epoch 16\n",
      "Session 2 Epoch 17 - Train Loss: 0.239184\n",
      "Checkpoint saved for fold 9, session 2, epoch 17\n",
      "Session 2 Epoch 18 - Train Loss: 0.237986\n",
      "Checkpoint saved for fold 9, session 2, epoch 18\n",
      "Session 2 Epoch 19 - Train Loss: 0.237230\n",
      "Checkpoint saved for fold 9, session 2, epoch 19\n",
      "Session 2 Epoch 20 - Train Loss: 0.237236\n",
      "Session 2 Epoch 21 - Train Loss: 0.236673\n",
      "Checkpoint saved for fold 9, session 2, epoch 21\n",
      "Session 2 Epoch 22 - Train Loss: 0.236737\n",
      "Session 2 Epoch 23 - Train Loss: 0.235450\n",
      "Checkpoint saved for fold 9, session 2, epoch 23\n",
      "Session 2 Epoch 24 - Train Loss: 0.235696\n",
      "Session 2 Epoch 25 - Train Loss: 0.234516\n",
      "Checkpoint saved for fold 9, session 2, epoch 25\n",
      "Session 2 Epoch 26 - Train Loss: 0.234538\n",
      "Session 2 Epoch 27 - Train Loss: 0.234593\n",
      "Session 2 Epoch 28 - Train Loss: 0.234166\n",
      "Checkpoint saved for fold 9, session 2, epoch 28\n",
      "Session 2 Epoch 29 - Train Loss: 0.233977\n",
      "Checkpoint saved for fold 9, session 2, epoch 29\n",
      "Session 2 Epoch 30 - Train Loss: 0.233984\n",
      "Session 2 Epoch 31 - Train Loss: 0.233662\n",
      "Checkpoint saved for fold 9, session 2, epoch 31\n",
      "Session 2 Epoch 32 - Train Loss: 0.233599\n",
      "Session 2 Epoch 33 - Train Loss: 0.233165\n",
      "Checkpoint saved for fold 9, session 2, epoch 33\n",
      "Session 2 Epoch 34 - Train Loss: 0.233229\n",
      "Session 2 Epoch 35 - Train Loss: 0.233716\n",
      "Session 2 Epoch 36 - Train Loss: 0.233568\n",
      "Session 2 Epoch 37 - Train Loss: 0.232959\n",
      "Checkpoint saved for fold 9, session 2, epoch 37\n",
      "Session 2 Epoch 38 - Train Loss: 0.233229\n",
      "Session 2 Epoch 39 - Train Loss: 0.233252\n",
      "Session 2 Epoch 40 - Train Loss: 0.232352\n",
      "Checkpoint saved for fold 9, session 2, epoch 40\n",
      "Session 2 Epoch 41 - Train Loss: 0.232352\n",
      "Session 2 Epoch 42 - Train Loss: 0.233314\n",
      "Session 2 Epoch 43 - Train Loss: 0.232295\n",
      "Session 2 Epoch 44 - Train Loss: 0.232583\n",
      "Session 2 Epoch 45 - Train Loss: 0.232573\n",
      "Session 2 Epoch 46 - Train Loss: 0.233042\n",
      "Session 2 Epoch 47 - Train Loss: 0.231959\n",
      "Checkpoint saved for fold 9, session 2, epoch 47\n",
      "Session 2 Epoch 48 - Train Loss: 0.232329\n",
      "Session 2 Epoch 49 - Train Loss: 0.231946\n",
      "Session 2 Epoch 50 - Train Loss: 0.231826\n",
      "Checkpoint saved for fold 9, session 2, epoch 50\n",
      "Session 2 Epoch 51 - Train Loss: 0.232083\n",
      "Session 2 Epoch 52 - Train Loss: 0.232225\n",
      "Session 2 Epoch 53 - Train Loss: 0.231897\n",
      "Session 2 Epoch 54 - Train Loss: 0.232318\n",
      "Session 2 Epoch 55 - Train Loss: 0.231370\n",
      "Checkpoint saved for fold 9, session 2, epoch 55\n",
      "Session 2 Epoch 56 - Train Loss: 0.231870\n",
      "Session 2 Epoch 57 - Train Loss: 0.231909\n",
      "Session 2 Epoch 58 - Train Loss: 0.231591\n",
      "Session 2 Epoch 59 - Train Loss: 0.231359\n",
      "Session 2 Epoch 60 - Train Loss: 0.231576\n",
      "Session 2 Epoch 61 - Train Loss: 0.231698\n",
      "Session 2 Epoch 62 - Train Loss: 0.231982\n",
      "Session 2 Epoch 63 - Train Loss: 0.231272\n",
      "Session 2 Epoch 64 - Train Loss: 0.230862\n",
      "Checkpoint saved for fold 9, session 2, epoch 64\n",
      "Session 2 Epoch 65 - Train Loss: 0.231784\n",
      "Session 2 Epoch 66 - Train Loss: 0.231033\n",
      "Session 2 Epoch 67 - Train Loss: 0.230201\n",
      "Checkpoint saved for fold 9, session 2, epoch 67\n",
      "Session 2 Epoch 68 - Train Loss: 0.231045\n",
      "Session 2 Epoch 69 - Train Loss: 0.230971\n",
      "Session 2 Epoch 70 - Train Loss: 0.231248\n",
      "Session 2 Epoch 71 - Train Loss: 0.230807\n",
      "Session 2 Epoch 72 - Train Loss: 0.230637\n",
      "Session 2 Epoch 73 - Train Loss: 0.230900\n",
      "Session 2 Epoch 74 - Train Loss: 0.230520\n",
      "Session 2 Epoch 75 - Train Loss: 0.230681\n",
      "Session 2 Epoch 76 - Train Loss: 0.230158\n",
      "Session 2 Epoch 77 - Train Loss: 0.229727\n",
      "Checkpoint saved for fold 9, session 2, epoch 77\n",
      "Session 2 Epoch 78 - Train Loss: 0.230041\n",
      "Session 2 Epoch 79 - Train Loss: 0.230134\n",
      "Session 2 Epoch 80 - Train Loss: 0.230051\n",
      "Training on Session 4/9\n",
      "Session 3 Epoch 1 - Train Loss: 0.173173\n",
      "Checkpoint saved for fold 9, session 3, epoch 1\n",
      "Session 3 Epoch 2 - Train Loss: 0.173213\n",
      "Session 3 Epoch 3 - Train Loss: 0.172772\n",
      "Checkpoint saved for fold 9, session 3, epoch 3\n",
      "Session 3 Epoch 4 - Train Loss: 0.172935\n",
      "Session 3 Epoch 5 - Train Loss: 0.172639\n",
      "Checkpoint saved for fold 9, session 3, epoch 5\n",
      "Session 3 Epoch 6 - Train Loss: 0.173064\n",
      "Session 3 Epoch 7 - Train Loss: 0.172845\n",
      "Session 3 Epoch 8 - Train Loss: 0.172125\n",
      "Checkpoint saved for fold 9, session 3, epoch 8\n",
      "Session 3 Epoch 9 - Train Loss: 0.172396\n",
      "Session 3 Epoch 10 - Train Loss: 0.172640\n",
      "Session 3 Epoch 11 - Train Loss: 0.172095\n",
      "Session 3 Epoch 12 - Train Loss: 0.171644\n",
      "Checkpoint saved for fold 9, session 3, epoch 12\n",
      "Session 3 Epoch 13 - Train Loss: 0.172141\n",
      "Session 3 Epoch 14 - Train Loss: 0.172497\n",
      "Session 3 Epoch 15 - Train Loss: 0.171805\n",
      "Session 3 Epoch 16 - Train Loss: 0.171877\n",
      "Session 3 Epoch 17 - Train Loss: 0.172116\n",
      "Session 3 Epoch 18 - Train Loss: 0.171885\n",
      "Session 3 Epoch 19 - Train Loss: 0.171654\n",
      "Session 3 Epoch 20 - Train Loss: 0.171276\n",
      "Checkpoint saved for fold 9, session 3, epoch 20\n",
      "Session 3 Epoch 21 - Train Loss: 0.171655\n",
      "Session 3 Epoch 22 - Train Loss: 0.171952\n",
      "Session 3 Epoch 23 - Train Loss: 0.171544\n",
      "Session 3 Epoch 24 - Train Loss: 0.170821\n",
      "Checkpoint saved for fold 9, session 3, epoch 24\n",
      "Session 3 Epoch 25 - Train Loss: 0.171735\n",
      "Session 3 Epoch 26 - Train Loss: 0.171933\n",
      "Session 3 Epoch 27 - Train Loss: 0.170732\n",
      "Session 3 Epoch 28 - Train Loss: 0.171096\n",
      "Session 3 Epoch 29 - Train Loss: 0.171568\n",
      "Session 3 Epoch 30 - Train Loss: 0.171208\n",
      "Session 3 Epoch 31 - Train Loss: 0.170546\n",
      "Checkpoint saved for fold 9, session 3, epoch 31\n",
      "Session 3 Epoch 32 - Train Loss: 0.171093\n",
      "Session 3 Epoch 33 - Train Loss: 0.170141\n",
      "Checkpoint saved for fold 9, session 3, epoch 33\n",
      "Session 3 Epoch 34 - Train Loss: 0.170659\n",
      "Session 3 Epoch 35 - Train Loss: 0.170183\n",
      "Session 3 Epoch 36 - Train Loss: 0.170688\n",
      "Session 3 Epoch 37 - Train Loss: 0.170095\n",
      "Session 3 Epoch 38 - Train Loss: 0.170098\n",
      "Session 3 Epoch 39 - Train Loss: 0.170124\n",
      "Session 3 Epoch 40 - Train Loss: 0.169987\n",
      "Checkpoint saved for fold 9, session 3, epoch 40\n",
      "Session 3 Epoch 41 - Train Loss: 0.169885\n",
      "Checkpoint saved for fold 9, session 3, epoch 41\n",
      "Session 3 Epoch 42 - Train Loss: 0.170718\n",
      "Session 3 Epoch 43 - Train Loss: 0.169980\n",
      "Session 3 Epoch 44 - Train Loss: 0.170162\n",
      "Session 3 Epoch 45 - Train Loss: 0.169828\n",
      "Session 3 Epoch 46 - Train Loss: 0.170231\n",
      "Session 3 Epoch 47 - Train Loss: 0.169745\n",
      "Checkpoint saved for fold 9, session 3, epoch 47\n",
      "Session 3 Epoch 48 - Train Loss: 0.169882\n",
      "Session 3 Epoch 49 - Train Loss: 0.169450\n",
      "Checkpoint saved for fold 9, session 3, epoch 49\n",
      "Session 3 Epoch 50 - Train Loss: 0.170171\n",
      "Session 3 Epoch 51 - Train Loss: 0.169266\n",
      "Checkpoint saved for fold 9, session 3, epoch 51\n",
      "Session 3 Epoch 52 - Train Loss: 0.169728\n",
      "Session 3 Epoch 53 - Train Loss: 0.169630\n",
      "Session 3 Epoch 54 - Train Loss: 0.169388\n",
      "Session 3 Epoch 55 - Train Loss: 0.169599\n",
      "Session 3 Epoch 56 - Train Loss: 0.169224\n",
      "Session 3 Epoch 57 - Train Loss: 0.168855\n",
      "Checkpoint saved for fold 9, session 3, epoch 57\n",
      "Session 3 Epoch 58 - Train Loss: 0.168923\n",
      "Session 3 Epoch 59 - Train Loss: 0.168585\n",
      "Checkpoint saved for fold 9, session 3, epoch 59\n",
      "Session 3 Epoch 60 - Train Loss: 0.169356\n",
      "Session 3 Epoch 61 - Train Loss: 0.169337\n",
      "Session 3 Epoch 62 - Train Loss: 0.168984\n",
      "Session 3 Epoch 63 - Train Loss: 0.168378\n",
      "Checkpoint saved for fold 9, session 3, epoch 63\n",
      "Session 3 Epoch 64 - Train Loss: 0.169191\n",
      "Session 3 Epoch 65 - Train Loss: 0.168569\n",
      "Session 3 Epoch 66 - Train Loss: 0.168603\n",
      "Session 3 Epoch 67 - Train Loss: 0.168433\n",
      "Session 3 Epoch 68 - Train Loss: 0.168848\n",
      "Session 3 Epoch 69 - Train Loss: 0.168805\n",
      "Session 3 Epoch 70 - Train Loss: 0.168250\n",
      "Checkpoint saved for fold 9, session 3, epoch 70\n",
      "Session 3 Epoch 71 - Train Loss: 0.167891\n",
      "Checkpoint saved for fold 9, session 3, epoch 71\n",
      "Session 3 Epoch 72 - Train Loss: 0.168448\n",
      "Session 3 Epoch 73 - Train Loss: 0.168632\n",
      "Session 3 Epoch 74 - Train Loss: 0.168093\n",
      "Session 3 Epoch 75 - Train Loss: 0.167661\n",
      "Checkpoint saved for fold 9, session 3, epoch 75\n",
      "Session 3 Epoch 76 - Train Loss: 0.167649\n",
      "Session 3 Epoch 77 - Train Loss: 0.168187\n",
      "Session 3 Epoch 78 - Train Loss: 0.168235\n",
      "Session 3 Epoch 79 - Train Loss: 0.167925\n",
      "Session 3 Epoch 80 - Train Loss: 0.167609\n",
      "Training on Session 5/9\n",
      "Session 4 Epoch 1 - Train Loss: 0.057570\n",
      "Checkpoint saved for fold 9, session 4, epoch 1\n",
      "Session 4 Epoch 2 - Train Loss: 0.057935\n",
      "Session 4 Epoch 3 - Train Loss: 0.057628\n",
      "Session 4 Epoch 4 - Train Loss: 0.057645\n",
      "Session 4 Epoch 5 - Train Loss: 0.057222\n",
      "Checkpoint saved for fold 9, session 4, epoch 5\n",
      "Session 4 Epoch 6 - Train Loss: 0.056858\n",
      "Checkpoint saved for fold 9, session 4, epoch 6\n",
      "Session 4 Epoch 7 - Train Loss: 0.057167\n",
      "Session 4 Epoch 8 - Train Loss: 0.057256\n",
      "Session 4 Epoch 9 - Train Loss: 0.057541\n",
      "Session 4 Epoch 10 - Train Loss: 0.057015\n",
      "Session 4 Epoch 11 - Train Loss: 0.057060\n",
      "Session 4 Epoch 12 - Train Loss: 0.058131\n",
      "Session 4 Epoch 13 - Train Loss: 0.057329\n",
      "Session 4 Epoch 14 - Train Loss: 0.057123\n",
      "Session 4 Epoch 15 - Train Loss: 0.057363\n",
      "Session 4 Epoch 16 - Train Loss: 0.057432\n",
      "Early stopping at epoch 16 for session 4\n",
      "Training on Session 6/9\n",
      "Session 5 Epoch 1 - Train Loss: 0.242796\n",
      "Checkpoint saved for fold 9, session 5, epoch 1\n",
      "Session 5 Epoch 2 - Train Loss: 0.243150\n",
      "Session 5 Epoch 3 - Train Loss: 0.242631\n",
      "Checkpoint saved for fold 9, session 5, epoch 3\n",
      "Session 5 Epoch 4 - Train Loss: 0.242775\n",
      "Session 5 Epoch 5 - Train Loss: 0.243340\n",
      "Session 5 Epoch 6 - Train Loss: 0.242917\n",
      "Session 5 Epoch 7 - Train Loss: 0.242815\n",
      "Session 5 Epoch 8 - Train Loss: 0.242988\n",
      "Session 5 Epoch 9 - Train Loss: 0.242371\n",
      "Checkpoint saved for fold 9, session 5, epoch 9\n",
      "Session 5 Epoch 10 - Train Loss: 0.242955\n",
      "Session 5 Epoch 11 - Train Loss: 0.242528\n",
      "Session 5 Epoch 12 - Train Loss: 0.242173\n",
      "Checkpoint saved for fold 9, session 5, epoch 12\n",
      "Session 5 Epoch 13 - Train Loss: 0.242073\n",
      "Checkpoint saved for fold 9, session 5, epoch 13\n",
      "Session 5 Epoch 14 - Train Loss: 0.242783\n",
      "Session 5 Epoch 15 - Train Loss: 0.241821\n",
      "Checkpoint saved for fold 9, session 5, epoch 15\n",
      "Session 5 Epoch 16 - Train Loss: 0.242550\n",
      "Session 5 Epoch 17 - Train Loss: 0.242137\n",
      "Session 5 Epoch 18 - Train Loss: 0.242658\n",
      "Session 5 Epoch 19 - Train Loss: 0.242080\n",
      "Session 5 Epoch 20 - Train Loss: 0.242534\n",
      "Session 5 Epoch 21 - Train Loss: 0.242217\n",
      "Session 5 Epoch 22 - Train Loss: 0.242180\n",
      "Session 5 Epoch 23 - Train Loss: 0.242574\n",
      "Session 5 Epoch 24 - Train Loss: 0.241834\n",
      "Session 5 Epoch 25 - Train Loss: 0.241775\n",
      "Early stopping at epoch 25 for session 5\n",
      "Training on Session 7/9\n",
      "Session 6 Epoch 1 - Train Loss: 0.180455\n",
      "Checkpoint saved for fold 9, session 6, epoch 1\n",
      "Session 6 Epoch 2 - Train Loss: 0.180767\n",
      "Session 6 Epoch 3 - Train Loss: 0.180461\n",
      "Session 6 Epoch 4 - Train Loss: 0.180544\n",
      "Session 6 Epoch 5 - Train Loss: 0.179728\n",
      "Checkpoint saved for fold 9, session 6, epoch 5\n",
      "Session 6 Epoch 6 - Train Loss: 0.180353\n",
      "Session 6 Epoch 7 - Train Loss: 0.180010\n",
      "Session 6 Epoch 8 - Train Loss: 0.180091\n",
      "Session 6 Epoch 9 - Train Loss: 0.180235\n",
      "Session 6 Epoch 10 - Train Loss: 0.179322\n",
      "Checkpoint saved for fold 9, session 6, epoch 10\n",
      "Session 6 Epoch 11 - Train Loss: 0.179668\n",
      "Session 6 Epoch 12 - Train Loss: 0.179442\n",
      "Session 6 Epoch 13 - Train Loss: 0.179673\n",
      "Session 6 Epoch 14 - Train Loss: 0.179525\n",
      "Session 6 Epoch 15 - Train Loss: 0.179724\n",
      "Session 6 Epoch 16 - Train Loss: 0.179549\n",
      "Session 6 Epoch 17 - Train Loss: 0.179319\n",
      "Session 6 Epoch 18 - Train Loss: 0.179555\n",
      "Session 6 Epoch 19 - Train Loss: 0.178878\n",
      "Checkpoint saved for fold 9, session 6, epoch 19\n",
      "Session 6 Epoch 20 - Train Loss: 0.179101\n",
      "Session 6 Epoch 21 - Train Loss: 0.178984\n",
      "Session 6 Epoch 22 - Train Loss: 0.178735\n",
      "Checkpoint saved for fold 9, session 6, epoch 22\n",
      "Session 6 Epoch 23 - Train Loss: 0.178872\n",
      "Session 6 Epoch 24 - Train Loss: 0.179242\n",
      "Session 6 Epoch 25 - Train Loss: 0.179077\n",
      "Session 6 Epoch 26 - Train Loss: 0.178623\n",
      "Checkpoint saved for fold 9, session 6, epoch 26\n",
      "Session 6 Epoch 27 - Train Loss: 0.178923\n",
      "Session 6 Epoch 28 - Train Loss: 0.178410\n",
      "Checkpoint saved for fold 9, session 6, epoch 28\n",
      "Session 6 Epoch 29 - Train Loss: 0.178055\n",
      "Checkpoint saved for fold 9, session 6, epoch 29\n",
      "Session 6 Epoch 30 - Train Loss: 0.178809\n",
      "Session 6 Epoch 31 - Train Loss: 0.178764\n",
      "Session 6 Epoch 32 - Train Loss: 0.178943\n",
      "Session 6 Epoch 33 - Train Loss: 0.178365\n",
      "Session 6 Epoch 34 - Train Loss: 0.177703\n",
      "Checkpoint saved for fold 9, session 6, epoch 34\n",
      "Session 6 Epoch 35 - Train Loss: 0.178282\n",
      "Session 6 Epoch 36 - Train Loss: 0.177930\n",
      "Session 6 Epoch 37 - Train Loss: 0.178124\n",
      "Session 6 Epoch 38 - Train Loss: 0.177878\n",
      "Session 6 Epoch 39 - Train Loss: 0.178107\n",
      "Session 6 Epoch 40 - Train Loss: 0.178026\n",
      "Session 6 Epoch 41 - Train Loss: 0.177926\n",
      "Session 6 Epoch 42 - Train Loss: 0.177272\n",
      "Checkpoint saved for fold 9, session 6, epoch 42\n",
      "Session 6 Epoch 43 - Train Loss: 0.177378\n",
      "Session 6 Epoch 44 - Train Loss: 0.177044\n",
      "Checkpoint saved for fold 9, session 6, epoch 44\n",
      "Session 6 Epoch 45 - Train Loss: 0.177369\n",
      "Session 6 Epoch 46 - Train Loss: 0.177523\n",
      "Session 6 Epoch 47 - Train Loss: 0.177498\n",
      "Session 6 Epoch 48 - Train Loss: 0.177433\n",
      "Session 6 Epoch 49 - Train Loss: 0.177339\n",
      "Session 6 Epoch 50 - Train Loss: 0.177380\n",
      "Session 6 Epoch 51 - Train Loss: 0.176931\n",
      "Checkpoint saved for fold 9, session 6, epoch 51\n",
      "Session 6 Epoch 52 - Train Loss: 0.176857\n",
      "Session 6 Epoch 53 - Train Loss: 0.177546\n",
      "Session 6 Epoch 54 - Train Loss: 0.177278\n",
      "Session 6 Epoch 55 - Train Loss: 0.176797\n",
      "Checkpoint saved for fold 9, session 6, epoch 55\n",
      "Session 6 Epoch 56 - Train Loss: 0.177326\n",
      "Session 6 Epoch 57 - Train Loss: 0.176649\n",
      "Checkpoint saved for fold 9, session 6, epoch 57\n",
      "Session 6 Epoch 58 - Train Loss: 0.176802\n",
      "Session 6 Epoch 59 - Train Loss: 0.176562\n",
      "Session 6 Epoch 60 - Train Loss: 0.176721\n",
      "Session 6 Epoch 61 - Train Loss: 0.176499\n",
      "Checkpoint saved for fold 9, session 6, epoch 61\n",
      "Session 6 Epoch 62 - Train Loss: 0.176078\n",
      "Checkpoint saved for fold 9, session 6, epoch 62\n",
      "Session 6 Epoch 63 - Train Loss: 0.176189\n",
      "Session 6 Epoch 64 - Train Loss: 0.176054\n",
      "Session 6 Epoch 65 - Train Loss: 0.176305\n",
      "Session 6 Epoch 66 - Train Loss: 0.176199\n",
      "Session 6 Epoch 67 - Train Loss: 0.176099\n",
      "Session 6 Epoch 68 - Train Loss: 0.175925\n",
      "Checkpoint saved for fold 9, session 6, epoch 68\n",
      "Session 6 Epoch 69 - Train Loss: 0.175846\n",
      "Session 6 Epoch 70 - Train Loss: 0.176124\n",
      "Session 6 Epoch 71 - Train Loss: 0.176205\n",
      "Session 6 Epoch 72 - Train Loss: 0.175715\n",
      "Checkpoint saved for fold 9, session 6, epoch 72\n",
      "Session 6 Epoch 73 - Train Loss: 0.175311\n",
      "Checkpoint saved for fold 9, session 6, epoch 73\n",
      "Session 6 Epoch 74 - Train Loss: 0.175525\n",
      "Session 6 Epoch 75 - Train Loss: 0.175570\n",
      "Session 6 Epoch 76 - Train Loss: 0.175957\n",
      "Session 6 Epoch 77 - Train Loss: 0.175247\n",
      "Session 6 Epoch 78 - Train Loss: 0.175442\n",
      "Session 6 Epoch 79 - Train Loss: 0.175123\n",
      "Checkpoint saved for fold 9, session 6, epoch 79\n",
      "Session 6 Epoch 80 - Train Loss: 0.175040\n",
      "Training on Session 8/9\n",
      "Session 7 Epoch 1 - Train Loss: 0.063096\n",
      "Checkpoint saved for fold 9, session 7, epoch 1\n",
      "Session 7 Epoch 2 - Train Loss: 0.062721\n",
      "Checkpoint saved for fold 9, session 7, epoch 2\n",
      "Session 7 Epoch 3 - Train Loss: 0.063128\n",
      "Session 7 Epoch 4 - Train Loss: 0.063013\n",
      "Session 7 Epoch 5 - Train Loss: 0.062476\n",
      "Checkpoint saved for fold 9, session 7, epoch 5\n",
      "Session 7 Epoch 6 - Train Loss: 0.063004\n",
      "Session 7 Epoch 7 - Train Loss: 0.062646\n",
      "Session 7 Epoch 8 - Train Loss: 0.063140\n",
      "Session 7 Epoch 9 - Train Loss: 0.062928\n",
      "Session 7 Epoch 10 - Train Loss: 0.063161\n",
      "Session 7 Epoch 11 - Train Loss: 0.062992\n",
      "Session 7 Epoch 12 - Train Loss: 0.062891\n",
      "Session 7 Epoch 13 - Train Loss: 0.062675\n",
      "Session 7 Epoch 14 - Train Loss: 0.062612\n",
      "Session 7 Epoch 15 - Train Loss: 0.062742\n",
      "Early stopping at epoch 15 for session 7\n",
      "Training on Session 9/9\n",
      "Session 8 Epoch 1 - Train Loss: 0.125203\n",
      "Checkpoint saved for fold 9, session 8, epoch 1\n",
      "Session 8 Epoch 2 - Train Loss: 0.124050\n",
      "Checkpoint saved for fold 9, session 8, epoch 2\n",
      "Session 8 Epoch 3 - Train Loss: 0.124652\n",
      "Session 8 Epoch 4 - Train Loss: 0.124590\n",
      "Session 8 Epoch 5 - Train Loss: 0.124917\n",
      "Session 8 Epoch 6 - Train Loss: 0.124398\n",
      "Session 8 Epoch 7 - Train Loss: 0.124135\n",
      "Session 8 Epoch 8 - Train Loss: 0.124483\n",
      "Session 8 Epoch 9 - Train Loss: 0.124373\n",
      "Session 8 Epoch 10 - Train Loss: 0.124257\n",
      "Session 8 Epoch 11 - Train Loss: 0.124560\n",
      "Session 8 Epoch 12 - Train Loss: 0.123783\n",
      "Checkpoint saved for fold 9, session 8, epoch 12\n",
      "Session 8 Epoch 13 - Train Loss: 0.124234\n",
      "Session 8 Epoch 14 - Train Loss: 0.123876\n",
      "Session 8 Epoch 15 - Train Loss: 0.124335\n",
      "Session 8 Epoch 16 - Train Loss: 0.123618\n",
      "Checkpoint saved for fold 9, session 8, epoch 16\n",
      "Session 8 Epoch 17 - Train Loss: 0.124218\n",
      "Session 8 Epoch 18 - Train Loss: 0.123811\n",
      "Session 8 Epoch 19 - Train Loss: 0.124015\n",
      "Session 8 Epoch 20 - Train Loss: 0.123691\n",
      "Session 8 Epoch 21 - Train Loss: 0.124131\n",
      "Session 8 Epoch 22 - Train Loss: 0.124243\n",
      "Session 8 Epoch 23 - Train Loss: 0.123348\n",
      "Checkpoint saved for fold 9, session 8, epoch 23\n",
      "Session 8 Epoch 24 - Train Loss: 0.124246\n",
      "Session 8 Epoch 25 - Train Loss: 0.123934\n",
      "Session 8 Epoch 26 - Train Loss: 0.123365\n",
      "Session 8 Epoch 27 - Train Loss: 0.123544\n",
      "Session 8 Epoch 28 - Train Loss: 0.123697\n",
      "Session 8 Epoch 29 - Train Loss: 0.123507\n",
      "Session 8 Epoch 30 - Train Loss: 0.123673\n",
      "Session 8 Epoch 31 - Train Loss: 0.123166\n",
      "Checkpoint saved for fold 9, session 8, epoch 31\n",
      "Session 8 Epoch 32 - Train Loss: 0.123714\n",
      "Session 8 Epoch 33 - Train Loss: 0.123305\n",
      "Session 8 Epoch 34 - Train Loss: 0.123495\n",
      "Session 8 Epoch 35 - Train Loss: 0.123142\n",
      "Session 8 Epoch 36 - Train Loss: 0.123466\n",
      "Session 8 Epoch 37 - Train Loss: 0.123103\n",
      "Session 8 Epoch 38 - Train Loss: 0.123271\n",
      "Session 8 Epoch 39 - Train Loss: 0.123333\n",
      "Session 8 Epoch 40 - Train Loss: 0.123429\n",
      "Session 8 Epoch 41 - Train Loss: 0.123104\n",
      "Early stopping at epoch 41 for session 8\n",
      "Fold 9 - Test Loss: 0.0909, R^2: -15567876172247.1699\n",
      "\n",
      "=== Summary Across All Folds ===\n",
      "Fold 0 | Test Loss: 0.0845 | R^2: -349153187188641.9375\n",
      "Fold 1 | Test Loss: 0.1132 | R^2: -44688662408493.7109\n",
      "Fold 2 | Test Loss: 0.4074 | R^2: -2575502151738.9575\n",
      "Fold 3 | Test Loss: 0.1421 | R^2: -3469154865284.7056\n",
      "Fold 4 | Test Loss: 0.0547 | R^2: -153035927648403.3750\n",
      "Fold 5 | Test Loss: 0.2903 | R^2: -2017141352979.1960\n",
      "Fold 6 | Test Loss: 0.1641 | R^2: -2746117809502.3521\n",
      "Fold 7 | Test Loss: 0.0641 | R^2: -25866959645424.9609\n",
      "Fold 8 | Test Loss: 0.2071 | R^2: -42277316069751.1328\n",
      "Fold 9 | Test Loss: 0.0909 | R^2: -15567876172247.1699\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from glob import glob\n",
    "from sklearn.metrics import r2_score\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load session paths\n",
    "processed_data_l_X = sorted(glob('/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/**/X.npy', recursive=True))\n",
    "processed_data_l_y = sorted(glob('/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/**/y.npy', recursive=True))\n",
    "\n",
    "# Create train-test splits (leave-one-session-out)\n",
    "results = []\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "os.makedirs(\"runs\", exist_ok=True)\n",
    "\n",
    "patience = 10\n",
    "min_delta = 1e-4\n",
    "\n",
    "for KFOLD in range(len(processed_data_l_X)):\n",
    "    print(f\"\\n=== Fold {KFOLD} ===\")\n",
    "\n",
    "    test_X_path = processed_data_l_X[KFOLD]\n",
    "    test_y_path = processed_data_l_y[KFOLD]\n",
    "\n",
    "    train_X_paths = [x for i, x in enumerate(processed_data_l_X) if i != KFOLD]\n",
    "    train_y_paths = [y for i, y in enumerate(processed_data_l_y) if i != KFOLD]\n",
    "\n",
    "    # Initialize model and optimizer\n",
    "    model = EcogToMotionNet().to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=5)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    writer = SummaryWriter(log_dir=f\"runs/fold_{KFOLD}\")\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    # Sequential training over sessions\n",
    "    for session_idx, (X_path, y_path) in enumerate(zip(train_X_paths, train_y_paths)):\n",
    "        print(f\"Training on Session {session_idx + 1}/{len(train_X_paths)}\")\n",
    "        X = np.load(X_path)\n",
    "        y = np.load(y_path)\n",
    "        dataset = EcogMotionDataset(X, y)\n",
    "        loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "        model.train()\n",
    "        epochs_no_improve = 0\n",
    "        best_loss = float(\"inf\")\n",
    "\n",
    "        for epoch in range(80):\n",
    "            running_loss = 0.0\n",
    "            for X_batch, y_batch in loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                preds = model(X_batch)\n",
    "                loss = criterion(preds, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "            avg_loss = running_loss / len(loader.dataset)\n",
    "            writer.add_scalar(f\"Loss/Train_Session_{session_idx}\", avg_loss, epoch)\n",
    "            print(f\"Session {session_idx} Epoch {epoch+1} - Train Loss: {avg_loss:.6f}\")\n",
    "            scheduler.step(avg_loss)\n",
    "\n",
    "            # Save checkpoint if best loss\n",
    "            if avg_loss < best_loss - min_delta:\n",
    "                best_loss = avg_loss\n",
    "                epochs_no_improve = 0\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': best_loss\n",
    "                }, f\"checkpoints/model_fold_{KFOLD}_session_{session_idx}.pt\")\n",
    "                print(f\"Checkpoint saved for fold {KFOLD}, session {session_idx}, epoch {epoch+1}\")\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1} for session {session_idx}\")\n",
    "                break\n",
    "\n",
    "    # Evaluate on test session\n",
    "    X_test = np.load(test_X_path)\n",
    "    y_test = np.load(test_y_path)\n",
    "    test_dataset = EcogMotionDataset(X_test, y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_r2 = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            total_r2 += r2_score(targets.cpu().numpy(), outputs.cpu().numpy()) * inputs.size(0)\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "    avg_test_loss = total_loss / total_samples\n",
    "    avg_test_r2 = total_r2 / total_samples\n",
    "    print(f\"Fold {KFOLD} - Test Loss: {avg_test_loss:.4f}, R^2: {avg_test_r2:.4f}\")\n",
    "\n",
    "    writer.add_scalar(\"Loss/Test\", avg_test_loss, 0)\n",
    "    writer.add_scalar(\"R2/Test\", avg_test_r2, 0)\n",
    "\n",
    "    # Final model save\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'test_loss': avg_test_loss,\n",
    "        'test_r2': avg_test_r2\n",
    "    }, f\"checkpoints/model_fold_{KFOLD}_final.pt\")\n",
    "\n",
    "    writer.close()\n",
    "    results.append((KFOLD, avg_test_loss, avg_test_r2))\n",
    "\n",
    "# Summary of all folds\n",
    "print(\"\\n=== Summary Across All Folds ===\")\n",
    "for fold, loss, r2 in results:\n",
    "    print(f\"Fold {fold} | Test Loss: {loss:.4f} | R^2: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c5640c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. LSTM\n",
    "# lstm_model = EcogLSTM(input_size=64, hidden_size=128, num_layers=1, output_size=3)\n",
    "# lstm_optimizer = torch.optim.Adam(lstm_model.parameters(), lr=1e-3)\n",
    "# lstm_train_losses, lstm_val_losses = train_model(lstm_model, train_loader, val_loader, criterion, lstm_optimizer, device, epochs=20, model_name=\"LSTM\")\n",
    "\n",
    "# 3. Linear\n",
    "# input_channels = X.shape[2]\n",
    "# sequence_length = X.shape[1]\n",
    "# linear_model = LinearEcogToMotionNet(input_channels, sequence_length)\n",
    "# linear_optimizer = torch.optim.Adam(linear_model.parameters(), lr=1e-3)\n",
    "# linear_train_losses, linear_val_losses = train_model(linear_model, train_loader, val_loader, criterion, linear_optimizer, device, epochs=20, model_name=\"Linear\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d4fbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Plot all losses together\n",
    "# plot_losses({\n",
    "#     # \"LSTM\": (lstm_train_losses, lstm_val_losses),\n",
    "#     \"CNN\": (cnn_train_losses, cnn_val_losses),\n",
    "#     # \"Linear\": (linear_train_losses, linear_val_losses)\n",
    "# })\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892acd38",
   "metadata": {},
   "source": [
    "## Loading the model and making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f7a1c0ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EcogToMotionNet(\n",
       "  (convolv): Sequential(\n",
       "    (0): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (7): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (lstm): LSTM(256, 128, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (attn_weight): Linear(in_features=256, out_features=1, bias=False)\n",
       "  (fc): Sequential(\n",
       "    (0): ReLU(inplace=True)\n",
       "    (1): Dropout(p=0.3, inplace=False)\n",
       "    (2): Linear(in_features=256, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recreate the model structure\n",
    "hybrid_model = EcogToMotionNet()\n",
    "hybrid_model.load_state_dict(torch.load(\"models/Hybrid_CNN_LSTM_ipsilateral_3_output.pth\"))\n",
    "hybrid_model.to(device)\n",
    "hybrid_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4044352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define K-fold sets\n",
    "test_list_X = []\n",
    "train_list_X = []\n",
    "test_list_y = []\n",
    "train_list_y = []\n",
    "\n",
    "for i in range(len(processed_data_l_X)):\n",
    "    test_list_X.append(processed_data_l_X[i])\n",
    "    test_list_y.append(processed_data_l_y[i])\n",
    "    train_X = [x for idx, x in enumerate(processed_data_l_X) if idx != i]\n",
    "    train_y = [y for idx, y in enumerate(processed_data_l_y) if idx != i]\n",
    "    train_list_X.append(train_X)\n",
    "    train_list_y.append(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38d2a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_list_X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0851377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_list_y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58814ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_list_X[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a383535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trained on the 8th session and predicting on the first session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2548d318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_ecog_data_file = ecog_data_file_l[8]\n",
    "# current_motion_data_file = motion_data_file_l[8]\n",
    "# preprocessor = PreprocessData(current_ecog_data_file, current_motion_data_file)\n",
    "# X, y = preprocessor.process()\n",
    "# preprocessor.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b0098217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trained using data from session 8; Testing using data from session 1\n",
    "X = np.load(train_list_X[0][1])\n",
    "y = np.load(train_list_y[0][1])\n",
    "# scaler = joblib.load(train_list_X[0][1].strip(\"X.npy\") + \"scaler_ecog.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d1ce49f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89999, 20, 64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "025bd69b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89999, 3)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3678f2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Train and Validation Sets\n",
    "dataset = EcogMotionDataset(X, y)\n",
    "test_loader = DataLoader(dataset, batch_size=64, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a0418427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to ecog_predictions.mat\n"
     ]
    }
   ],
   "source": [
    "# Example Call\n",
    "hybrid_model.to(device)\n",
    "output_file_path = train_list_X[0][1].strip(\"X.npy\") + \"ecog_predictions.mat\"\n",
    "predictions, targets = predict_and_export(hybrid_model, test_loader, device, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "507e1c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-04-29_(S1)/ecog_predictions.mat'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ccb6f993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.23674075,  0.13311261, -0.3458554 ], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0d6f27a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.094205596)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_rmse(targets[0], predictions[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
